/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [2,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [6,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [7,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [8,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [9,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [10,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [11,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [12,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [13,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [14,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [15,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [16,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [17,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [18,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [19,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [20,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [21,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [22,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [23,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataListLoader' is deprecated, use 'loader.DataListLoader' instead
  warnings.warn(out)
/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)


Training Data
Sequence: 02
[[1.000e+00 1.000e+00 9.120e+02 ... 0.000e+00 7.000e+00 1.000e+00]
 [2.000e+00 1.000e+00 9.120e+02 ... 0.000e+00 7.000e+00 1.000e+00]
 [3.000e+00 1.000e+00 9.120e+02 ... 0.000e+00 7.000e+00 1.000e+00]
 ...
 [6.000e+02 8.100e+01 1.007e+03 ... 1.000e+00 1.000e+00 0.000e+00]
 [6.000e+02 8.200e+01 9.870e+02 ... 1.000e+00 1.000e+00 0.000e+00]
 [6.000e+02 8.300e+01 7.490e+02 ... 1.000e+00 1.000e+00 5.000e-01]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-02-FRCNN/img1
Sequence: 02, Frame: 2/500
Sequence: 02, Frame: 7/500
Sequence: 02, Frame: 12/500
Sequence: 02, Frame: 17/500
Sequence: 02, Frame: 22/500
Sequence: 02, Frame: 27/500
Sequence: 02, Frame: 32/500
Sequence: 02, Frame: 37/500
Sequence: 02, Frame: 42/500
Sequence: 02, Frame: 47/500
Sequence: 02, Frame: 52/500
Sequence: 02, Frame: 57/500
Sequence: 02, Frame: 62/500
Sequence: 02, Frame: 67/500
Sequence: 02, Frame: 72/500
Sequence: 02, Frame: 77/500
Sequence: 02, Frame: 82/500
Sequence: 02, Frame: 87/500
Sequence: 02, Frame: 92/500
Sequence: 02, Frame: 97/500
Sequence: 02, Frame: 102/500
Sequence: 02, Frame: 107/500
Sequence: 02, Frame: 112/500
Sequence: 02, Frame: 117/500
Sequence: 02, Frame: 122/500
Sequence: 02, Frame: 127/500
Sequence: 02, Frame: 132/500
Sequence: 02, Frame: 137/500
Sequence: 02, Frame: 142/500
Sequence: 02, Frame: 147/500
Sequence: 02, Frame: 152/500
Sequence: 02, Frame: 157/500
Sequence: 02, Frame: 162/500
Sequence: 02, Frame: 167/500
Sequence: 02, Frame: 172/500
Sequence: 02, Frame: 177/500
Sequence: 02, Frame: 182/500
Sequence: 02, Frame: 187/500
Sequence: 02, Frame: 192/500
Sequence: 02, Frame: 197/500
Sequence: 02, Frame: 202/500
Sequence: 02, Frame: 207/500
Sequence: 02, Frame: 212/500
Sequence: 02, Frame: 217/500
Sequence: 02, Frame: 222/500
Sequence: 02, Frame: 227/500
Sequence: 02, Frame: 232/500
Sequence: 02, Frame: 237/500
Sequence: 02, Frame: 242/500
Sequence: 02, Frame: 247/500
Sequence: 02, Frame: 252/500
Sequence: 02, Frame: 257/500
Sequence: 02, Frame: 262/500
Sequence: 02, Frame: 267/500
Sequence: 02, Frame: 272/500
Sequence: 02, Frame: 277/500
Sequence: 02, Frame: 282/500
Sequence: 02, Frame: 287/500
Sequence: 02, Frame: 292/500
Sequence: 02, Frame: 297/500
Sequence: 02, Frame: 302/500
Sequence: 02, Frame: 307/500
Sequence: 02, Frame: 312/500
Sequence: 02, Frame: 317/500
Sequence: 02, Frame: 322/500
Sequence: 02, Frame: 327/500
Sequence: 02, Frame: 332/500
Sequence: 02, Frame: 337/500
Sequence: 02, Frame: 342/500
Sequence: 02, Frame: 347/500
Sequence: 02, Frame: 352/500
Sequence: 02, Frame: 357/500
Sequence: 02, Frame: 362/500
Sequence: 02, Frame: 367/500
Sequence: 02, Frame: 372/500
Sequence: 02, Frame: 377/500
Sequence: 02, Frame: 382/500
Sequence: 02, Frame: 387/500
Sequence: 02, Frame: 392/500
Sequence: 02, Frame: 397/500
Sequence: 02, Frame: 402/500
Sequence: 02, Frame: 407/500
Sequence: 02, Frame: 412/500
Sequence: 02, Frame: 417/500
Sequence: 02, Frame: 422/500
Sequence: 02, Frame: 427/500
Sequence: 02, Frame: 432/500
Sequence: 02, Frame: 437/500
Sequence: 02, Frame: 442/500
Sequence: 02, Frame: 447/500
Sequence: 02, Frame: 452/500
Sequence: 02, Frame: 457/500
Sequence: 02, Frame: 462/500
Sequence: 02, Frame: 467/500
Sequence: 02, Frame: 472/500
Sequence: 02, Frame: 477/500
Sequence: 02, Frame: 482/500
Sequence: 02, Frame: 487/500
Sequence: 02, Frame: 492/500
Sequence: 02, Frame: 497/500
Data preparation finished
Sequence: 04
[[1.0000e+00 1.0000e+00 1.3630e+03 ... 1.0000e+00 1.0000e+00 8.6014e-01]
 [2.0000e+00 1.0000e+00 1.3620e+03 ... 1.0000e+00 1.0000e+00 8.6173e-01]
 [3.0000e+00 1.0000e+00 1.3620e+03 ... 1.0000e+00 1.0000e+00 8.6173e-01]
 ...
 [1.0480e+03 1.4100e+02 7.2800e+02 ... 1.0000e+00 1.0000e+00 1.4143e-01]
 [1.0490e+03 1.4100e+02 7.2900e+02 ... 1.0000e+00 1.0000e+00 1.3481e-01]
 [1.0500e+03 1.4100e+02 7.3100e+02 ... 1.0000e+00 1.0000e+00 1.1869e-01]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-04-FRCNN/img1
Sequence: 04, Frame: 2/900
Sequence: 04, Frame: 7/900
Sequence: 04, Frame: 12/900
Sequence: 04, Frame: 17/900
Sequence: 04, Frame: 22/900
Sequence: 04, Frame: 27/900
Sequence: 04, Frame: 32/900
Sequence: 04, Frame: 37/900
Sequence: 04, Frame: 42/900
Sequence: 04, Frame: 47/900
Sequence: 04, Frame: 52/900
Sequence: 04, Frame: 57/900
Sequence: 04, Frame: 62/900
Sequence: 04, Frame: 67/900
Sequence: 04, Frame: 72/900
Sequence: 04, Frame: 77/900
Sequence: 04, Frame: 82/900
Sequence: 04, Frame: 87/900
Sequence: 04, Frame: 92/900
Sequence: 04, Frame: 97/900
Sequence: 04, Frame: 102/900
Sequence: 04, Frame: 107/900
Sequence: 04, Frame: 112/900
Sequence: 04, Frame: 117/900
Sequence: 04, Frame: 122/900
Sequence: 04, Frame: 127/900
Sequence: 04, Frame: 132/900
Sequence: 04, Frame: 137/900
Sequence: 04, Frame: 142/900
Sequence: 04, Frame: 147/900
Sequence: 04, Frame: 152/900
Sequence: 04, Frame: 157/900
Sequence: 04, Frame: 162/900
Sequence: 04, Frame: 167/900
Sequence: 04, Frame: 172/900
Sequence: 04, Frame: 177/900
Sequence: 04, Frame: 182/900
Sequence: 04, Frame: 187/900
Sequence: 04, Frame: 192/900
Sequence: 04, Frame: 197/900
Sequence: 04, Frame: 202/900
Sequence: 04, Frame: 207/900
Sequence: 04, Frame: 212/900
Sequence: 04, Frame: 217/900
Sequence: 04, Frame: 222/900
Sequence: 04, Frame: 227/900
Sequence: 04, Frame: 232/900
Sequence: 04, Frame: 237/900
Sequence: 04, Frame: 242/900
Sequence: 04, Frame: 247/900
Sequence: 04, Frame: 252/900
Sequence: 04, Frame: 257/900
Sequence: 04, Frame: 262/900
Sequence: 04, Frame: 267/900
Sequence: 04, Frame: 272/900
Sequence: 04, Frame: 277/900
Sequence: 04, Frame: 282/900
Sequence: 04, Frame: 287/900
Sequence: 04, Frame: 292/900
Sequence: 04, Frame: 297/900
Sequence: 04, Frame: 302/900
Sequence: 04, Frame: 307/900
Sequence: 04, Frame: 312/900
Sequence: 04, Frame: 317/900
Sequence: 04, Frame: 322/900
Sequence: 04, Frame: 327/900
Sequence: 04, Frame: 332/900
Sequence: 04, Frame: 337/900
Sequence: 04, Frame: 342/900
Sequence: 04, Frame: 347/900
Sequence: 04, Frame: 352/900
Sequence: 04, Frame: 357/900
Sequence: 04, Frame: 362/900
Sequence: 04, Frame: 367/900
Sequence: 04, Frame: 372/900
Sequence: 04, Frame: 377/900
Sequence: 04, Frame: 382/900
Sequence: 04, Frame: 387/900
Sequence: 04, Frame: 392/900
Sequence: 04, Frame: 397/900
Sequence: 04, Frame: 402/900
Sequence: 04, Frame: 407/900
Sequence: 04, Frame: 412/900
Sequence: 04, Frame: 417/900
Sequence: 04, Frame: 422/900
Sequence: 04, Frame: 427/900
Sequence: 04, Frame: 432/900
Sequence: 04, Frame: 437/900
Sequence: 04, Frame: 442/900
Sequence: 04, Frame: 447/900
Sequence: 04, Frame: 452/900
Sequence: 04, Frame: 457/900
Sequence: 04, Frame: 462/900
Sequence: 04, Frame: 467/900
Sequence: 04, Frame: 472/900
Sequence: 04, Frame: 477/900
Sequence: 04, Frame: 482/900
Sequence: 04, Frame: 487/900
Sequence: 04, Frame: 492/900
Sequence: 04, Frame: 497/900
Sequence: 04, Frame: 502/900
Sequence: 04, Frame: 507/900
Sequence: 04, Frame: 512/900
Sequence: 04, Frame: 517/900
Sequence: 04, Frame: 522/900
Sequence: 04, Frame: 527/900
Sequence: 04, Frame: 532/900
Sequence: 04, Frame: 537/900
Sequence: 04, Frame: 542/900
Sequence: 04, Frame: 547/900
Sequence: 04, Frame: 552/900
Sequence: 04, Frame: 557/900
Sequence: 04, Frame: 562/900
Sequence: 04, Frame: 567/900
Sequence: 04, Frame: 572/900
Sequence: 04, Frame: 577/900
Sequence: 04, Frame: 582/900
Sequence: 04, Frame: 587/900
Sequence: 04, Frame: 592/900
Sequence: 04, Frame: 597/900
Sequence: 04, Frame: 602/900
Sequence: 04, Frame: 607/900
Sequence: 04, Frame: 612/900
Sequence: 04, Frame: 617/900
Sequence: 04, Frame: 622/900
Sequence: 04, Frame: 627/900
Sequence: 04, Frame: 632/900
Sequence: 04, Frame: 637/900
Sequence: 04, Frame: 642/900
Sequence: 04, Frame: 647/900
Sequence: 04, Frame: 652/900
Sequence: 04, Frame: 657/900
Sequence: 04, Frame: 662/900
Sequence: 04, Frame: 667/900
Sequence: 04, Frame: 672/900
Sequence: 04, Frame: 677/900
Sequence: 04, Frame: 682/900
Sequence: 04, Frame: 687/900
Sequence: 04, Frame: 692/900
Sequence: 04, Frame: 697/900
Sequence: 04, Frame: 702/900
Sequence: 04, Frame: 707/900
Sequence: 04, Frame: 712/900
Sequence: 04, Frame: 717/900
Sequence: 04, Frame: 722/900
Sequence: 04, Frame: 727/900
Sequence: 04, Frame: 732/900
Sequence: 04, Frame: 737/900
Sequence: 04, Frame: 742/900
Sequence: 04, Frame: 747/900
Sequence: 04, Frame: 752/900
Sequence: 04, Frame: 757/900
Sequence: 04, Frame: 762/900
Sequence: 04, Frame: 767/900
Sequence: 04, Frame: 772/900
Sequence: 04, Frame: 777/900
Sequence: 04, Frame: 782/900
Sequence: 04, Frame: 787/900
Sequence: 04, Frame: 792/900
Sequence: 04, Frame: 797/900
Sequence: 04, Frame: 802/900
Sequence: 04, Frame: 807/900
Sequence: 04, Frame: 812/900
Sequence: 04, Frame: 817/900
Sequence: 04, Frame: 822/900
Sequence: 04, Frame: 827/900
Sequence: 04, Frame: 832/900
Sequence: 04, Frame: 837/900
Sequence: 04, Frame: 842/900
Sequence: 04, Frame: 847/900
Sequence: 04, Frame: 852/900
Sequence: 04, Frame: 857/900
Sequence: 04, Frame: 862/900
Sequence: 04, Frame: 867/900
Sequence: 04, Frame: 872/900
Sequence: 04, Frame: 877/900
Sequence: 04, Frame: 882/900
Sequence: 04, Frame: 887/900
Sequence: 04, Frame: 892/900
Sequence: 04, Frame: 897/900
Data preparation finished
Sequence: 05
[[1.0000e+00 1.0000e+00 1.7000e+01 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [2.0000e+00 1.0000e+00 2.0000e+01 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [3.0000e+00 1.0000e+00 2.3000e+01 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 ...
 [7.1400e+02 1.5400e+02 4.2600e+02 ... 1.0000e+00 1.0000e+00 0.0000e+00]
 [8.3700e+02 1.5500e+02 6.0500e+02 ... 0.0000e+00 9.0000e+00 1.2704e-01]
 [8.3700e+02 1.5600e+02 3.0600e+02 ... 1.0000e+00 1.0000e+00 5.2174e-01]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-05-FRCNN/img1
Sequence: 05, Frame: 2/780
Sequence: 05, Frame: 7/780
Sequence: 05, Frame: 12/780
Sequence: 05, Frame: 17/780
Sequence: 05, Frame: 22/780
Sequence: 05, Frame: 27/780
Sequence: 05, Frame: 32/780
Sequence: 05, Frame: 37/780
Sequence: 05, Frame: 42/780
Sequence: 05, Frame: 47/780
Sequence: 05, Frame: 52/780
Sequence: 05, Frame: 57/780
Sequence: 05, Frame: 62/780
Sequence: 05, Frame: 67/780
Sequence: 05, Frame: 72/780
Sequence: 05, Frame: 77/780
Sequence: 05, Frame: 82/780
Sequence: 05, Frame: 87/780
Sequence: 05, Frame: 92/780
Sequence: 05, Frame: 97/780
Sequence: 05, Frame: 102/780
Sequence: 05, Frame: 107/780
Sequence: 05, Frame: 112/780
Sequence: 05, Frame: 117/780
Sequence: 05, Frame: 122/780
Sequence: 05, Frame: 127/780
Sequence: 05, Frame: 132/780
Sequence: 05, Frame: 137/780
Sequence: 05, Frame: 142/780
Sequence: 05, Frame: 147/780
Sequence: 05, Frame: 152/780
Sequence: 05, Frame: 157/780
Sequence: 05, Frame: 162/780
Sequence: 05, Frame: 167/780
Sequence: 05, Frame: 172/780
Sequence: 05, Frame: 177/780
Sequence: 05, Frame: 182/780
Sequence: 05, Frame: 187/780
Sequence: 05, Frame: 192/780
Sequence: 05, Frame: 197/780
Sequence: 05, Frame: 202/780
Sequence: 05, Frame: 207/780
Sequence: 05, Frame: 212/780
Sequence: 05, Frame: 217/780
Sequence: 05, Frame: 222/780
Sequence: 05, Frame: 227/780
Sequence: 05, Frame: 232/780
Sequence: 05, Frame: 237/780
Sequence: 05, Frame: 242/780
Sequence: 05, Frame: 247/780
Sequence: 05, Frame: 252/780
Sequence: 05, Frame: 257/780
Sequence: 05, Frame: 262/780
Sequence: 05, Frame: 267/780
Sequence: 05, Frame: 272/780
Sequence: 05, Frame: 277/780
Sequence: 05, Frame: 282/780
Sequence: 05, Frame: 287/780
Sequence: 05, Frame: 292/780
Sequence: 05, Frame: 297/780
Sequence: 05, Frame: 302/780
Sequence: 05, Frame: 307/780
Sequence: 05, Frame: 312/780
Sequence: 05, Frame: 317/780
Sequence: 05, Frame: 322/780
Sequence: 05, Frame: 327/780
Sequence: 05, Frame: 332/780
Sequence: 05, Frame: 337/780
Sequence: 05, Frame: 342/780
Sequence: 05, Frame: 347/780
Sequence: 05, Frame: 352/780
Sequence: 05, Frame: 357/780
Sequence: 05, Frame: 362/780
Sequence: 05, Frame: 367/780
Sequence: 05, Frame: 372/780
Sequence: 05, Frame: 377/780
Sequence: 05, Frame: 382/780
Sequence: 05, Frame: 387/780
Sequence: 05, Frame: 392/780
Sequence: 05, Frame: 397/780
Sequence: 05, Frame: 402/780
Sequence: 05, Frame: 407/780
Sequence: 05, Frame: 412/780
Sequence: 05, Frame: 417/780
Sequence: 05, Frame: 422/780
Sequence: 05, Frame: 427/780
Sequence: 05, Frame: 432/780
Sequence: 05, Frame: 437/780
Sequence: 05, Frame: 442/780
Sequence: 05, Frame: 447/780
Sequence: 05, Frame: 452/780
Sequence: 05, Frame: 457/780
Sequence: 05, Frame: 462/780
Sequence: 05, Frame: 467/780
Sequence: 05, Frame: 472/780
Sequence: 05, Frame: 477/780
Sequence: 05, Frame: 482/780
Sequence: 05, Frame: 487/780
Sequence: 05, Frame: 492/780
Sequence: 05, Frame: 497/780
Sequence: 05, Frame: 502/780
Sequence: 05, Frame: 507/780
Sequence: 05, Frame: 512/780
Sequence: 05, Frame: 517/780
Sequence: 05, Frame: 522/780
Sequence: 05, Frame: 527/780
Sequence: 05, Frame: 532/780
Sequence: 05, Frame: 537/780
Sequence: 05, Frame: 542/780
Sequence: 05, Frame: 547/780
Sequence: 05, Frame: 552/780
Sequence: 05, Frame: 557/780
Sequence: 05, Frame: 562/780
Sequence: 05, Frame: 567/780
Sequence: 05, Frame: 572/780
Sequence: 05, Frame: 577/780
Sequence: 05, Frame: 582/780
Sequence: 05, Frame: 587/780
Sequence: 05, Frame: 592/780
Sequence: 05, Frame: 597/780
Sequence: 05, Frame: 602/780
Sequence: 05, Frame: 607/780
Sequence: 05, Frame: 612/780
Sequence: 05, Frame: 617/780
Sequence: 05, Frame: 622/780
Sequence: 05, Frame: 627/780
Sequence: 05, Frame: 632/780
Sequence: 05, Frame: 637/780
Sequence: 05, Frame: 642/780
Sequence: 05, Frame: 647/780
Sequence: 05, Frame: 652/780
Sequence: 05, Frame: 657/780
Sequence: 05, Frame: 662/780
Sequence: 05, Frame: 667/780
Sequence: 05, Frame: 672/780
Sequence: 05, Frame: 677/780
Sequence: 05, Frame: 682/780
Sequence: 05, Frame: 687/780
Sequence: 05, Frame: 692/780
Sequence: 05, Frame: 697/780
Sequence: 05, Frame: 702/780
Sequence: 05, Frame: 707/780
Sequence: 05, Frame: 712/780
Sequence: 05, Frame: 717/780
Sequence: 05, Frame: 722/780
Sequence: 05, Frame: 727/780
Sequence: 05, Frame: 732/780
Sequence: 05, Frame: 737/780
Sequence: 05, Frame: 742/780
Sequence: 05, Frame: 747/780
Sequence: 05, Frame: 752/780
Sequence: 05, Frame: 757/780
Sequence: 05, Frame: 762/780
Sequence: 05, Frame: 767/780
Sequence: 05, Frame: 772/780
Sequence: 05, Frame: 777/780
Data preparation finished
Sequence: 09
[[1.0000e+00 1.0000e+00 2.6000e+02 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [2.0000e+00 1.0000e+00 2.6200e+02 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [3.0000e+00 1.0000e+00 2.6400e+02 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 ...
 [2.0900e+02 6.4000e+01 8.8300e+02 ... 0.0000e+00 1.2000e+01 4.4444e-01]
 [2.1000e+02 6.4000e+01 8.8400e+02 ... 0.0000e+00 1.2000e+01 5.8333e-01]
 [2.1100e+02 6.4000e+01 8.8500e+02 ... 0.0000e+00 1.2000e+01 5.8333e-01]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-09-FRCNN/img1
Sequence: 09, Frame: 2/450
Sequence: 09, Frame: 7/450
Sequence: 09, Frame: 12/450
Sequence: 09, Frame: 17/450
Sequence: 09, Frame: 22/450
Sequence: 09, Frame: 27/450
Sequence: 09, Frame: 32/450
Sequence: 09, Frame: 37/450
Sequence: 09, Frame: 42/450
Sequence: 09, Frame: 47/450
Sequence: 09, Frame: 52/450
Sequence: 09, Frame: 57/450
Sequence: 09, Frame: 62/450
Sequence: 09, Frame: 67/450
Sequence: 09, Frame: 72/450
Sequence: 09, Frame: 77/450
Sequence: 09, Frame: 82/450
Sequence: 09, Frame: 87/450
Sequence: 09, Frame: 92/450
Sequence: 09, Frame: 97/450
Sequence: 09, Frame: 102/450
Sequence: 09, Frame: 107/450
Sequence: 09, Frame: 112/450
Sequence: 09, Frame: 117/450
Sequence: 09, Frame: 122/450
Sequence: 09, Frame: 127/450
Sequence: 09, Frame: 132/450
Sequence: 09, Frame: 137/450
Sequence: 09, Frame: 142/450
Sequence: 09, Frame: 147/450
Sequence: 09, Frame: 152/450
Sequence: 09, Frame: 157/450
Sequence: 09, Frame: 162/450
Sequence: 09, Frame: 167/450
Sequence: 09, Frame: 172/450
Sequence: 09, Frame: 177/450
Sequence: 09, Frame: 182/450
Sequence: 09, Frame: 187/450
Sequence: 09, Frame: 192/450
Sequence: 09, Frame: 197/450
Sequence: 09, Frame: 202/450
Sequence: 09, Frame: 207/450
Sequence: 09, Frame: 212/450
Sequence: 09, Frame: 217/450
Sequence: 09, Frame: 222/450
Sequence: 09, Frame: 227/450
Sequence: 09, Frame: 232/450
Sequence: 09, Frame: 237/450
Sequence: 09, Frame: 242/450
Sequence: 09, Frame: 247/450
Sequence: 09, Frame: 252/450
Sequence: 09, Frame: 257/450
Sequence: 09, Frame: 262/450
Sequence: 09, Frame: 267/450
Sequence: 09, Frame: 272/450
Sequence: 09, Frame: 277/450
Sequence: 09, Frame: 282/450
Sequence: 09, Frame: 287/450
Sequence: 09, Frame: 292/450
Sequence: 09, Frame: 297/450
Sequence: 09, Frame: 302/450
Sequence: 09, Frame: 307/450
Sequence: 09, Frame: 312/450
Sequence: 09, Frame: 317/450
Sequence: 09, Frame: 322/450
Sequence: 09, Frame: 327/450
Sequence: 09, Frame: 332/450
Sequence: 09, Frame: 337/450
Sequence: 09, Frame: 342/450
Sequence: 09, Frame: 347/450
Sequence: 09, Frame: 352/450
Sequence: 09, Frame: 357/450
Sequence: 09, Frame: 362/450
Sequence: 09, Frame: 367/450
Sequence: 09, Frame: 372/450
Sequence: 09, Frame: 377/450
Sequence: 09, Frame: 382/450
Sequence: 09, Frame: 387/450
Sequence: 09, Frame: 392/450
Sequence: 09, Frame: 397/450
Sequence: 09, Frame: 402/450
Sequence: 09, Frame: 407/450
Sequence: 09, Frame: 412/450
Sequence: 09, Frame: 417/450
Sequence: 09, Frame: 422/450
Sequence: 09, Frame: 427/450
Sequence: 09, Frame: 432/450
Sequence: 09, Frame: 437/450
Sequence: 09, Frame: 442/450
Sequence: 09, Frame: 447/450
Data preparation finished
Sequence: 10
[[1.0000e+00 1.0000e+00 1.3680e+03 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [2.0000e+00 1.0000e+00 1.3660e+03 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [3.0000e+00 1.0000e+00 1.3650e+03 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 ...
 [6.5200e+02 7.3000e+01 7.8400e+02 ... 1.0000e+00 1.0000e+00 1.0000e+00]
 [6.5300e+02 7.3000e+01 7.8800e+02 ... 1.0000e+00 1.0000e+00 8.3013e-01]
 [6.5400e+02 7.3000e+01 7.9200e+02 ... 1.0000e+00 1.0000e+00 8.3418e-01]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-10-FRCNN/img1
Sequence: 10, Frame: 2/550
Sequence: 10, Frame: 7/550
Sequence: 10, Frame: 12/550
Sequence: 10, Frame: 17/550
Sequence: 10, Frame: 22/550
Sequence: 10, Frame: 27/550
Sequence: 10, Frame: 32/550
Sequence: 10, Frame: 37/550
Sequence: 10, Frame: 42/550
Sequence: 10, Frame: 47/550
Sequence: 10, Frame: 52/550
Sequence: 10, Frame: 57/550
Sequence: 10, Frame: 62/550
Sequence: 10, Frame: 67/550
Sequence: 10, Frame: 72/550
Sequence: 10, Frame: 77/550
Sequence: 10, Frame: 82/550
Sequence: 10, Frame: 87/550
Sequence: 10, Frame: 92/550
Sequence: 10, Frame: 97/550
Sequence: 10, Frame: 102/550
Sequence: 10, Frame: 107/550
Sequence: 10, Frame: 112/550
Sequence: 10, Frame: 117/550
Sequence: 10, Frame: 122/550
Sequence: 10, Frame: 127/550
Sequence: 10, Frame: 132/550
Sequence: 10, Frame: 137/550
Sequence: 10, Frame: 142/550
Sequence: 10, Frame: 147/550
Sequence: 10, Frame: 152/550
Sequence: 10, Frame: 157/550
Sequence: 10, Frame: 162/550
Sequence: 10, Frame: 167/550
Sequence: 10, Frame: 172/550
Sequence: 10, Frame: 177/550
Sequence: 10, Frame: 182/550
Sequence: 10, Frame: 187/550
Sequence: 10, Frame: 192/550
Sequence: 10, Frame: 197/550
Sequence: 10, Frame: 202/550
Sequence: 10, Frame: 207/550
Sequence: 10, Frame: 212/550
Sequence: 10, Frame: 217/550
Sequence: 10, Frame: 222/550
Sequence: 10, Frame: 227/550
Sequence: 10, Frame: 232/550
Sequence: 10, Frame: 237/550
Sequence: 10, Frame: 242/550
Sequence: 10, Frame: 247/550
Sequence: 10, Frame: 252/550
Sequence: 10, Frame: 257/550
Sequence: 10, Frame: 262/550
Sequence: 10, Frame: 267/550
Sequence: 10, Frame: 272/550
Sequence: 10, Frame: 277/550
Sequence: 10, Frame: 282/550
Sequence: 10, Frame: 287/550
Sequence: 10, Frame: 292/550
Sequence: 10, Frame: 297/550
Sequence: 10, Frame: 302/550
Sequence: 10, Frame: 307/550
Sequence: 10, Frame: 312/550
Sequence: 10, Frame: 317/550
Sequence: 10, Frame: 322/550
Sequence: 10, Frame: 327/550
Sequence: 10, Frame: 332/550
Sequence: 10, Frame: 337/550
Sequence: 10, Frame: 342/550
Sequence: 10, Frame: 347/550
Sequence: 10, Frame: 352/550
Sequence: 10, Frame: 357/550
Sequence: 10, Frame: 362/550
Sequence: 10, Frame: 367/550
Sequence: 10, Frame: 372/550
Sequence: 10, Frame: 377/550
Sequence: 10, Frame: 382/550
Sequence: 10, Frame: 387/550
Sequence: 10, Frame: 392/550
Sequence: 10, Frame: 397/550
Sequence: 10, Frame: 402/550
Sequence: 10, Frame: 407/550
Sequence: 10, Frame: 412/550
Sequence: 10, Frame: 417/550
Sequence: 10, Frame: 422/550
Sequence: 10, Frame: 427/550
Sequence: 10, Frame: 432/550
Sequence: 10, Frame: 437/550
Sequence: 10, Frame: 442/550
Sequence: 10, Frame: 447/550
Sequence: 10, Frame: 452/550
Sequence: 10, Frame: 457/550
Sequence: 10, Frame: 462/550
Sequence: 10, Frame: 467/550
Sequence: 10, Frame: 472/550
Sequence: 10, Frame: 477/550
Sequence: 10, Frame: 482/550
Sequence: 10, Frame: 487/550
Sequence: 10, Frame: 492/550
Sequence: 10, Frame: 497/550
Sequence: 10, Frame: 502/550
Sequence: 10, Frame: 507/550
Sequence: 10, Frame: 512/550
Sequence: 10, Frame: 517/550
Sequence: 10, Frame: 522/550
Sequence: 10, Frame: 527/550
Sequence: 10, Frame: 532/550
Sequence: 10, Frame: 537/550
Sequence: 10, Frame: 542/550
Sequence: 10, Frame: 547/550
Data preparation finished
Sequence: 11
[[ 1.0000e+00  1.0000e+00  8.6700e+02 ...  1.0000e+00  1.0000e+00
   1.0000e+00]
 [ 2.0000e+00  1.0000e+00  8.7000e+02 ...  1.0000e+00  1.0000e+00
   1.0000e+00]
 [ 3.0000e+00  1.0000e+00  8.7300e+02 ...  1.0000e+00  1.0000e+00
   1.0000e+00]
 ...
 [ 3.2600e+02  8.9000e+01 -3.0800e+02 ...  1.0000e+00  1.0000e+00
   3.3247e-01]
 [ 9.0000e+02  9.0000e+01  1.3510e+03 ...  1.0000e+00  1.0000e+00
   1.0000e+00]
 [ 9.0000e+02  9.1000e+01  6.9400e+02 ...  1.0000e+00  1.0000e+00
   0.0000e+00]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-11-FRCNN/img1
Sequence: 11, Frame: 2/800
Sequence: 11, Frame: 7/800
Sequence: 11, Frame: 12/800
Sequence: 11, Frame: 17/800
Sequence: 11, Frame: 22/800
Sequence: 11, Frame: 27/800
Sequence: 11, Frame: 32/800
Sequence: 11, Frame: 37/800
Sequence: 11, Frame: 42/800
Sequence: 11, Frame: 47/800
Sequence: 11, Frame: 52/800
Sequence: 11, Frame: 57/800
Sequence: 11, Frame: 62/800
Sequence: 11, Frame: 67/800
Sequence: 11, Frame: 72/800
Sequence: 11, Frame: 77/800
Sequence: 11, Frame: 82/800
Sequence: 11, Frame: 87/800
Sequence: 11, Frame: 92/800
Sequence: 11, Frame: 97/800
Sequence: 11, Frame: 102/800
Sequence: 11, Frame: 107/800
Sequence: 11, Frame: 112/800
Sequence: 11, Frame: 117/800
Sequence: 11, Frame: 122/800
Sequence: 11, Frame: 127/800
Sequence: 11, Frame: 132/800
Sequence: 11, Frame: 137/800
Sequence: 11, Frame: 142/800
Sequence: 11, Frame: 147/800
Sequence: 11, Frame: 152/800
Sequence: 11, Frame: 157/800
Sequence: 11, Frame: 162/800
Sequence: 11, Frame: 167/800
Sequence: 11, Frame: 172/800
Sequence: 11, Frame: 177/800
Sequence: 11, Frame: 182/800
Sequence: 11, Frame: 187/800
Sequence: 11, Frame: 192/800
Sequence: 11, Frame: 197/800
Sequence: 11, Frame: 202/800
Sequence: 11, Frame: 207/800
Sequence: 11, Frame: 212/800
Sequence: 11, Frame: 217/800
Sequence: 11, Frame: 222/800
Sequence: 11, Frame: 227/800
Sequence: 11, Frame: 232/800
Sequence: 11, Frame: 237/800
Sequence: 11, Frame: 242/800
Sequence: 11, Frame: 247/800
Sequence: 11, Frame: 252/800
Sequence: 11, Frame: 257/800
Sequence: 11, Frame: 262/800
Sequence: 11, Frame: 267/800
Sequence: 11, Frame: 272/800
Sequence: 11, Frame: 277/800
Sequence: 11, Frame: 282/800
Sequence: 11, Frame: 287/800
Sequence: 11, Frame: 292/800
Sequence: 11, Frame: 297/800
Sequence: 11, Frame: 302/800
Sequence: 11, Frame: 307/800
Sequence: 11, Frame: 312/800
Sequence: 11, Frame: 317/800
Sequence: 11, Frame: 322/800
Sequence: 11, Frame: 327/800
Sequence: 11, Frame: 332/800
Sequence: 11, Frame: 337/800
Sequence: 11, Frame: 342/800
Sequence: 11, Frame: 347/800
Sequence: 11, Frame: 352/800
Sequence: 11, Frame: 357/800
Sequence: 11, Frame: 362/800
Sequence: 11, Frame: 367/800
Sequence: 11, Frame: 372/800
Sequence: 11, Frame: 377/800
Sequence: 11, Frame: 382/800
Sequence: 11, Frame: 387/800
Sequence: 11, Frame: 392/800
Sequence: 11, Frame: 397/800
Sequence: 11, Frame: 402/800
Sequence: 11, Frame: 407/800
Sequence: 11, Frame: 412/800
Sequence: 11, Frame: 417/800
Sequence: 11, Frame: 422/800
Sequence: 11, Frame: 427/800
Sequence: 11, Frame: 432/800
Sequence: 11, Frame: 437/800
Sequence: 11, Frame: 442/800
Sequence: 11, Frame: 447/800
Sequence: 11, Frame: 452/800
Sequence: 11, Frame: 457/800
Sequence: 11, Frame: 462/800
Sequence: 11, Frame: 467/800
Sequence: 11, Frame: 472/800
Sequence: 11, Frame: 477/800
Sequence: 11, Frame: 482/800
Sequence: 11, Frame: 487/800
Sequence: 11, Frame: 492/800
Sequence: 11, Frame: 497/800
Sequence: 11, Frame: 502/800
Sequence: 11, Frame: 507/800
Sequence: 11, Frame: 512/800
Sequence: 11, Frame: 517/800
Sequence: 11, Frame: 522/800
Sequence: 11, Frame: 527/800
Sequence: 11, Frame: 532/800
Sequence: 11, Frame: 537/800
Sequence: 11, Frame: 542/800
Sequence: 11, Frame: 547/800
Sequence: 11, Frame: 552/800
Sequence: 11, Frame: 557/800
Sequence: 11, Frame: 562/800
Sequence: 11, Frame: 567/800
Sequence: 11, Frame: 572/800
Sequence: 11, Frame: 577/800
Sequence: 11, Frame: 582/800
Sequence: 11, Frame: 587/800
Sequence: 11, Frame: 592/800
Sequence: 11, Frame: 597/800
Sequence: 11, Frame: 602/800
Sequence: 11, Frame: 607/800
Sequence: 11, Frame: 612/800
Sequence: 11, Frame: 617/800
Sequence: 11, Frame: 622/800
Sequence: 11, Frame: 627/800
Sequence: 11, Frame: 632/800
Sequence: 11, Frame: 637/800
Sequence: 11, Frame: 642/800
Sequence: 11, Frame: 647/800
Sequence: 11, Frame: 652/800
Sequence: 11, Frame: 657/800
Sequence: 11, Frame: 662/800
Sequence: 11, Frame: 667/800
Sequence: 11, Frame: 672/800
Sequence: 11, Frame: 677/800
Sequence: 11, Frame: 682/800
Sequence: 11, Frame: 687/800
Sequence: 11, Frame: 692/800
Sequence: 11, Frame: 697/800
Sequence: 11, Frame: 702/800
Sequence: 11, Frame: 707/800
Sequence: 11, Frame: 712/800
Sequence: 11, Frame: 717/800
Sequence: 11, Frame: 722/800
Sequence: 11, Frame: 727/800
Sequence: 11, Frame: 732/800
Sequence: 11, Frame: 737/800
Sequence: 11, Frame: 742/800
Sequence: 11, Frame: 747/800
Sequence: 11, Frame: 752/800
Sequence: 11, Frame: 757/800
Sequence: 11, Frame: 762/800
Sequence: 11, Frame: 767/800
Sequence: 11, Frame: 772/800
Sequence: 11, Frame: 777/800
Sequence: 11, Frame: 782/800
Sequence: 11, Frame: 787/800
Sequence: 11, Frame: 792/800
Sequence: 11, Frame: 797/800
Data preparation finished
Sequence: 13
[[1.0000e+00 1.0000e+00 1.3760e+03 ... 0.0000e+00 1.1000e+01 1.0000e+00]
 [2.0000e+00 1.0000e+00 1.3790e+03 ... 0.0000e+00 1.1000e+01 1.0000e+00]
 [3.0000e+00 1.0000e+00 1.3820e+03 ... 0.0000e+00 1.1000e+01 1.0000e+00]
 ...
 [7.5000e+02 1.8600e+02 1.7900e+03 ... 0.0000e+00 1.0000e+01 1.7287e-01]
 [7.5000e+02 1.8700e+02 2.4800e+02 ... 0.0000e+00 1.2000e+01 1.0000e+00]
 [7.5000e+02 1.8800e+02 9.9300e+02 ... 0.0000e+00 3.0000e+00 1.0000e+00]] /home/Galoaa.b/ondemand/data/MOT17/train/MOT17-13-FRCNN/img1
Sequence: 13, Frame: 2/650
Sequence: 13, Frame: 7/650
Sequence: 13, Frame: 12/650
Sequence: 13, Frame: 17/650
Sequence: 13, Frame: 22/650
Sequence: 13, Frame: 27/650
Sequence: 13, Frame: 32/650
Sequence: 13, Frame: 37/650
Sequence: 13, Frame: 42/650
Sequence: 13, Frame: 47/650
Sequence: 13, Frame: 52/650
Sequence: 13, Frame: 57/650
Sequence: 13, Frame: 62/650
Sequence: 13, Frame: 67/650
Sequence: 13, Frame: 72/650
Sequence: 13, Frame: 77/650
Sequence: 13, Frame: 82/650
Sequence: 13, Frame: 87/650
Sequence: 13, Frame: 92/650
Sequence: 13, Frame: 97/650
Sequence: 13, Frame: 102/650
Sequence: 13, Frame: 107/650
Sequence: 13, Frame: 112/650
Sequence: 13, Frame: 117/650
Sequence: 13, Frame: 122/650
Sequence: 13, Frame: 127/650
Sequence: 13, Frame: 132/650
Sequence: 13, Frame: 137/650
Sequence: 13, Frame: 142/650
Sequence: 13, Frame: 147/650
Sequence: 13, Frame: 152/650
Sequence: 13, Frame: 157/650
Sequence: 13, Frame: 162/650
Sequence: 13, Frame: 167/650
Sequence: 13, Frame: 172/650
Sequence: 13, Frame: 177/650
Sequence: 13, Frame: 182/650
Sequence: 13, Frame: 187/650
Sequence: 13, Frame: 192/650
Sequence: 13, Frame: 197/650
Sequence: 13, Frame: 202/650
Sequence: 13, Frame: 207/650
Sequence: 13, Frame: 212/650
Sequence: 13, Frame: 217/650
Sequence: 13, Frame: 222/650
Sequence: 13, Frame: 227/650
Sequence: 13, Frame: 232/650
Sequence: 13, Frame: 237/650
Sequence: 13, Frame: 242/650
Sequence: 13, Frame: 247/650
Sequence: 13, Frame: 252/650
Sequence: 13, Frame: 257/650
Sequence: 13, Frame: 262/650
Sequence: 13, Frame: 267/650
Sequence: 13, Frame: 272/650
Sequence: 13, Frame: 277/650
Sequence: 13, Frame: 282/650
Sequence: 13, Frame: 287/650
Sequence: 13, Frame: 292/650
Sequence: 13, Frame: 297/650
Sequence: 13, Frame: 302/650
Sequence: 13, Frame: 307/650
Sequence: 13, Frame: 312/650
Sequence: 13, Frame: 317/650
Sequence: 13, Frame: 322/650
Sequence: 13, Frame: 327/650
Sequence: 13, Frame: 332/650
Sequence: 13, Frame: 337/650
Sequence: 13, Frame: 342/650
Sequence: 13, Frame: 347/650
Sequence: 13, Frame: 352/650
Sequence: 13, Frame: 357/650
Sequence: 13, Frame: 362/650
Sequence: 13, Frame: 367/650
Sequence: 13, Frame: 372/650
Sequence: 13, Frame: 377/650
Sequence: 13, Frame: 382/650
Sequence: 13, Frame: 387/650
Sequence: 13, Frame: 392/650
Sequence: 13, Frame: 397/650
Sequence: 13, Frame: 402/650
Sequence: 13, Frame: 407/650
Sequence: 13, Frame: 412/650
Sequence: 13, Frame: 417/650
Sequence: 13, Frame: 422/650
Sequence: 13, Frame: 427/650
Sequence: 13, Frame: 432/650
Sequence: 13, Frame: 437/650
Sequence: 13, Frame: 442/650
Sequence: 13, Frame: 447/650
Sequence: 13, Frame: 452/650
Sequence: 13, Frame: 457/650
Sequence: 13, Frame: 462/650
Sequence: 13, Frame: 467/650
Sequence: 13, Frame: 472/650
Sequence: 13, Frame: 477/650
Sequence: 13, Frame: 482/650
Sequence: 13, Frame: 487/650
Sequence: 13, Frame: 492/650
Sequence: 13, Frame: 497/650
Sequence: 13, Frame: 502/650
Sequence: 13, Frame: 507/650
Sequence: 13, Frame: 512/650
Sequence: 13, Frame: 517/650
Sequence: 13, Frame: 522/650
Sequence: 13, Frame: 527/650
Sequence: 13, Frame: 532/650
Sequence: 13, Frame: 537/650
Sequence: 13, Frame: 542/650
Sequence: 13, Frame: 547/650
Sequence: 13, Frame: 552/650
Sequence: 13, Frame: 557/650
Sequence: 13, Frame: 562/650
Sequence: 13, Frame: 567/650
Sequence: 13, Frame: 572/650
Sequence: 13, Frame: 577/650
Sequence: 13, Frame: 582/650
Sequence: 13, Frame: 587/650
Sequence: 13, Frame: 592/650
Sequence: 13, Frame: 597/650
Sequence: 13, Frame: 602/650
Sequence: 13, Frame: 607/650
Sequence: 13, Frame: 612/650
Sequence: 13, Frame: 617/650
Sequence: 13, Frame: 622/650
Sequence: 13, Frame: 627/650
Sequence: 13, Frame: 632/650
Sequence: 13, Frame: 637/650
Sequence: 13, Frame: 642/650
Sequence: 13, Frame: 647/650
Data preparation finished
Saved to pickle file 

Validation Data
Sequence: 02
Sequence: 02, Frame: 500/600
Sequence: 02, Frame: 505/600
Sequence: 02, Frame: 510/600
Sequence: 02, Frame: 515/600
Sequence: 02, Frame: 520/600
Sequence: 02, Frame: 525/600
Sequence: 02, Frame: 530/600
Sequence: 02, Frame: 535/600
Sequence: 02, Frame: 540/600
Sequence: 02, Frame: 545/600
Sequence: 02, Frame: 550/600
Sequence: 02, Frame: 555/600
Sequence: 02, Frame: 560/600
Sequence: 02, Frame: 565/600
Sequence: 02, Frame: 570/600
Sequence: 02, Frame: 575/600
Sequence: 02, Frame: 580/600
Sequence: 02, Frame: 585/600
Sequence: 02, Frame: 590/600
Sequence: 02, Frame: 595/600
Sequence: 02, Frame: 600/600
Data preparation finished
Sequence: 04
Sequence: 04, Frame: 900/1050
Sequence: 04, Frame: 905/1050
Sequence: 04, Frame: 910/1050
Sequence: 04, Frame: 915/1050
Sequence: 04, Frame: 920/1050
Sequence: 04, Frame: 925/1050
Sequence: 04, Frame: 930/1050
Sequence: 04, Frame: 935/1050
Sequence: 04, Frame: 940/1050
Sequence: 04, Frame: 945/1050
Sequence: 04, Frame: 950/1050
Sequence: 04, Frame: 955/1050
Sequence: 04, Frame: 960/1050
Sequence: 04, Frame: 965/1050
Sequence: 04, Frame: 970/1050
Sequence: 04, Frame: 975/1050
Sequence: 04, Frame: 980/1050
Sequence: 04, Frame: 985/1050
Sequence: 04, Frame: 990/1050
Sequence: 04, Frame: 995/1050
Sequence: 04, Frame: 1000/1050
Sequence: 04, Frame: 1005/1050
Sequence: 04, Frame: 1010/1050
Sequence: 04, Frame: 1015/1050
Sequence: 04, Frame: 1020/1050
Sequence: 04, Frame: 1025/1050
Sequence: 04, Frame: 1030/1050
Sequence: 04, Frame: 1035/1050
Sequence: 04, Frame: 1040/1050
Sequence: 04, Frame: 1045/1050
Sequence: 04, Frame: 1050/1050
Data preparation finished
Sequence: 05
Sequence: 05, Frame: 780/837
Sequence: 05, Frame: 785/837
Sequence: 05, Frame: 790/837
Sequence: 05, Frame: 795/837
Sequence: 05, Frame: 800/837
Sequence: 05, Frame: 805/837
Sequence: 05, Frame: 810/837
Sequence: 05, Frame: 815/837
Sequence: 05, Frame: 820/837
Sequence: 05, Frame: 825/837
Sequence: 05, Frame: 830/837
Sequence: 05, Frame: 835/837
Data preparation finished
Sequence: 09
Sequence: 09, Frame: 450/525
Sequence: 09, Frame: 455/525
Sequence: 09, Frame: 460/525
Sequence: 09, Frame: 465/525
Sequence: 09, Frame: 470/525
Sequence: 09, Frame: 475/525
Sequence: 09, Frame: 480/525
Sequence: 09, Frame: 485/525
Sequence: 09, Frame: 490/525
Sequence: 09, Frame: 495/525
Sequence: 09, Frame: 500/525
Sequence: 09, Frame: 505/525
Sequence: 09, Frame: 510/525
Sequence: 09, Frame: 515/525
Sequence: 09, Frame: 520/525
Sequence: 09, Frame: 525/525
Data preparation finished
Sequence: 10
Sequence: 10, Frame: 550/654
Sequence: 10, Frame: 555/654
Sequence: 10, Frame: 560/654
Sequence: 10, Frame: 565/654
Sequence: 10, Frame: 570/654
Sequence: 10, Frame: 575/654
Sequence: 10, Frame: 580/654
Sequence: 10, Frame: 585/654
Sequence: 10, Frame: 590/654
Sequence: 10, Frame: 595/654
Sequence: 10, Frame: 600/654
Sequence: 10, Frame: 605/654
Sequence: 10, Frame: 610/654
Sequence: 10, Frame: 615/654
Sequence: 10, Frame: 620/654
Sequence: 10, Frame: 625/654
Sequence: 10, Frame: 630/654
Sequence: 10, Frame: 635/654
Sequence: 10, Frame: 640/654
Sequence: 10, Frame: 645/654
Sequence: 10, Frame: 650/654
Data preparation finished
Sequence: 11
Sequence: 11, Frame: 800/900
Sequence: 11, Frame: 805/900
Sequence: 11, Frame: 810/900
Sequence: 11, Frame: 815/900
Sequence: 11, Frame: 820/900
Sequence: 11, Frame: 825/900
Sequence: 11, Frame: 830/900
Sequence: 11, Frame: 835/900
Sequence: 11, Frame: 840/900
Sequence: 11, Frame: 845/900
Sequence: 11, Frame: 850/900
Sequence: 11, Frame: 855/900
Sequence: 11, Frame: 860/900
Sequence: 11, Frame: 865/900
Sequence: 11, Frame: 870/900
Sequence: 11, Frame: 875/900
Sequence: 11, Frame: 880/900
Sequence: 11, Frame: 885/900
Sequence: 11, Frame: 890/900
Sequence: 11, Frame: 895/900
Sequence: 11, Frame: 900/900
Data preparation finished
Sequence: 13
Sequence: 13, Frame: 650/750
Sequence: 13, Frame: 655/750
Sequence: 13, Frame: 660/750
Sequence: 13, Frame: 665/750
Sequence: 13, Frame: 670/750
Sequence: 13, Frame: 675/750
Sequence: 13, Frame: 680/750
Sequence: 13, Frame: 685/750
Sequence: 13, Frame: 690/750
Sequence: 13, Frame: 695/750
Sequence: 13, Frame: 700/750
Sequence: 13, Frame: 705/750
Sequence: 13, Frame: 710/750
Sequence: 13, Frame: 715/750
Sequence: 13, Frame: 720/750
Sequence: 13, Frame: 725/750
Sequence: 13, Frame: 730/750
Sequence: 13, Frame: 735/750
Sequence: 13, Frame: 740/750
Sequence: 13, Frame: 745/750
Sequence: 13, Frame: 750/750
Data preparation finished
Saved to pickle file 

Epoch: [1] | Batch: [1] | Training_Loss: 2.274 |
Epoch: [1] | Batch: [2] | Training_Loss: 2.459 |
Epoch: [1] | Batch: [3] | Training_Loss: 2.806 |
Epoch: [1] | Batch: [4] | Training_Loss: 1.797 |
Epoch: [1] | Batch: [5] | Training_Loss: 2.349 |
Epoch: [1] | Batch: [6] | Training_Loss: 1.568 |
Epoch: [1] | Batch: [7] | Training_Loss: 2.147 |
Epoch: [1] | Batch: [8] | Training_Loss: 2.197 |
Epoch: [1] | Batch: [9] | Training_Loss: 1.978 |
Epoch: [1] | Batch: [10] | Training_Loss: 2.424 |
Epoch: [1] | Batch: [11] | Training_Loss: 2.203 |
Epoch: [1] | Batch: [12] | Training_Loss: 1.010 |
Epoch: [1] | Batch: [13] | Training_Loss: 1.804 |
Epoch: [1] | Batch: [14] | Training_Loss: 1.177 |
Epoch: [1] | Batch: [15] | Training_Loss: 2.688 |
Epoch: [1] | Batch: [16] | Training_Loss: 1.786 |
Epoch: [1] | Batch: [17] | Training_Loss: 2.155 |
Epoch: [1] | Batch: [18] | Training_Loss: 2.654 |
Epoch: [1] | Batch: [19] | Training_Loss: 2.818 |
Epoch: [1] | Batch: [20] | Training_Loss: 2.095 |
Epoch: [1] | Batch: [21] | Training_Loss: 3.229 |
Epoch: [1] | Batch: [22] | Training_Loss: 1.639 |
Epoch: [1] | Batch: [23] | Training_Loss: 1.059 |
Epoch: [1] | Batch: [24] | Training_Loss: 1.588 |
Epoch: [1] | Batch: [25] | Training_Loss: 2.093 |
Epoch: [1] | Batch: [26] | Training_Loss: 1.569 |
Epoch: [1] | Batch: [27] | Training_Loss: 2.033 |
Epoch: [1] | Batch: [28] | Training_Loss: 1.028 |
Epoch: [1] | Batch: [29] | Training_Loss: 2.299 |
Epoch: [1] | Batch: [30] | Training_Loss: 1.848 |
Epoch: [1] | Batch: [31] | Training_Loss: 2.219 |
Epoch: [1] | Batch: [32] | Training_Loss: 2.166 |
Epoch: [1] | Batch: [33] | Training_Loss: 1.818 |
Epoch: [1] | Batch: [34] | Training_Loss: 1.848 |
Epoch: [1] | Batch: [35] | Training_Loss: 2.114 |
Epoch: [1] | Batch: [36] | Training_Loss: 3.146 |
Epoch: [1] | Batch: [37] | Training_Loss: 1.728 |
Epoch: [1] | Batch: [38] | Training_Loss: 1.552 |
Epoch: [1] | Batch: [39] | Training_Loss: 2.464 |
Epoch: [1] | Batch: [40] | Training_Loss: 0.937 |
Epoch: [1] | Batch: [41] | Training_Loss: 2.001 |
Epoch: [1] | Batch: [42] | Training_Loss: 1.800 |
Epoch: [1] | Batch: [43] | Training_Loss: 1.009 |
Epoch: [1] | Batch: [44] | Training_Loss: 1.064 |
Epoch: [1] | Batch: [45] | Training_Loss: 1.039 |
Epoch: [1] | Batch: [46] | Training_Loss: 2.591 |
Epoch: [1] | Batch: [47] | Training_Loss: 3.070 |
Epoch: [1] | Batch: [48] | Training_Loss: 1.823 |
Epoch: [1] | Batch: [49] | Training_Loss: 2.120 |
Epoch: [1] | Batch: [50] | Training_Loss: 1.484 |
Epoch: [1] | Batch: [51] | Training_Loss: 3.585 |
Epoch: [1] | Batch: [52] | Training_Loss: 2.166 |
Epoch: [1] | Batch: [53] | Training_Loss: 4.154 |
Epoch: [1] | Batch: [54] | Training_Loss: 1.640 |
Epoch: [1] | Batch: [55] | Training_Loss: 1.617 |
Epoch: [1] | Batch: [56] | Training_Loss: 2.256 |
Epoch: [1] | Batch: [57] | Training_Loss: 1.479 |
Epoch: [1] | Batch: [58] | Training_Loss: 1.376 |
Epoch: [1] | Batch: [59] | Training_Loss: 1.222 |
Epoch: [1] | Batch: [60] | Training_Loss: 1.488 |
Epoch: [1] | Batch: [61] | Training_Loss: 2.804 |
Epoch: [1] | Batch: [62] | Training_Loss: 2.577 |
Epoch: [1] | Batch: [63] | Training_Loss: 1.364 |
Epoch: [1] | Batch: [64] | Training_Loss: 1.614 |
Epoch: [1] | Batch: [65] | Training_Loss: 2.590 |
Epoch: [1] | Batch: [66] | Training_Loss: 2.758 |
Epoch: [1] | Batch: [67] | Training_Loss: 2.312 |
Epoch: [1] | Batch: [68] | Training_Loss: 2.673 |
Epoch: [1] | Batch: [69] | Training_Loss: 1.034 |
Epoch: [1] | Batch: [70] | Training_Loss: 2.994 |
Epoch: [1] | Batch: [71] | Training_Loss: 3.211 |
Epoch: [1] | Batch: [72] | Training_Loss: 1.303 |
Epoch: [1] | Batch: [73] | Training_Loss: 1.450 |
Epoch: [1] | Batch: [74] | Training_Loss: 1.879 |
Epoch: [1] | Batch: [75] | Training_Loss: 2.067 |
Epoch: [1] | Batch: [76] | Training_Loss: 1.636 |
Epoch: [1] | Batch: [77] | Training_Loss: 1.733 |
Epoch: [1] | Batch: [78] | Training_Loss: 2.469 |
Epoch: [1] | Batch: [79] | Training_Loss: 1.352 |
Epoch: [1] | Batch: [80] | Training_Loss: 2.220 |
Epoch: [1] | Batch: [81] | Training_Loss: 1.851 |
Epoch: [1] | Batch: [82] | Training_Loss: 2.099 |
Epoch: [1] | Batch: [83] | Training_Loss: 2.408 |
Epoch: [1] | Batch: [84] | Training_Loss: 1.649 |
Epoch: [1] | Batch: [85] | Training_Loss: 1.455 |
Epoch: [1] | Batch: [86] | Training_Loss: 2.875 |
Epoch: [1] | Batch: [87] | Training_Loss: 1.261 |
Epoch: [1] | Batch: [88] | Training_Loss: 1.814 |
Epoch: [1] | Batch: [89] | Training_Loss: 2.911 |
Epoch: [1] | Batch: [90] | Training_Loss: 0.970 |
Epoch: [1] | Batch: [91] | Training_Loss: 1.845 |
Epoch: [1] | Batch: [92] | Training_Loss: 2.206 |
Epoch: [1] | Batch: [93] | Training_Loss: 2.691 |
Epoch: [1] | Batch: [94] | Training_Loss: 2.232 |
Epoch: [1] | Batch: [95] | Training_Loss: 2.394 |
Epoch: [1] | Batch: [96] | Training_Loss: 2.113 |
Epoch: [1] | Batch: [97] | Training_Loss: 1.445 |
Epoch: [1] | Batch: [98] | Training_Loss: 2.466 |
Epoch: [1] | Batch: [99] | Training_Loss: 3.126 |
Epoch: [1] | Batch: [100] | Training_Loss: 1.042 |
Epoch: [1] | Batch: [101] | Training_Loss: 2.496 |
Epoch: [1] | Batch: [102] | Training_Loss: 1.149 |
Epoch: [1] | Batch: [103] | Training_Loss: 1.291 |
Epoch: [1] | Batch: [104] | Training_Loss: 1.524 |
Epoch: [1] | Batch: [105] | Training_Loss: 1.925 |
Epoch: [1] | Batch: [106] | Training_Loss: 2.352 |
Epoch: [1] | Batch: [107] | Training_Loss: 1.625 |
Epoch: [1] | Batch: [108] | Training_Loss: 2.097 |
Epoch: [1] | Batch: [109] | Training_Loss: 0.839 |
Epoch: [1] | Batch: [110] | Training_Loss: 1.873 |
Epoch: [1] | Batch: [111] | Training_Loss: 1.833 |
Epoch: [1] | Batch: [112] | Training_Loss: 1.193 |
Epoch: [1] | Batch: [113] | Training_Loss: 2.712 |
Epoch: [1] | Batch: [114] | Training_Loss: 2.420 |
Epoch: [1] | Batch: [115] | Training_Loss: 1.238 |
Epoch: [1] | Batch: [116] | Training_Loss: 1.062 |
Epoch: [1] | Batch: [117] | Training_Loss: 2.172 |
Epoch: [1] | Batch: [118] | Training_Loss: 0.963 |
Epoch: [1] | Batch: [119] | Training_Loss: 1.630 |
Epoch: [1] | Batch: [120] | Training_Loss: 2.010 |
Epoch: [1] | Batch: [121] | Training_Loss: 1.998 |
Epoch: [1] | Batch: [122] | Training_Loss: 2.116 |
Epoch: [1] | Batch: [123] | Training_Loss: 0.850 |
Epoch: [1] | Batch: [124] | Training_Loss: 1.523 |
Epoch: [1] | Batch: [125] | Training_Loss: 1.720 |
Epoch: [1] | Batch: [126] | Training_Loss: 1.366 |
Epoch: [1] | Batch: [127] | Training_Loss: 0.886 |
Epoch: [1] | Batch: [128] | Training_Loss: 3.240 |
Epoch: [1] | Batch: [129] | Training_Loss: 1.455 |
Epoch: [1] | Batch: [130] | Training_Loss: 1.381 |
Epoch: [1] | Batch: [131] | Training_Loss: 3.120 |
Epoch: [1] | Batch: [132] | Training_Loss: 2.405 |
Epoch: [1] | Batch: [133] | Training_Loss: 2.488 |
Epoch: [1] | Batch: [134] | Training_Loss: 1.579 |
Epoch: [1] | Batch: [135] | Training_Loss: 1.829 |
Epoch: [1] | Batch: [136] | Training_Loss: 1.409 |
Epoch: [1] | Batch: [137] | Training_Loss: 2.301 |
Epoch: [1] | Batch: [138] | Training_Loss: 1.657 |
Epoch: [1] | Batch: [139] | Training_Loss: 2.189 |
Epoch: [1] | Batch: [140] | Training_Loss: 1.345 |
Epoch: [1] | Batch: [141] | Training_Loss: 0.793 |
Epoch: [1] | Batch: [142] | Training_Loss: 0.828 |
Epoch: [1] | Batch: [143] | Training_Loss: 2.134 |
Epoch: [1] | Batch: [144] | Training_Loss: 1.342 |
Epoch: [1] | Batch: [145] | Training_Loss: 1.194 |
Epoch: [1] | Batch: [146] | Training_Loss: 2.456 |
Epoch: [1] | Batch: [147] | Training_Loss: 2.062 |
Epoch: [1] | Batch: [148] | Training_Loss: 2.423 |
Epoch: [1] | Batch: [149] | Training_Loss: 1.417 |
Epoch: [1] | Batch: [150] | Training_Loss: 1.911 |
Epoch: [1] | Batch: [151] | Training_Loss: 1.832 |
Epoch: [1] | Batch: [152] | Training_Loss: 0.886 |
Epoch: [1] | Batch: [153] | Training_Loss: 1.892 |
Epoch: [1] | Batch: [154] | Training_Loss: 1.601 |
Epoch: [1] | Batch: [155] | Training_Loss: 0.898 |
Epoch: [1] | Batch: [156] | Training_Loss: 2.515 |
Epoch: [1] | Batch: [157] | Training_Loss: 2.055 |
Epoch: [1] | Batch: [158] | Training_Loss: 0.827 |
Epoch: [1] | Batch: [159] | Training_Loss: 0.843 |
Epoch: [1] | Batch: [160] | Training_Loss: 2.880 |
Epoch: [1] | Batch: [161] | Training_Loss: 0.955 |
Epoch: [1] | Batch: [162] | Training_Loss: 1.966 |
Epoch: [1] | Batch: [163] | Training_Loss: 1.786 |
Epoch: [1] | Batch: [164] | Training_Loss: 1.322 |
Epoch: [1] | Batch: [165] | Training_Loss: 1.452 |
Epoch: [1] | Batch: [166] | Training_Loss: 2.708 |
Epoch: [1] | Batch: [167] | Training_Loss: 2.136 |
Epoch: [1] | Batch: [168] | Training_Loss: 2.047 |
Epoch: [1] | Batch: [169] | Training_Loss: 0.809 |
Epoch: [1] | Batch: [170] | Training_Loss: 1.731 |
Epoch: [1] | Batch: [171] | Training_Loss: 0.860 |
Epoch: [1] | Batch: [172] | Training_Loss: 1.326 |
Epoch: [1] | Batch: [173] | Training_Loss: 1.996 |
Epoch: [1] | Batch: [174] | Training_Loss: 2.095 |
Epoch: [1] | Batch: [175] | Training_Loss: 1.912 |
Epoch: [1] | Batch: [176] | Training_Loss: 1.802 |
Epoch: [1] | Batch: [177] | Training_Loss: 1.428 |
Epoch: [1] | Batch: [178] | Training_Loss: 1.149 |
Epoch: [1] | Batch: [179] | Training_Loss: 2.227 |
Epoch: [1] | Batch: [180] | Training_Loss: 1.542 |
Epoch: [1] | Batch: [181] | Training_Loss: 1.631 |
Epoch: [1] | Batch: [182] | Training_Loss: 1.525 |
Epoch: [1] | Batch: [183] | Training_Loss: 1.705 |
Epoch: [1] | Batch: [184] | Training_Loss: 1.175 |
Epoch: [1] | Batch: [185] | Training_Loss: 1.886 |
Epoch: [1] | Batch: [186] | Training_Loss: 1.770 |
Epoch: [1] | Batch: [187] | Training_Loss: 1.690 |
Epoch: [1] | Batch: [188] | Training_Loss: 0.700 |
Epoch: [1] | Batch: [189] | Training_Loss: 2.069 |
Epoch: [1] | Batch: [190] | Training_Loss: 1.368 |
Epoch: [1] | Batch: [191] | Training_Loss: 1.089 |
Epoch: [1] | Batch: [192] | Training_Loss: 2.582 |
Epoch: [1] | Batch: [193] | Training_Loss: 1.408 |
Epoch: [1] | Batch: [194] | Training_Loss: 1.089 |
Epoch: [1] | Batch: [195] | Training_Loss: 1.219 |
Epoch: [1] | Batch: [196] | Training_Loss: 1.975 |
Epoch: [1] | Batch: [197] | Training_Loss: 1.903 |
Epoch: [1] | Batch: [198] | Training_Loss: 1.329 |
Epoch: [1] | Batch: [199] | Training_Loss: 1.974 |
Epoch: [1] | Batch: [200] | Training_Loss: 1.776 |
Epoch: [1] | Batch: [201] | Training_Loss: 1.294 |
Epoch: [1] | Batch: [202] | Training_Loss: 1.025 |
Epoch: [1] | Batch: [203] | Training_Loss: 1.661 |
Epoch: [1] | Batch: [204] | Training_Loss: 1.326 |
Epoch: [1] | Batch: [205] | Training_Loss: 1.090 |
Epoch: [1] | Batch: [206] | Training_Loss: 0.728 |
Epoch: [1] | Batch: [207] | Training_Loss: 0.667 |
Epoch: [1] | Batch: [208] | Training_Loss: 1.790 |
Epoch: [1] | Batch: [209] | Training_Loss: 0.788 |
Epoch: [1] | Batch: [210] | Training_Loss: 1.141 |
Epoch: [1] | Batch: [211] | Training_Loss: 1.831 |
Epoch: [1] | Batch: [212] | Training_Loss: 0.694 |
Epoch: [1] | Batch: [213] | Training_Loss: 1.446 |
Epoch: [1] | Batch: [214] | Training_Loss: 1.227 |
Epoch: [1] | Batch: [215] | Training_Loss: 1.414 |
Epoch: [1] | Batch: [216] | Training_Loss: 2.259 |
Epoch: [1] | Batch: [217] | Training_Loss: 0.785 |
Epoch: [1] | Batch: [218] | Training_Loss: 1.757 |
Epoch: [1] | Batch: [219] | Training_Loss: 0.866 |
Epoch: [1] | Batch: [220] | Training_Loss: 1.313 |
Epoch: [1] | Batch: [221] | Training_Loss: 1.815 |
Epoch: [1] | Batch: [222] | Training_Loss: 1.113 |
Epoch: [1] | Batch: [223] | Training_Loss: 1.121 |
Epoch: [1] | Batch: [224] | Training_Loss: 0.744 |
Epoch: [1] | Batch: [225] | Training_Loss: 1.795 |
Epoch: [1] | Batch: [226] | Training_Loss: 1.067 |
Epoch: [1] | Batch: [227] | Training_Loss: 1.014 |
Epoch: [1] | Batch: [228] | Training_Loss: 2.008 |
Epoch: [1] | Batch: [229] | Training_Loss: 1.047 |
Epoch: [1] | Batch: [230] | Training_Loss: 1.357 |
Epoch: [1] | Batch: [231] | Training_Loss: 1.875 |
Epoch: [1] | Batch: [232] | Training_Loss: 2.286 |
Epoch: [1] | Batch: [233] | Training_Loss: 0.738 |
Epoch: [1] | Batch: [234] | Training_Loss: 1.934 |
Epoch: [1] | Batch: [235] | Training_Loss: 2.643 |
Epoch: [1] | Batch: [236] | Training_Loss: 1.059 |
Epoch: [1] | Batch: [237] | Training_Loss: 1.524 |
Epoch: [1] | Batch: [238] | Training_Loss: 1.146 |
Epoch: [1] | Batch: [239] | Training_Loss: 0.729 |
Epoch: [1] | Batch: [240] | Training_Loss: 0.854 |
Epoch: [1] | Batch: [241] | Training_Loss: 2.024 |
Epoch: [1] | Batch: [242] | Training_Loss: 1.286 |
Epoch: [1] | Batch: [243] | Training_Loss: 1.078 |
Epoch: [1] | Batch: [244] | Training_Loss: 2.087 |
Epoch: [1] | Batch: [245] | Training_Loss: 1.280 |
Epoch: [1] | Batch: [246] | Training_Loss: 0.830 |
Epoch: [1] | Batch: [247] | Training_Loss: 0.730 |
Epoch: [1] | Batch: [248] | Training_Loss: 2.087 |
Epoch: [1] | Batch: [249] | Training_Loss: 2.407 |
Epoch: [1] | Batch: [250] | Training_Loss: 1.046 |
Epoch: [1] | Batch: [251] | Training_Loss: 0.833 |
Epoch: [1] | Batch: [252] | Training_Loss: 0.940 |
Epoch: [1] | Batch: [253] | Training_Loss: 0.702 |
Epoch: [1] | Batch: [254] | Training_Loss: 1.094 |
Epoch: [1] | Batch: [255] | Training_Loss: 2.482 |
Epoch: [1] | Batch: [256] | Training_Loss: 2.010 |
Epoch: [1] | Batch: [257] | Training_Loss: 1.033 |
Epoch: [1] | Batch: [258] | Training_Loss: 1.661 |
Epoch: [1] | Batch: [259] | Training_Loss: 1.380 |
Epoch: [1] | Batch: [260] | Training_Loss: 1.506 |
Epoch: [1] | Batch: [261] | Training_Loss: 1.972 |
Epoch: [1] | Batch: [262] | Training_Loss: 1.982 |
Epoch: [1] | Batch: [263] | Training_Loss: 1.939 |
Epoch: [1] | Batch: [264] | Training_Loss: 1.910 |
Epoch: [1] | Batch: [265] | Training_Loss: 1.149 |
Epoch: [1] | Batch: [266] | Training_Loss: 1.996 |
Epoch: [1] | Batch: [267] | Training_Loss: 0.621 |
Epoch: [1] | Batch: [268] | Training_Loss: 1.090 |
Epoch: [1] | Batch: [269] | Training_Loss: 1.668 |
Epoch: [1] | Batch: [270] | Training_Loss: 2.070 |
Epoch: [1] | Batch: [271] | Training_Loss: 1.804 |
Epoch: [1] | Batch: [272] | Training_Loss: 1.750 |
Epoch: [1] | Batch: [273] | Training_Loss: 1.257 |
Epoch: [1] | Batch: [274] | Training_Loss: 2.584 |
Epoch: [1] | Batch: [275] | Training_Loss: 1.054 |
Epoch: [1] | Batch: [276] | Training_Loss: 0.746 |
Epoch: [1] | Batch: [277] | Training_Loss: 1.807 |
Epoch: [1] | Batch: [278] | Training_Loss: 1.535 |
Epoch: [1] | Batch: [279] | Training_Loss: 1.416 |
Epoch: [1] | Batch: [280] | Training_Loss: 1.238 |
Epoch: [1] | Batch: [281] | Training_Loss: 1.060 |
Epoch: [1] | Batch: [282] | Training_Loss: 1.639 |
Epoch: [1] | Batch: [283] | Training_Loss: 1.074 |
Epoch: [1] | Batch: [284] | Training_Loss: 0.927 |
Epoch: [1] | Batch: [285] | Training_Loss: 1.932 |
Epoch: [1] | Batch: [286] | Training_Loss: 1.652 |
Epoch: [1] | Batch: [287] | Training_Loss: 1.041 |
Epoch: [1] | Batch: [288] | Training_Loss: 1.121 |
Epoch: [1] | Batch: [289] | Training_Loss: 1.707 |
Epoch: [1] | Batch: [290] | Training_Loss: 2.011 |
Epoch: [1] | Batch: [291] | Training_Loss: 1.919 |
Epoch: [1] | Batch: [292] | Training_Loss: 1.202 |
Epoch: [1] | Batch: [293] | Training_Loss: 1.219 |
Epoch: [1] | Batch: [294] | Training_Loss: 0.679 |
Epoch: [1] | Batch: [295] | Training_Loss: 1.066 |
Epoch: [1] | Batch: [296] | Training_Loss: 2.000 |
Epoch: [1] | Batch: [297] | Training_Loss: 1.445 |
Epoch: [1] | Batch: [298] | Training_Loss: 0.661 |
Epoch: [1] | Batch: [299] | Training_Loss: 0.670 |
Epoch: [1] | Batch: [300] | Training_Loss: 0.901 |
Epoch: [1] | Batch: [301] | Training_Loss: 1.896 |
Epoch: [1] | Batch: [302] | Training_Loss: 1.086 |
Epoch: [1] | Batch: [303] | Training_Loss: 0.996 |
Epoch: [1] | Batch: [304] | Training_Loss: 0.683 |
Epoch: [1] | Batch: [305] | Training_Loss: 0.627 |
Epoch: [1] | Batch: [306] | Training_Loss: 0.994 |
Epoch: [1] | Batch: [307] | Training_Loss: 0.818 |
Epoch: [1] | Batch: [308] | Training_Loss: 2.280 |
Epoch: [1] | Batch: [309] | Training_Loss: 0.677 |
Epoch: [1] | Batch: [310] | Training_Loss: 2.208 |
Epoch: [1] | Batch: [311] | Training_Loss: 2.138 |
Epoch: [1] | Batch: [312] | Training_Loss: 2.265 |
Epoch: [1] | Batch: [313] | Training_Loss: 1.723 |
Epoch: [1] | Batch: [314] | Training_Loss: 0.902 |
Epoch: [1] | Batch: [315] | Training_Loss: 1.568 |
Epoch: [1] | Batch: [316] | Training_Loss: 1.405 |
Epoch: [1] | Batch: [317] | Training_Loss: 1.638 |
Epoch: [1] | Batch: [318] | Training_Loss: 1.030 |
Epoch: [1] | Batch: [319] | Training_Loss: 1.063 |
Epoch: [1] | Batch: [320] | Training_Loss: 0.571 |
Epoch: [1] | Batch: [321] | Training_Loss: 0.761 |
Epoch: [1] | Batch: [322] | Training_Loss: 2.085 |
Epoch: [1] | Batch: [323] | Training_Loss: 0.555 |
Epoch: [1] | Batch: [324] | Training_Loss: 0.856 |
Epoch: [1] | Batch: [325] | Training_Loss: 1.117 |
Epoch: [1] | Batch: [326] | Training_Loss: 1.074 |
Epoch: [1] | Batch: [327] | Training_Loss: 2.250 |
Epoch: [1] | Batch: [328] | Training_Loss: 0.613 |
Epoch: [1] | Batch: [329] | Training_Loss: 1.833 |
Epoch: [1] | Batch: [330] | Training_Loss: 1.256 |
Epoch: [1] | Batch: [331] | Training_Loss: 0.967 |
Epoch: [1] | Batch: [332] | Training_Loss: 1.962 |
Epoch: [1] | Batch: [333] | Training_Loss: 1.074 |
Epoch: [1] | Batch: [334] | Training_Loss: 1.741 |
Epoch: [1] | Batch: [335] | Training_Loss: 1.355 |
Epoch: [1] | Batch: [336] | Training_Loss: 0.863 |
Epoch: [1] | Batch: [337] | Training_Loss: 1.871 |
Epoch: [1] | Batch: [338] | Training_Loss: 0.677 |
Epoch: [1] | Batch: [339] | Training_Loss: 0.595 |
Epoch: [1] | Batch: [340] | Training_Loss: 0.652 |
Epoch: [1] | Batch: [341] | Training_Loss: 0.760 |
Epoch: [1] | Batch: [342] | Training_Loss: 1.053 |
Epoch: [1] | Batch: [343] | Training_Loss: 0.804 |
Epoch: [1] | Batch: [344] | Training_Loss: 0.616 |
Epoch: [1] | Batch: [345] | Training_Loss: 0.740 |
Epoch: [1] | Batch: [346] | Training_Loss: 0.973 |
Epoch: [1] | Batch: [347] | Training_Loss: 2.571 |
Epoch: [1] | Batch: [348] | Training_Loss: 0.719 |
Epoch: [1] | Batch: [349] | Training_Loss: 1.128 |
Epoch: [1] | Batch: [350] | Training_Loss: 1.200 |
Epoch: [1] | Batch: [351] | Training_Loss: 0.672 |
Epoch: [1] | Batch: [352] | Training_Loss: 0.735 |
Epoch: [1] | Batch: [353] | Training_Loss: 1.898 |
Epoch: [1] | Batch: [354] | Training_Loss: 0.693 |
Epoch: [1] | Batch: [355] | Training_Loss: 0.677 |
Epoch: [1] | Batch: [356] | Training_Loss: 0.575 |
Epoch: [1] | Batch: [357] | Training_Loss: 2.033 |
Epoch: [1] | Batch: [358] | Training_Loss: 0.681 |
Epoch: [1] | Batch: [359] | Training_Loss: 0.825 |
Epoch: [1] | Batch: [360] | Training_Loss: 0.777 |
Epoch: [1] | Batch: [361] | Training_Loss: 0.885 |
Epoch: [1] | Batch: [362] | Training_Loss: 0.677 |
Epoch: [1] | Batch: [363] | Training_Loss: 0.859 |
Epoch: [1] | Batch: [364] | Training_Loss: 0.590 |
Epoch: [1] | Batch: [365] | Training_Loss: 0.562 |
Epoch: [1] | Batch: [366] | Training_Loss: 1.506 |
Epoch: [1] | Batch: [367] | Training_Loss: 1.392 |
Epoch: [1] | Batch: [368] | Training_Loss: 1.289 |
Epoch: [1] | Batch: [369] | Training_Loss: 2.091 |
Epoch: [1] | Batch: [370] | Training_Loss: 0.558 |
Epoch: [1] | Batch: [371] | Training_Loss: 2.156 |
Epoch: [1] | Batch: [372] | Training_Loss: 2.169 |
Epoch: [1] | Batch: [373] | Training_Loss: 1.877 |
Epoch: [1] | Batch: [374] | Training_Loss: 2.188 |
Epoch: [1] | Batch: [375] | Training_Loss: 0.875 |
Epoch: [1] | Batch: [376] | Training_Loss: 0.831 |
Epoch: [1] | Batch: [377] | Training_Loss: 0.738 |
Epoch: [1] | Batch: [378] | Training_Loss: 0.752 |
Epoch: [1] | Batch: [379] | Training_Loss: 0.898 |
Epoch: [1] | Batch: [380] | Training_Loss: 0.945 |
Epoch: [1] | Batch: [381] | Training_Loss: 1.756 |
Epoch: [1] | Batch: [382] | Training_Loss: 0.822 |
Epoch: [1] | Batch: [383] | Training_Loss: 0.796 |
Epoch: [1] | Batch: [384] | Training_Loss: 1.632 |
Epoch: [1] | Batch: [385] | Training_Loss: 1.122 |
Epoch: [1] | Batch: [386] | Training_Loss: 0.598 |
Epoch: [1] | Batch: [387] | Training_Loss: 1.206 |
Epoch: [1] | Batch: [388] | Training_Loss: 1.227 |
Epoch: [1] | Batch: [389] | Training_Loss: 0.523 |
Epoch: [1] | Batch: [390] | Training_Loss: 1.407 |
Epoch: [1] | Batch: [391] | Training_Loss: 0.595 |
Epoch: [1] | Batch: [392] | Training_Loss: 1.269 |
Epoch: [1] | Batch: [393] | Training_Loss: 0.965 |
Epoch: [1] | Batch: [394] | Training_Loss: 2.403 |
Epoch: [1] | Batch: [395] | Training_Loss: 0.919 |
Epoch: [1] | Batch: [396] | Training_Loss: 1.612 |
Epoch: [1] | Batch: [397] | Training_Loss: 0.735 |
Epoch: [1] | Batch: [398] | Training_Loss: 0.971 |
Epoch: [1] | Batch: [399] | Training_Loss: 0.586 |
Epoch: [1] | Batch: [400] | Training_Loss: 0.606 |
Epoch: [1] | Batch: [401] | Training_Loss: 1.123 |
Epoch: [1] | Batch: [402] | Training_Loss: 1.155 |
Epoch: [1] | Batch: [403] | Training_Loss: 1.540 |
Epoch: [1] | Batch: [404] | Training_Loss: 0.881 |
Epoch: [1] | Batch: [405] | Training_Loss: 1.917 |
Epoch: [1] | Batch: [406] | Training_Loss: 1.853 |
Epoch: [1] | Batch: [407] | Training_Loss: 2.217 |
Epoch: [1] | Batch: [408] | Training_Loss: 1.386 |
Epoch: [1] | Batch: [409] | Training_Loss: 0.821 |
Epoch: [1] | Batch: [410] | Training_Loss: 0.819 |
Epoch: [1] | Batch: [411] | Training_Loss: 1.031 |
Epoch: [1] | Batch: [412] | Training_Loss: 1.389 |
Epoch: [1] | Batch: [413] | Training_Loss: 1.429 |
Epoch: [1] | Batch: [414] | Training_Loss: 2.044 |
Epoch: [1] | Batch: [415] | Training_Loss: 1.154 |
Epoch: [1] | Batch: [416] | Training_Loss: 1.721 |
Epoch: [1] | Batch: [417] | Training_Loss: 1.403 |
Epoch: [1] | Batch: [418] | Training_Loss: 0.536 |
Epoch: [1] | Batch: [419] | Training_Loss: 0.662 |
Epoch: [1] | Batch: [420] | Training_Loss: 2.087 |
Epoch: [1] | Batch: [421] | Training_Loss: 1.058 |
Epoch: [1] | Batch: [422] | Training_Loss: 1.938 |
Epoch: [1] | Batch: [423] | Training_Loss: 0.645 |
Epoch: [1] | Batch: [424] | Training_Loss: 0.579 |
Epoch: [1] | Batch: [425] | Training_Loss: 1.448 |
Epoch: [1] | Batch: [426] | Training_Loss: 0.824 |
Epoch: [1] | Batch: [427] | Training_Loss: 1.945 |
Epoch: [1] | Batch: [428] | Training_Loss: 0.819 |
Epoch: [1] | Batch: [429] | Training_Loss: 0.667 |
Epoch: [1] | Batch: [430] | Training_Loss: 0.755 |
Epoch: [1] | Batch: [431] | Training_Loss: 0.726 |
Epoch: [1] | Batch: [432] | Training_Loss: 0.522 |
Epoch: [1] | Batch: [433] | Training_Loss: 1.025 |
Epoch: [1] | Batch: [434] | Training_Loss: 0.855 |
Epoch: [1] | Batch: [435] | Training_Loss: 0.772 |
Epoch: [1] | Batch: [436] | Training_Loss: 0.529 |
Epoch: [1] | Batch: [437] | Training_Loss: 2.024 |
Epoch: [1] | Batch: [438] | Training_Loss: 1.483 |
Epoch: [1] | Batch: [439] | Training_Loss: 0.575 |
Epoch: [1] | Batch: [440] | Training_Loss: 2.311 |
Epoch: [1] | Batch: [441] | Training_Loss: 0.724 |
Epoch: [1] | Batch: [442] | Training_Loss: 1.059 |
Epoch: [1] | Batch: [443] | Training_Loss: 1.398 |
Epoch: [1] | Batch: [444] | Training_Loss: 1.252 |
Epoch: [1] | Batch: [445] | Training_Loss: 1.522 |
Epoch: [1] | Batch: [446] | Training_Loss: 0.702 |
Epoch: [1] | Batch: [447] | Training_Loss: 1.762 |
Epoch: [1] | Batch: [448] | Training_Loss: 0.516 |
Epoch: [1] | Batch: [449] | Training_Loss: 0.831 |
Epoch: [1] | Batch: [450] | Training_Loss: 0.562 |
Epoch: [1] | Batch: [451] | Training_Loss: 0.889 |
Epoch: [1] | Batch: [452] | Training_Loss: 0.833 |
Epoch: [1] | Batch: [453] | Training_Loss: 1.461 |
Epoch: [1] | Batch: [454] | Training_Loss: 1.656 |
Epoch: [1] | Batch: [455] | Training_Loss: 0.653 |
Epoch: [1] | Batch: [456] | Training_Loss: 0.604 |
Epoch: [1] | Batch: [457] | Training_Loss: 0.520 |
Epoch: [1] | Batch: [458] | Training_Loss: 1.221 |
Epoch: [1] | Batch: [459] | Training_Loss: 2.043 |
Epoch: [1] | Batch: [460] | Training_Loss: 1.319 |
Epoch: [1] | Batch: [461] | Training_Loss: 0.460 |
Epoch: [1] | Batch: [462] | Training_Loss: 1.074 |
Epoch: [1] | Batch: [463] | Training_Loss: 0.562 |
Epoch: [1] | Training_Loss: 1.511 |
Epoch: [2] | Batch: [1] | Training_Loss: 0.839 |
Epoch: [2] | Batch: [2] | Training_Loss: 0.507 |
Epoch: [2] | Batch: [3] | Training_Loss: 0.465 |
Epoch: [2] | Batch: [4] | Training_Loss: 0.454 |
Epoch: [2] | Batch: [5] | Training_Loss: 1.355 |
Epoch: [2] | Batch: [6] | Training_Loss: 0.654 |
Epoch: [2] | Batch: [7] | Training_Loss: 1.504 |
Epoch: [2] | Batch: [8] | Training_Loss: 0.818 |
Epoch: [2] | Batch: [9] | Training_Loss: 2.036 |
Epoch: [2] | Batch: [10] | Training_Loss: 1.375 |
Epoch: [2] | Batch: [11] | Training_Loss: 0.896 |
Epoch: [2] | Batch: [12] | Training_Loss: 0.677 |
Epoch: [2] | Batch: [13] | Training_Loss: 0.422 |
Epoch: [2] | Batch: [14] | Training_Loss: 1.846 |
Epoch: [2] | Batch: [15] | Training_Loss: 0.749 |
Epoch: [2] | Batch: [16] | Training_Loss: 0.434 |
Epoch: [2] | Batch: [17] | Training_Loss: 0.654 |
Epoch: [2] | Batch: [18] | Training_Loss: 2.182 |
Epoch: [2] | Batch: [19] | Training_Loss: 0.613 |
Epoch: [2] | Batch: [20] | Training_Loss: 0.503 |
Epoch: [2] | Batch: [21] | Training_Loss: 0.494 |
Epoch: [2] | Batch: [22] | Training_Loss: 1.435 |
Epoch: [2] | Batch: [23] | Training_Loss: 0.591 |
Epoch: [2] | Batch: [24] | Training_Loss: 0.569 |
Epoch: [2] | Batch: [25] | Training_Loss: 0.334 |
Epoch: [2] | Batch: [26] | Training_Loss: 0.698 |
Epoch: [2] | Batch: [27] | Training_Loss: 0.611 |
Epoch: [2] | Batch: [28] | Training_Loss: 2.066 |
Epoch: [2] | Batch: [29] | Training_Loss: 1.656 |
Epoch: [2] | Batch: [30] | Training_Loss: 0.715 |
Epoch: [2] | Batch: [31] | Training_Loss: 0.427 |
Epoch: [2] | Batch: [32] | Training_Loss: 1.990 |
Epoch: [2] | Batch: [33] | Training_Loss: 1.320 |
Epoch: [2] | Batch: [34] | Training_Loss: 1.269 |
Epoch: [2] | Batch: [35] | Training_Loss: 2.555 |
Epoch: [2] | Batch: [36] | Training_Loss: 0.416 |
Epoch: [2] | Batch: [37] | Training_Loss: 1.720 |
Epoch: [2] | Batch: [38] | Training_Loss: 0.463 |
Epoch: [2] | Batch: [39] | Training_Loss: 0.610 |
Epoch: [2] | Batch: [40] | Training_Loss: 0.603 |
Epoch: [2] | Batch: [41] | Training_Loss: 1.641 |
Epoch: [2] | Batch: [42] | Training_Loss: 0.434 |
Epoch: [2] | Batch: [43] | Training_Loss: 1.023 |
Epoch: [2] | Batch: [44] | Training_Loss: 1.057 |
Epoch: [2] | Batch: [45] | Training_Loss: 0.813 |
Epoch: [2] | Batch: [46] | Training_Loss: 0.617 |
Epoch: [2] | Batch: [47] | Training_Loss: 1.192 |
Epoch: [2] | Batch: [48] | Training_Loss: 0.993 |
Epoch: [2] | Batch: [49] | Training_Loss: 0.653 |
Epoch: [2] | Batch: [50] | Training_Loss: 1.337 |
Epoch: [2] | Batch: [51] | Training_Loss: 2.045 |
Epoch: [2] | Batch: [52] | Training_Loss: 1.066 |
Epoch: [2] | Batch: [53] | Training_Loss: 0.935 |
Epoch: [2] | Batch: [54] | Training_Loss: 1.954 |
Epoch: [2] | Batch: [55] | Training_Loss: 0.422 |
Epoch: [2] | Batch: [56] | Training_Loss: 1.376 |
Epoch: [2] | Batch: [57] | Training_Loss: 1.209 |
Epoch: [2] | Batch: [58] | Training_Loss: 0.851 |
Epoch: [2] | Batch: [59] | Training_Loss: 0.805 |
Epoch: [2] | Batch: [60] | Training_Loss: 1.430 |
Epoch: [2] | Batch: [61] | Training_Loss: 0.718 |
Epoch: [2] | Batch: [62] | Training_Loss: 2.140 |
Epoch: [2] | Batch: [63] | Training_Loss: 0.449 |
Epoch: [2] | Batch: [64] | Training_Loss: 0.594 |
Epoch: [2] | Batch: [65] | Training_Loss: 0.408 |
Epoch: [2] | Batch: [66] | Training_Loss: 2.145 |
Epoch: [2] | Batch: [67] | Training_Loss: 1.365 |
Epoch: [2] | Batch: [68] | Training_Loss: 1.389 |
Epoch: [2] | Batch: [69] | Training_Loss: 0.666 |
Epoch: [2] | Batch: [70] | Training_Loss: 0.751 |
Epoch: [2] | Batch: [71] | Training_Loss: 1.255 |
Epoch: [2] | Batch: [72] | Training_Loss: 0.647 |
Epoch: [2] | Batch: [73] | Training_Loss: 0.495 |
Epoch: [2] | Batch: [74] | Training_Loss: 1.705 |
Epoch: [2] | Batch: [75] | Training_Loss: 0.553 |
Epoch: [2] | Batch: [76] | Training_Loss: 0.471 |
Epoch: [2] | Batch: [77] | Training_Loss: 0.629 |
Epoch: [2] | Batch: [78] | Training_Loss: 1.185 |
Epoch: [2] | Batch: [79] | Training_Loss: 1.806 |
Epoch: [2] | Batch: [80] | Training_Loss: 0.784 |
Epoch: [2] | Batch: [81] | Training_Loss: 1.533 |
Epoch: [2] | Batch: [82] | Training_Loss: 0.722 |
Epoch: [2] | Batch: [83] | Training_Loss: 0.556 |
Epoch: [2] | Batch: [84] | Training_Loss: 1.278 |
Epoch: [2] | Batch: [85] | Training_Loss: 1.296 |
Epoch: [2] | Batch: [86] | Training_Loss: 2.648 |
Epoch: [2] | Batch: [87] | Training_Loss: 0.681 |
Epoch: [2] | Batch: [88] | Training_Loss: 1.576 |
Epoch: [2] | Batch: [89] | Training_Loss: 1.202 |
Epoch: [2] | Batch: [90] | Training_Loss: 0.618 |
Epoch: [2] | Batch: [91] | Training_Loss: 1.891 |
Epoch: [2] | Batch: [92] | Training_Loss: 2.068 |
Epoch: [2] | Batch: [93] | Training_Loss: 1.263 |
Epoch: [2] | Batch: [94] | Training_Loss: 0.423 |
Epoch: [2] | Batch: [95] | Training_Loss: 0.769 |
Epoch: [2] | Batch: [96] | Training_Loss: 0.448 |
Epoch: [2] | Batch: [97] | Training_Loss: 0.613 |
Epoch: [2] | Batch: [98] | Training_Loss: 0.472 |
Epoch: [2] | Batch: [99] | Training_Loss: 0.762 |
Epoch: [2] | Batch: [100] | Training_Loss: 1.408 |
Epoch: [2] | Batch: [101] | Training_Loss: 0.516 |
Epoch: [2] | Batch: [102] | Training_Loss: 0.845 |
Epoch: [2] | Batch: [103] | Training_Loss: 1.595 |
Epoch: [2] | Batch: [104] | Training_Loss: 1.389 |
Epoch: [2] | Batch: [105] | Training_Loss: 0.394 |
Epoch: [2] | Batch: [106] | Training_Loss: 0.703 |
Epoch: [2] | Batch: [107] | Training_Loss: 0.596 |
Epoch: [2] | Batch: [108] | Training_Loss: 1.283 |
Epoch: [2] | Batch: [109] | Training_Loss: 0.885 |
Epoch: [2] | Batch: [110] | Training_Loss: 0.596 |
Epoch: [2] | Batch: [111] | Training_Loss: 1.337 |
Epoch: [2] | Batch: [112] | Training_Loss: 0.668 |
Epoch: [2] | Batch: [113] | Training_Loss: 1.705 |
Epoch: [2] | Batch: [114] | Training_Loss: 1.474 |
Epoch: [2] | Batch: [115] | Training_Loss: 0.659 |
Epoch: [2] | Batch: [116] | Training_Loss: 0.687 |
Epoch: [2] | Batch: [117] | Training_Loss: 1.972 |
Epoch: [2] | Batch: [118] | Training_Loss: 2.102 |
Epoch: [2] | Batch: [119] | Training_Loss: 0.426 |
Epoch: [2] | Batch: [120] | Training_Loss: 1.338 |
Epoch: [2] | Batch: [121] | Training_Loss: 0.545 |
Epoch: [2] | Batch: [122] | Training_Loss: 0.597 |
Epoch: [2] | Batch: [123] | Training_Loss: 1.933 |
Epoch: [2] | Batch: [124] | Training_Loss: 0.811 |
Epoch: [2] | Batch: [125] | Training_Loss: 0.449 |
Epoch: [2] | Batch: [126] | Training_Loss: 1.508 |
Epoch: [2] | Batch: [127] | Training_Loss: 1.566 |
Epoch: [2] | Batch: [128] | Training_Loss: 0.762 |
Epoch: [2] | Batch: [129] | Training_Loss: 0.471 |
Epoch: [2] | Batch: [130] | Training_Loss: 0.530 |
Epoch: [2] | Batch: [131] | Training_Loss: 0.407 |
Epoch: [2] | Batch: [132] | Training_Loss: 0.817 |
Epoch: [2] | Batch: [133] | Training_Loss: 1.347 |
Epoch: [2] | Batch: [134] | Training_Loss: 2.051 |
Epoch: [2] | Batch: [135] | Training_Loss: 1.795 |
Epoch: [2] | Batch: [136] | Training_Loss: 1.259 |
Epoch: [2] | Batch: [137] | Training_Loss: 1.426 |
Epoch: [2] | Batch: [138] | Training_Loss: 1.537 |
Epoch: [2] | Batch: [139] | Training_Loss: 0.694 |
Epoch: [2] | Batch: [140] | Training_Loss: 0.559 |
Epoch: [2] | Batch: [141] | Training_Loss: 1.835 |
Epoch: [2] | Batch: [142] | Training_Loss: 0.405 |
Epoch: [2] | Batch: [143] | Training_Loss: 1.040 |
Epoch: [2] | Batch: [144] | Training_Loss: 2.288 |
Epoch: [2] | Batch: [145] | Training_Loss: 1.529 |
Epoch: [2] | Batch: [146] | Training_Loss: 1.166 |
Epoch: [2] | Batch: [147] | Training_Loss: 2.420 |
Epoch: [2] | Batch: [148] | Training_Loss: 1.751 |
Epoch: [2] | Batch: [149] | Training_Loss: 0.434 |
Epoch: [2] | Batch: [150] | Training_Loss: 1.791 |
Epoch: [2] | Batch: [151] | Training_Loss: 1.485 |
Epoch: [2] | Batch: [152] | Training_Loss: 0.905 |
Epoch: [2] | Batch: [153] | Training_Loss: 1.024 |
Epoch: [2] | Batch: [154] | Training_Loss: 1.311 |
Epoch: [2] | Batch: [155] | Training_Loss: 1.308 |
Epoch: [2] | Batch: [156] | Training_Loss: 1.103 |
Epoch: [2] | Batch: [157] | Training_Loss: 0.582 |
Epoch: [2] | Batch: [158] | Training_Loss: 0.829 |
Epoch: [2] | Batch: [159] | Training_Loss: 0.520 |
Epoch: [2] | Batch: [160] | Training_Loss: 2.326 |
Epoch: [2] | Batch: [161] | Training_Loss: 0.411 |
Epoch: [2] | Batch: [162] | Training_Loss: 1.570 |
Epoch: [2] | Batch: [163] | Training_Loss: 0.672 |
Epoch: [2] | Batch: [164] | Training_Loss: 0.886 |
Epoch: [2] | Batch: [165] | Training_Loss: 0.650 |
Epoch: [2] | Batch: [166] | Training_Loss: 0.797 |
Epoch: [2] | Batch: [167] | Training_Loss: 0.609 |
Epoch: [2] | Batch: [168] | Training_Loss: 0.644 |
Epoch: [2] | Batch: [169] | Training_Loss: 0.956 |
Epoch: [2] | Batch: [170] | Training_Loss: 1.099 |
Epoch: [2] | Batch: [171] | Training_Loss: 2.400 |
Epoch: [2] | Batch: [172] | Training_Loss: 0.364 |
Epoch: [2] | Batch: [173] | Training_Loss: 1.519 |
Epoch: [2] | Batch: [174] | Training_Loss: 0.855 |
Epoch: [2] | Batch: [175] | Training_Loss: 0.945 |
Epoch: [2] | Batch: [176] | Training_Loss: 1.306 |
Epoch: [2] | Batch: [177] | Training_Loss: 0.708 |
Epoch: [2] | Batch: [178] | Training_Loss: 1.219 |
Epoch: [2] | Batch: [179] | Training_Loss: 1.468 |
Epoch: [2] | Batch: [180] | Training_Loss: 0.620 |
Epoch: [2] | Batch: [181] | Training_Loss: 1.728 |
Epoch: [2] | Batch: [182] | Training_Loss: 0.357 |
Epoch: [2] | Batch: [183] | Training_Loss: 1.466 |
Epoch: [2] | Batch: [184] | Training_Loss: 0.316 |
Epoch: [2] | Batch: [185] | Training_Loss: 0.370 |
Epoch: [2] | Batch: [186] | Training_Loss: 1.709 |
Epoch: [2] | Batch: [187] | Training_Loss: 0.595 |
Epoch: [2] | Batch: [188] | Training_Loss: 1.293 |
Epoch: [2] | Batch: [189] | Training_Loss: 0.372 |
Epoch: [2] | Batch: [190] | Training_Loss: 0.384 |
Epoch: [2] | Batch: [191] | Training_Loss: 0.762 |
Epoch: [2] | Batch: [192] | Training_Loss: 0.450 |
Epoch: [2] | Batch: [193] | Training_Loss: 2.169 |
Epoch: [2] | Batch: [194] | Training_Loss: 0.833 |
Epoch: [2] | Batch: [195] | Training_Loss: 0.555 |
Epoch: [2] | Batch: [196] | Training_Loss: 1.599 |
Epoch: [2] | Batch: [197] | Training_Loss: 1.276 |
Epoch: [2] | Batch: [198] | Training_Loss: 1.467 |
Epoch: [2] | Batch: [199] | Training_Loss: 1.957 |
Epoch: [2] | Batch: [200] | Training_Loss: 1.708 |
Epoch: [2] | Batch: [201] | Training_Loss: 0.377 |
Epoch: [2] | Batch: [202] | Training_Loss: 0.581 |
Epoch: [2] | Batch: [203] | Training_Loss: 1.424 |
Epoch: [2] | Batch: [204] | Training_Loss: 2.150 |
Epoch: [2] | Batch: [205] | Training_Loss: 1.526 |
Epoch: [2] | Batch: [206] | Training_Loss: 0.445 |
Epoch: [2] | Batch: [207] | Training_Loss: 0.569 |
Epoch: [2] | Batch: [208] | Training_Loss: 0.975 |
Epoch: [2] | Batch: [209] | Training_Loss: 1.490 |
Epoch: [2] | Batch: [210] | Training_Loss: 2.246 |
Epoch: [2] | Batch: [211] | Training_Loss: 1.852 |
Epoch: [2] | Batch: [212] | Training_Loss: 1.053 |
Epoch: [2] | Batch: [213] | Training_Loss: 0.477 |
Epoch: [2] | Batch: [214] | Training_Loss: 1.516 |
Epoch: [2] | Batch: [215] | Training_Loss: 1.358 |
Epoch: [2] | Batch: [216] | Training_Loss: 0.830 |
Epoch: [2] | Batch: [217] | Training_Loss: 0.395 |
Epoch: [2] | Batch: [218] | Training_Loss: 0.495 |
Epoch: [2] | Batch: [219] | Training_Loss: 1.211 |
Epoch: [2] | Batch: [220] | Training_Loss: 0.452 |
Epoch: [2] | Batch: [221] | Training_Loss: 3.061 |
Epoch: [2] | Batch: [222] | Training_Loss: 0.771 |
Epoch: [2] | Batch: [223] | Training_Loss: 1.108 |
Epoch: [2] | Batch: [224] | Training_Loss: 0.392 |
Epoch: [2] | Batch: [225] | Training_Loss: 1.456 |
Epoch: [2] | Batch: [226] | Training_Loss: 0.449 |
Epoch: [2] | Batch: [227] | Training_Loss: 1.233 |
Epoch: [2] | Batch: [228] | Training_Loss: 1.107 |
Epoch: [2] | Batch: [229] | Training_Loss: 1.989 |
Epoch: [2] | Batch: [230] | Training_Loss: 2.227 |
Epoch: [2] | Batch: [231] | Training_Loss: 0.414 |
Epoch: [2] | Batch: [232] | Training_Loss: 1.621 |
Epoch: [2] | Batch: [233] | Training_Loss: 0.321 |
Epoch: [2] | Batch: [234] | Training_Loss: 1.789 |
Epoch: [2] | Batch: [235] | Training_Loss: 0.321 |
Epoch: [2] | Batch: [236] | Training_Loss: 0.474 |
Epoch: [2] | Batch: [237] | Training_Loss: 1.245 |
Epoch: [2] | Batch: [238] | Training_Loss: 2.563 |
Epoch: [2] | Batch: [239] | Training_Loss: 1.084 |
Epoch: [2] | Batch: [240] | Training_Loss: 1.321 |
Epoch: [2] | Batch: [241] | Training_Loss: 0.580 |
Epoch: [2] | Batch: [242] | Training_Loss: 0.700 |
Epoch: [2] | Batch: [243] | Training_Loss: 0.892 |
Epoch: [2] | Batch: [244] | Training_Loss: 2.160 |
Epoch: [2] | Batch: [245] | Training_Loss: 2.085 |
Epoch: [2] | Batch: [246] | Training_Loss: 1.675 |
Epoch: [2] | Batch: [247] | Training_Loss: 0.370 |
Epoch: [2] | Batch: [248] | Training_Loss: 1.065 |
Epoch: [2] | Batch: [249] | Training_Loss: 0.543 |
Epoch: [2] | Batch: [250] | Training_Loss: 1.761 |
Epoch: [2] | Batch: [251] | Training_Loss: 1.386 |
Epoch: [2] | Batch: [252] | Training_Loss: 0.781 |
Epoch: [2] | Batch: [253] | Training_Loss: 0.618 |
Epoch: [2] | Batch: [254] | Training_Loss: 1.957 |
Epoch: [2] | Batch: [255] | Training_Loss: 0.739 |
Epoch: [2] | Batch: [256] | Training_Loss: 1.669 |
Epoch: [2] | Batch: [257] | Training_Loss: 0.512 |
Epoch: [2] | Batch: [258] | Training_Loss: 0.726 |
Epoch: [2] | Batch: [259] | Training_Loss: 1.694 |
Epoch: [2] | Batch: [260] | Training_Loss: 1.079 |
Epoch: [2] | Batch: [261] | Training_Loss: 0.649 |
Epoch: [2] | Batch: [262] | Training_Loss: 1.283 |
Epoch: [2] | Batch: [263] | Training_Loss: 0.923 |
Epoch: [2] | Batch: [264] | Training_Loss: 0.639 |
Epoch: [2] | Batch: [265] | Training_Loss: 0.350 |
Epoch: [2] | Batch: [266] | Training_Loss: 0.344 |
Epoch: [2] | Batch: [267] | Training_Loss: 0.803 |
Epoch: [2] | Batch: [268] | Training_Loss: 0.499 |
Epoch: [2] | Batch: [269] | Training_Loss: 0.531 |
Epoch: [2] | Batch: [270] | Training_Loss: 0.378 |
Epoch: [2] | Batch: [271] | Training_Loss: 1.745 |
Epoch: [2] | Batch: [272] | Training_Loss: 2.052 |
Epoch: [2] | Batch: [273] | Training_Loss: 0.585 |
Epoch: [2] | Batch: [274] | Training_Loss: 0.446 |
Epoch: [2] | Batch: [275] | Training_Loss: 0.696 |
Epoch: [2] | Batch: [276] | Training_Loss: 1.655 |
Epoch: [2] | Batch: [277] | Training_Loss: 2.012 |
Epoch: [2] | Batch: [278] | Training_Loss: 1.497 |
Epoch: [2] | Batch: [279] | Training_Loss: 0.370 |
Epoch: [2] | Batch: [280] | Training_Loss: 0.942 |
Epoch: [2] | Batch: [281] | Training_Loss: 1.304 |
Epoch: [2] | Batch: [282] | Training_Loss: 1.836 |
Epoch: [2] | Batch: [283] | Training_Loss: 1.252 |
Epoch: [2] | Batch: [284] | Training_Loss: 0.967 |
Epoch: [2] | Batch: [285] | Training_Loss: 0.785 |
Epoch: [2] | Batch: [286] | Training_Loss: 1.763 |
Epoch: [2] | Batch: [287] | Training_Loss: 1.942 |
Epoch: [2] | Batch: [288] | Training_Loss: 0.711 |
Epoch: [2] | Batch: [289] | Training_Loss: 0.327 |
Epoch: [2] | Batch: [290] | Training_Loss: 0.340 |
Epoch: [2] | Batch: [291] | Training_Loss: 0.751 |
Epoch: [2] | Batch: [292] | Training_Loss: 0.444 |
Epoch: [2] | Batch: [293] | Training_Loss: 0.596 |
Epoch: [2] | Batch: [294] | Training_Loss: 2.118 |
Epoch: [2] | Batch: [295] | Training_Loss: 0.525 |
Epoch: [2] | Batch: [296] | Training_Loss: 1.405 |
Epoch: [2] | Batch: [297] | Training_Loss: 0.608 |
Epoch: [2] | Batch: [298] | Training_Loss: 1.435 |
Epoch: [2] | Batch: [299] | Training_Loss: 0.506 |
Epoch: [2] | Batch: [300] | Training_Loss: 1.591 |
Epoch: [2] | Batch: [301] | Training_Loss: 1.318 |
Epoch: [2] | Batch: [302] | Training_Loss: 0.524 |
Epoch: [2] | Batch: [303] | Training_Loss: 1.297 |
Epoch: [2] | Batch: [304] | Training_Loss: 1.866 |
Epoch: [2] | Batch: [305] | Training_Loss: 1.437 |
Epoch: [2] | Batch: [306] | Training_Loss: 1.027 |
Epoch: [2] | Batch: [307] | Training_Loss: 1.209 |
Epoch: [2] | Batch: [308] | Training_Loss: 0.480 |
Epoch: [2] | Batch: [309] | Training_Loss: 0.369 |
Epoch: [2] | Batch: [310] | Training_Loss: 0.338 |
Epoch: [2] | Batch: [311] | Training_Loss: 1.988 |
Epoch: [2] | Batch: [312] | Training_Loss: 2.032 |
Epoch: [2] | Batch: [313] | Training_Loss: 1.710 |
Epoch: [2] | Batch: [314] | Training_Loss: 0.577 |
Epoch: [2] | Batch: [315] | Training_Loss: 0.456 |
Epoch: [2] | Batch: [316] | Training_Loss: 1.460 |
Epoch: [2] | Batch: [317] | Training_Loss: 1.154 |
Epoch: [2] | Batch: [318] | Training_Loss: 1.208 |
Epoch: [2] | Batch: [319] | Training_Loss: 0.687 |
Epoch: [2] | Batch: [320] | Training_Loss: 2.332 |
Epoch: [2] | Batch: [321] | Training_Loss: 1.227 |
Epoch: [2] | Batch: [322] | Training_Loss: 1.357 |
Epoch: [2] | Batch: [323] | Training_Loss: 1.880 |
Epoch: [2] | Batch: [324] | Training_Loss: 2.327 |
Epoch: [2] | Batch: [325] | Training_Loss: 0.549 |
Epoch: [2] | Batch: [326] | Training_Loss: 1.206 |
Epoch: [2] | Batch: [327] | Training_Loss: 1.019 |
Epoch: [2] | Batch: [328] | Training_Loss: 0.749 |
Epoch: [2] | Batch: [329] | Training_Loss: 1.232 |
Epoch: [2] | Batch: [330] | Training_Loss: 1.290 |
Epoch: [2] | Batch: [331] | Training_Loss: 0.423 |
Epoch: [2] | Batch: [332] | Training_Loss: 0.325 |
Epoch: [2] | Batch: [333] | Training_Loss: 0.413 |
Epoch: [2] | Batch: [334] | Training_Loss: 1.820 |
Epoch: [2] | Batch: [335] | Training_Loss: 2.395 |
Epoch: [2] | Batch: [336] | Training_Loss: 1.681 |
Epoch: [2] | Batch: [337] | Training_Loss: 1.016 |
Epoch: [2] | Batch: [338] | Training_Loss: 1.282 |
Epoch: [2] | Batch: [339] | Training_Loss: 2.071 |
Epoch: [2] | Batch: [340] | Training_Loss: 0.864 |
Epoch: [2] | Batch: [341] | Training_Loss: 0.472 |
Epoch: [2] | Batch: [342] | Training_Loss: 1.115 |
Epoch: [2] | Batch: [343] | Training_Loss: 0.511 |
Epoch: [2] | Batch: [344] | Training_Loss: 1.194 |
Epoch: [2] | Batch: [345] | Training_Loss: 1.175 |
Epoch: [2] | Batch: [346] | Training_Loss: 0.564 |
Epoch: [2] | Batch: [347] | Training_Loss: 0.314 |
Epoch: [2] | Batch: [348] | Training_Loss: 0.378 |
Epoch: [2] | Batch: [349] | Training_Loss: 1.444 |
Epoch: [2] | Batch: [350] | Training_Loss: 0.631 |
Epoch: [2] | Batch: [351] | Training_Loss: 2.010 |
Epoch: [2] | Batch: [352] | Training_Loss: 1.884 |
Epoch: [2] | Batch: [353] | Training_Loss: 1.473 |
Epoch: [2] | Batch: [354] | Training_Loss: 0.313 |
Epoch: [2] | Batch: [355] | Training_Loss: 0.310 |
Epoch: [2] | Batch: [356] | Training_Loss: 0.524 |
Epoch: [2] | Batch: [357] | Training_Loss: 0.299 |
Epoch: [2] | Batch: [358] | Training_Loss: 1.356 |
Epoch: [2] | Batch: [359] | Training_Loss: 0.343 |
Epoch: [2] | Batch: [360] | Training_Loss: 1.528 |
Epoch: [2] | Batch: [361] | Training_Loss: 1.632 |
Epoch: [2] | Batch: [362] | Training_Loss: 1.390 |
Epoch: [2] | Batch: [363] | Training_Loss: 1.695 |
Epoch: [2] | Batch: [364] | Training_Loss: 0.860 |
Epoch: [2] | Batch: [365] | Training_Loss: 1.585 |
Epoch: [2] | Batch: [366] | Training_Loss: 2.114 |
Epoch: [2] | Batch: [367] | Training_Loss: 0.973 |
Epoch: [2] | Batch: [368] | Training_Loss: 0.996 |
Epoch: [2] | Batch: [369] | Training_Loss: 1.161 |
Epoch: [2] | Batch: [370] | Training_Loss: 0.504 |
Epoch: [2] | Batch: [371] | Training_Loss: 1.941 |
Epoch: [2] | Batch: [372] | Training_Loss: 1.055 |
Epoch: [2] | Batch: [373] | Training_Loss: 1.143 |
Epoch: [2] | Batch: [374] | Training_Loss: 2.348 |
Epoch: [2] | Batch: [375] | Training_Loss: 0.289 |
Epoch: [2] | Batch: [376] | Training_Loss: 1.264 |
Epoch: [2] | Batch: [377] | Training_Loss: 1.200 |
Epoch: [2] | Batch: [378] | Training_Loss: 0.405 |
Epoch: [2] | Batch: [379] | Training_Loss: 0.399 |
Epoch: [2] | Batch: [380] | Training_Loss: 0.521 |
Epoch: [2] | Batch: [381] | Training_Loss: 2.308 |
Epoch: [2] | Batch: [382] | Training_Loss: 1.801 |
Epoch: [2] | Batch: [383] | Training_Loss: 1.310 |
Epoch: [2] | Batch: [384] | Training_Loss: 0.597 |
Epoch: [2] | Batch: [385] | Training_Loss: 0.501 |
Epoch: [2] | Batch: [386] | Training_Loss: 1.231 |
Epoch: [2] | Batch: [387] | Training_Loss: 1.525 |
Epoch: [2] | Batch: [388] | Training_Loss: 0.338 |
Epoch: [2] | Batch: [389] | Training_Loss: 1.353 |
Epoch: [2] | Batch: [390] | Training_Loss: 1.230 |
Epoch: [2] | Batch: [391] | Training_Loss: 1.441 |
Epoch: [2] | Batch: [392] | Training_Loss: 1.544 |
Epoch: [2] | Batch: [393] | Training_Loss: 2.097 |
Epoch: [2] | Batch: [394] | Training_Loss: 1.566 |
Epoch: [2] | Batch: [395] | Training_Loss: 0.712 |
Epoch: [2] | Batch: [396] | Training_Loss: 0.358 |
Epoch: [2] | Batch: [397] | Training_Loss: 0.613 |
Epoch: [2] | Batch: [398] | Training_Loss: 1.902 |
Epoch: [2] | Batch: [399] | Training_Loss: 0.464 |
Epoch: [2] | Batch: [400] | Training_Loss: 1.054 |
Epoch: [2] | Batch: [401] | Training_Loss: 0.987 |
Epoch: [2] | Batch: [402] | Training_Loss: 0.799 |
Epoch: [2] | Batch: [403] | Training_Loss: 1.326 |
Epoch: [2] | Batch: [404] | Training_Loss: 1.201 |
Epoch: [2] | Batch: [405] | Training_Loss: 0.522 |
Epoch: [2] | Batch: [406] | Training_Loss: 0.612 |
Epoch: [2] | Batch: [407] | Training_Loss: 0.287 |
Epoch: [2] | Batch: [408] | Training_Loss: 0.783 |
Epoch: [2] | Batch: [409] | Training_Loss: 1.495 |
Epoch: [2] | Batch: [410] | Training_Loss: 2.107 |
Epoch: [2] | Batch: [411] | Training_Loss: 0.268 |
Epoch: [2] | Batch: [412] | Training_Loss: 0.774 |
Epoch: [2] | Batch: [413] | Training_Loss: 1.935 |
Epoch: [2] | Batch: [414] | Training_Loss: 0.315 |
Epoch: [2] | Batch: [415] | Training_Loss: 0.300 |
Epoch: [2] | Batch: [416] | Training_Loss: 0.290 |
Epoch: [2] | Batch: [417] | Training_Loss: 2.031 |
Epoch: [2] | Batch: [418] | Training_Loss: 0.528 |
Epoch: [2] | Batch: [419] | Training_Loss: 0.587 |
Epoch: [2] | Batch: [420] | Training_Loss: 0.388 |
Epoch: [2] | Batch: [421] | Training_Loss: 0.314 |
Epoch: [2] | Batch: [422] | Training_Loss: 0.397 |
Epoch: [2] | Batch: [423] | Training_Loss: 2.246 |
Epoch: [2] | Batch: [424] | Training_Loss: 1.138 |
Epoch: [2] | Batch: [425] | Training_Loss: 0.421 |
Epoch: [2] | Batch: [426] | Training_Loss: 0.481 |
Epoch: [2] | Batch: [427] | Training_Loss: 2.111 |
Epoch: [2] | Batch: [428] | Training_Loss: 1.077 |
Epoch: [2] | Batch: [429] | Training_Loss: 0.901 |
Epoch: [2] | Batch: [430] | Training_Loss: 1.971 |
Epoch: [2] | Batch: [431] | Training_Loss: 1.501 |
Epoch: [2] | Batch: [432] | Training_Loss: 0.626 |
Epoch: [2] | Batch: [433] | Training_Loss: 1.260 |
Epoch: [2] | Batch: [434] | Training_Loss: 0.491 |
Epoch: [2] | Batch: [435] | Training_Loss: 0.556 |
Epoch: [2] | Batch: [436] | Training_Loss: 0.806 |
Epoch: [2] | Batch: [437] | Training_Loss: 2.509 |
Epoch: [2] | Batch: [438] | Training_Loss: 0.393 |
Epoch: [2] | Batch: [439] | Training_Loss: 0.988 |
Epoch: [2] | Batch: [440] | Training_Loss: 0.890 |
Epoch: [2] | Batch: [441] | Training_Loss: 0.387 |
Epoch: [2] | Batch: [442] | Training_Loss: 0.349 |
Epoch: [2] | Batch: [443] | Training_Loss: 0.351 |
Epoch: [2] | Batch: [444] | Training_Loss: 0.487 |
Epoch: [2] | Batch: [445] | Training_Loss: 1.188 |
Epoch: [2] | Batch: [446] | Training_Loss: 1.720 |
Epoch: [2] | Batch: [447] | Training_Loss: 2.176 |
Epoch: [2] | Batch: [448] | Training_Loss: 1.856 |
Epoch: [2] | Batch: [449] | Training_Loss: 0.974 |
Epoch: [2] | Batch: [450] | Training_Loss: 1.425 |
Epoch: [2] | Batch: [451] | Training_Loss: 0.835 |
Epoch: [2] | Batch: [452] | Training_Loss: 1.243 |
Epoch: [2] | Batch: [453] | Training_Loss: 0.716 |
Epoch: [2] | Batch: [454] | Training_Loss: 0.644 |
Epoch: [2] | Batch: [455] | Training_Loss: 0.676 |
Epoch: [2] | Batch: [456] | Training_Loss: 0.297 |
Epoch: [2] | Batch: [457] | Training_Loss: 0.346 |
Epoch: [2] | Batch: [458] | Training_Loss: 0.456 |
Epoch: [2] | Batch: [459] | Training_Loss: 1.743 |
Epoch: [2] | Batch: [460] | Training_Loss: 0.301 |
Epoch: [2] | Batch: [461] | Training_Loss: 1.368 |
Epoch: [2] | Batch: [462] | Training_Loss: 1.368 |
Epoch: [2] | Batch: [463] | Training_Loss: 0.448 |
Epoch: [2] | Training_Loss: 1.071 |
Epoch: [3] | Batch: [1] | Training_Loss: 0.424 |
Epoch: [3] | Batch: [2] | Training_Loss: 0.306 |
Epoch: [3] | Batch: [3] | Training_Loss: 1.944 |
Epoch: [3] | Batch: [4] | Training_Loss: 1.521 |
Epoch: [3] | Batch: [5] | Training_Loss: 0.335 |
Epoch: [3] | Batch: [6] | Training_Loss: 1.450 |
Epoch: [3] | Batch: [7] | Training_Loss: 0.418 |
Epoch: [3] | Batch: [8] | Training_Loss: 0.536 |
Epoch: [3] | Batch: [9] | Training_Loss: 0.387 |
Epoch: [3] | Batch: [10] | Training_Loss: 0.358 |
Epoch: [3] | Batch: [11] | Training_Loss: 0.356 |
Epoch: [3] | Batch: [12] | Training_Loss: 1.561 |
Epoch: [3] | Batch: [13] | Training_Loss: 0.289 |
Epoch: [3] | Batch: [14] | Training_Loss: 0.286 |
Epoch: [3] | Batch: [15] | Training_Loss: 0.532 |
Epoch: [3] | Batch: [16] | Training_Loss: 1.301 |
Epoch: [3] | Batch: [17] | Training_Loss: 1.465 |
Epoch: [3] | Batch: [18] | Training_Loss: 0.554 |
Epoch: [3] | Batch: [19] | Training_Loss: 0.392 |
Epoch: [3] | Batch: [20] | Training_Loss: 0.326 |
Epoch: [3] | Batch: [21] | Training_Loss: 0.296 |
Epoch: [3] | Batch: [22] | Training_Loss: 1.116 |
Epoch: [3] | Batch: [23] | Training_Loss: 0.335 |
Epoch: [3] | Batch: [24] | Training_Loss: 0.953 |
Epoch: [3] | Batch: [25] | Training_Loss: 0.436 |
Epoch: [3] | Batch: [26] | Training_Loss: 0.714 |
Epoch: [3] | Batch: [27] | Training_Loss: 1.643 |
Epoch: [3] | Batch: [28] | Training_Loss: 0.481 |
Epoch: [3] | Batch: [29] | Training_Loss: 0.533 |
Epoch: [3] | Batch: [30] | Training_Loss: 0.385 |
Epoch: [3] | Batch: [31] | Training_Loss: 1.210 |
Epoch: [3] | Batch: [32] | Training_Loss: 0.391 |
Epoch: [3] | Batch: [33] | Training_Loss: 0.410 |
Epoch: [3] | Batch: [34] | Training_Loss: 2.082 |
Epoch: [3] | Batch: [35] | Training_Loss: 0.694 |
Epoch: [3] | Batch: [36] | Training_Loss: 1.212 |
Epoch: [3] | Batch: [37] | Training_Loss: 0.455 |
Epoch: [3] | Batch: [38] | Training_Loss: 0.311 |
Epoch: [3] | Batch: [39] | Training_Loss: 1.054 |
Epoch: [3] | Batch: [40] | Training_Loss: 1.887 |
Epoch: [3] | Batch: [41] | Training_Loss: 0.289 |
Epoch: [3] | Batch: [42] | Training_Loss: 2.968 |
Epoch: [3] | Batch: [43] | Training_Loss: 0.859 |
Epoch: [3] | Batch: [44] | Training_Loss: 1.686 |
Epoch: [3] | Batch: [45] | Training_Loss: 1.153 |
Epoch: [3] | Batch: [46] | Training_Loss: 0.469 |
Epoch: [3] | Batch: [47] | Training_Loss: 1.699 |
Epoch: [3] | Batch: [48] | Training_Loss: 0.865 |
Epoch: [3] | Batch: [49] | Training_Loss: 0.958 |
Epoch: [3] | Batch: [50] | Training_Loss: 1.373 |
Epoch: [3] | Batch: [51] | Training_Loss: 0.294 |
Epoch: [3] | Batch: [52] | Training_Loss: 1.651 |
Epoch: [3] | Batch: [53] | Training_Loss: 1.438 |
Epoch: [3] | Batch: [54] | Training_Loss: 0.712 |
Epoch: [3] | Batch: [55] | Training_Loss: 0.958 |
Epoch: [3] | Batch: [56] | Training_Loss: 1.784 |
Epoch: [3] | Batch: [57] | Training_Loss: 1.551 |
Epoch: [3] | Batch: [58] | Training_Loss: 1.223 |
Epoch: [3] | Batch: [59] | Training_Loss: 0.523 |
Epoch: [3] | Batch: [60] | Training_Loss: 0.322 |
Epoch: [3] | Batch: [61] | Training_Loss: 1.272 |
Epoch: [3] | Batch: [62] | Training_Loss: 0.817 |
Epoch: [3] | Batch: [63] | Training_Loss: 2.232 |
Epoch: [3] | Batch: [64] | Training_Loss: 0.451 |
Epoch: [3] | Batch: [65] | Training_Loss: 0.575 |
Epoch: [3] | Batch: [66] | Training_Loss: 0.315 |
Epoch: [3] | Batch: [67] | Training_Loss: 0.898 |
Epoch: [3] | Batch: [68] | Training_Loss: 0.453 |
Epoch: [3] | Batch: [69] | Training_Loss: 0.629 |
Epoch: [3] | Batch: [70] | Training_Loss: 1.549 |
Epoch: [3] | Batch: [71] | Training_Loss: 0.529 |
Epoch: [3] | Batch: [72] | Training_Loss: 1.542 |
Epoch: [3] | Batch: [73] | Training_Loss: 0.281 |
Epoch: [3] | Batch: [74] | Training_Loss: 1.218 |
Epoch: [3] | Batch: [75] | Training_Loss: 0.393 |
Epoch: [3] | Batch: [76] | Training_Loss: 0.241 |
Epoch: [3] | Batch: [77] | Training_Loss: 0.316 |
Epoch: [3] | Batch: [78] | Training_Loss: 1.206 |
Epoch: [3] | Batch: [79] | Training_Loss: 1.277 |
Epoch: [3] | Batch: [80] | Training_Loss: 0.905 |
Epoch: [3] | Batch: [81] | Training_Loss: 0.910 |
Epoch: [3] | Batch: [82] | Training_Loss: 1.777 |
Epoch: [3] | Batch: [83] | Training_Loss: 1.824 |
Epoch: [3] | Batch: [84] | Training_Loss: 0.905 |
Epoch: [3] | Batch: [85] | Training_Loss: 0.660 |
Epoch: [3] | Batch: [86] | Training_Loss: 0.364 |
Epoch: [3] | Batch: [87] | Training_Loss: 1.272 |
Epoch: [3] | Batch: [88] | Training_Loss: 0.439 |
Epoch: [3] | Batch: [89] | Training_Loss: 1.363 |
Epoch: [3] | Batch: [90] | Training_Loss: 0.380 |
Epoch: [3] | Batch: [91] | Training_Loss: 1.200 |
Epoch: [3] | Batch: [92] | Training_Loss: 0.343 |
Epoch: [3] | Batch: [93] | Training_Loss: 0.455 |
Epoch: [3] | Batch: [94] | Training_Loss: 0.225 |
Epoch: [3] | Batch: [95] | Training_Loss: 1.363 |
Epoch: [3] | Batch: [96] | Training_Loss: 0.285 |
Epoch: [3] | Batch: [97] | Training_Loss: 0.310 |
Epoch: [3] | Batch: [98] | Training_Loss: 0.859 |
Epoch: [3] | Batch: [99] | Training_Loss: 0.385 |
Epoch: [3] | Batch: [100] | Training_Loss: 1.544 |
Epoch: [3] | Batch: [101] | Training_Loss: 0.720 |
Epoch: [3] | Batch: [102] | Training_Loss: 0.369 |
Epoch: [3] | Batch: [103] | Training_Loss: 0.440 |
Epoch: [3] | Batch: [104] | Training_Loss: 1.306 |
Epoch: [3] | Batch: [105] | Training_Loss: 0.276 |
Epoch: [3] | Batch: [106] | Training_Loss: 0.299 |
Epoch: [3] | Batch: [107] | Training_Loss: 2.193 |
Epoch: [3] | Batch: [108] | Training_Loss: 0.755 |
Epoch: [3] | Batch: [109] | Training_Loss: 0.387 |
Epoch: [3] | Batch: [110] | Training_Loss: 1.068 |
Epoch: [3] | Batch: [111] | Training_Loss: 0.360 |
Epoch: [3] | Batch: [112] | Training_Loss: 0.390 |
Epoch: [3] | Batch: [113] | Training_Loss: 0.386 |
Epoch: [3] | Batch: [114] | Training_Loss: 0.498 |
Epoch: [3] | Batch: [115] | Training_Loss: 0.332 |
Epoch: [3] | Batch: [116] | Training_Loss: 1.111 |
Epoch: [3] | Batch: [117] | Training_Loss: 0.912 |
Epoch: [3] | Batch: [118] | Training_Loss: 0.317 |
Epoch: [3] | Batch: [119] | Training_Loss: 0.352 |
Epoch: [3] | Batch: [120] | Training_Loss: 2.221 |
Epoch: [3] | Batch: [121] | Training_Loss: 1.126 |
Epoch: [3] | Batch: [122] | Training_Loss: 2.142 |
Epoch: [3] | Batch: [123] | Training_Loss: 0.887 |
Epoch: [3] | Batch: [124] | Training_Loss: 0.277 |
Epoch: [3] | Batch: [125] | Training_Loss: 0.292 |
Epoch: [3] | Batch: [126] | Training_Loss: 1.224 |
Epoch: [3] | Batch: [127] | Training_Loss: 0.338 |
Epoch: [3] | Batch: [128] | Training_Loss: 1.927 |
Epoch: [3] | Batch: [129] | Training_Loss: 0.318 |
Epoch: [3] | Batch: [130] | Training_Loss: 0.388 |
Epoch: [3] | Batch: [131] | Training_Loss: 0.402 |
Epoch: [3] | Batch: [132] | Training_Loss: 0.519 |
Epoch: [3] | Batch: [133] | Training_Loss: 0.356 |
Epoch: [3] | Batch: [134] | Training_Loss: 2.162 |
Epoch: [3] | Batch: [135] | Training_Loss: 1.957 |
Epoch: [3] | Batch: [136] | Training_Loss: 1.204 |
Epoch: [3] | Batch: [137] | Training_Loss: 1.255 |
Epoch: [3] | Batch: [138] | Training_Loss: 1.372 |
Epoch: [3] | Batch: [139] | Training_Loss: 1.591 |
Epoch: [3] | Batch: [140] | Training_Loss: 2.660 |
Epoch: [3] | Batch: [141] | Training_Loss: 0.254 |
Epoch: [3] | Batch: [142] | Training_Loss: 1.691 |
Epoch: [3] | Batch: [143] | Training_Loss: 1.201 |
Epoch: [3] | Batch: [144] | Training_Loss: 0.614 |
Epoch: [3] | Batch: [145] | Training_Loss: 1.941 |
Epoch: [3] | Batch: [146] | Training_Loss: 0.709 |
Epoch: [3] | Batch: [147] | Training_Loss: 0.647 |
Epoch: [3] | Batch: [148] | Training_Loss: 1.492 |
Epoch: [3] | Batch: [149] | Training_Loss: 1.242 |
Epoch: [3] | Batch: [150] | Training_Loss: 0.433 |
Epoch: [3] | Batch: [151] | Training_Loss: 1.623 |
Epoch: [3] | Batch: [152] | Training_Loss: 0.357 |
Epoch: [3] | Batch: [153] | Training_Loss: 1.448 |
Epoch: [3] | Batch: [154] | Training_Loss: 0.359 |
Epoch: [3] | Batch: [155] | Training_Loss: 0.351 |
Epoch: [3] | Batch: [156] | Training_Loss: 0.259 |
Epoch: [3] | Batch: [157] | Training_Loss: 1.840 |
Epoch: [3] | Batch: [158] | Training_Loss: 0.544 |
Epoch: [3] | Batch: [159] | Training_Loss: 1.106 |
Epoch: [3] | Batch: [160] | Training_Loss: 0.226 |
Epoch: [3] | Batch: [161] | Training_Loss: 0.409 |
Epoch: [3] | Batch: [162] | Training_Loss: 0.597 |
Epoch: [3] | Batch: [163] | Training_Loss: 1.706 |
Epoch: [3] | Batch: [164] | Training_Loss: 2.465 |
Epoch: [3] | Batch: [165] | Training_Loss: 0.299 |
Epoch: [3] | Batch: [166] | Training_Loss: 1.806 |
Epoch: [3] | Batch: [167] | Training_Loss: 1.652 |
Epoch: [3] | Batch: [168] | Training_Loss: 1.264 |
Epoch: [3] | Batch: [169] | Training_Loss: 0.555 |
Epoch: [3] | Batch: [170] | Training_Loss: 0.534 |
Epoch: [3] | Batch: [171] | Training_Loss: 1.278 |
Epoch: [3] | Batch: [172] | Training_Loss: 1.103 |
Epoch: [3] | Batch: [173] | Training_Loss: 2.316 |
Epoch: [3] | Batch: [174] | Training_Loss: 0.307 |
Epoch: [3] | Batch: [175] | Training_Loss: 0.357 |
Epoch: [3] | Batch: [176] | Training_Loss: 1.073 |
Epoch: [3] | Batch: [177] | Training_Loss: 0.763 |
Epoch: [3] | Batch: [178] | Training_Loss: 0.488 |
Epoch: [3] | Batch: [179] | Training_Loss: 0.622 |
Epoch: [3] | Batch: [180] | Training_Loss: 1.979 |
Epoch: [3] | Batch: [181] | Training_Loss: 0.718 |
Epoch: [3] | Batch: [182] | Training_Loss: 2.343 |
Epoch: [3] | Batch: [183] | Training_Loss: 0.277 |
Epoch: [3] | Batch: [184] | Training_Loss: 1.400 |
Epoch: [3] | Batch: [185] | Training_Loss: 0.455 |
Epoch: [3] | Batch: [186] | Training_Loss: 0.252 |
Epoch: [3] | Batch: [187] | Training_Loss: 1.582 |
Epoch: [3] | Batch: [188] | Training_Loss: 0.653 |
Epoch: [3] | Batch: [189] | Training_Loss: 2.006 |
Epoch: [3] | Batch: [190] | Training_Loss: 0.845 |
Epoch: [3] | Batch: [191] | Training_Loss: 0.921 |
Epoch: [3] | Batch: [192] | Training_Loss: 0.411 |
Epoch: [3] | Batch: [193] | Training_Loss: 1.742 |
Epoch: [3] | Batch: [194] | Training_Loss: 1.343 |
Epoch: [3] | Batch: [195] | Training_Loss: 0.769 |
Epoch: [3] | Batch: [196] | Training_Loss: 0.347 |
Epoch: [3] | Batch: [197] | Training_Loss: 0.255 |
Epoch: [3] | Batch: [198] | Training_Loss: 1.323 |
Epoch: [3] | Batch: [199] | Training_Loss: 1.878 |
Epoch: [3] | Batch: [200] | Training_Loss: 0.906 |
Epoch: [3] | Batch: [201] | Training_Loss: 0.271 |
Epoch: [3] | Batch: [202] | Training_Loss: 0.318 |
Epoch: [3] | Batch: [203] | Training_Loss: 0.376 |
Epoch: [3] | Batch: [204] | Training_Loss: 0.330 |
Epoch: [3] | Batch: [205] | Training_Loss: 0.468 |
Epoch: [3] | Batch: [206] | Training_Loss: 0.214 |
Epoch: [3] | Batch: [207] | Training_Loss: 2.088 |
Epoch: [3] | Batch: [208] | Training_Loss: 0.597 |
Epoch: [3] | Batch: [209] | Training_Loss: 0.776 |
Epoch: [3] | Batch: [210] | Training_Loss: 0.586 |
Epoch: [3] | Batch: [211] | Training_Loss: 1.663 |
Epoch: [3] | Batch: [212] | Training_Loss: 0.237 |
Epoch: [3] | Batch: [213] | Training_Loss: 0.279 |
Epoch: [3] | Batch: [214] | Training_Loss: 2.027 |
Epoch: [3] | Batch: [215] | Training_Loss: 1.327 |
Epoch: [3] | Batch: [216] | Training_Loss: 0.366 |
Epoch: [3] | Batch: [217] | Training_Loss: 0.236 |
Epoch: [3] | Batch: [218] | Training_Loss: 0.648 |
Epoch: [3] | Batch: [219] | Training_Loss: 0.690 |
Epoch: [3] | Batch: [220] | Training_Loss: 1.850 |
Epoch: [3] | Batch: [221] | Training_Loss: 0.428 |
Epoch: [3] | Batch: [222] | Training_Loss: 0.415 |
Epoch: [3] | Batch: [223] | Training_Loss: 1.480 |
Epoch: [3] | Batch: [224] | Training_Loss: 0.549 |
Epoch: [3] | Batch: [225] | Training_Loss: 0.412 |
Epoch: [3] | Batch: [226] | Training_Loss: 1.144 |
Epoch: [3] | Batch: [227] | Training_Loss: 1.713 |
Epoch: [3] | Batch: [228] | Training_Loss: 2.065 |
Epoch: [3] | Batch: [229] | Training_Loss: 1.404 |
Epoch: [3] | Batch: [230] | Training_Loss: 0.420 |
Epoch: [3] | Batch: [231] | Training_Loss: 1.583 |
Epoch: [3] | Batch: [232] | Training_Loss: 0.188 |
Epoch: [3] | Batch: [233] | Training_Loss: 1.530 |
Epoch: [3] | Batch: [234] | Training_Loss: 0.357 |
Epoch: [3] | Batch: [235] | Training_Loss: 1.096 |
Epoch: [3] | Batch: [236] | Training_Loss: 1.186 |
Epoch: [3] | Batch: [237] | Training_Loss: 0.359 |
Epoch: [3] | Batch: [238] | Training_Loss: 0.295 |
Epoch: [3] | Batch: [239] | Training_Loss: 0.239 |
Epoch: [3] | Batch: [240] | Training_Loss: 0.373 |
Epoch: [3] | Batch: [241] | Training_Loss: 1.926 |
Epoch: [3] | Batch: [242] | Training_Loss: 0.350 |
Epoch: [3] | Batch: [243] | Training_Loss: 2.090 |
Epoch: [3] | Batch: [244] | Training_Loss: 0.688 |
Epoch: [3] | Batch: [245] | Training_Loss: 0.943 |
Epoch: [3] | Batch: [246] | Training_Loss: 1.196 |
Epoch: [3] | Batch: [247] | Training_Loss: 1.482 |
Epoch: [3] | Batch: [248] | Training_Loss: 1.330 |
Epoch: [3] | Batch: [249] | Training_Loss: 0.907 |
Epoch: [3] | Batch: [250] | Training_Loss: 0.610 |
Epoch: [3] | Batch: [251] | Training_Loss: 1.456 |
Epoch: [3] | Batch: [252] | Training_Loss: 1.530 |
Epoch: [3] | Batch: [253] | Training_Loss: 1.904 |
Epoch: [3] | Batch: [254] | Training_Loss: 0.489 |
Epoch: [3] | Batch: [255] | Training_Loss: 0.948 |
Epoch: [3] | Batch: [256] | Training_Loss: 1.036 |
Epoch: [3] | Batch: [257] | Training_Loss: 1.178 |
Epoch: [3] | Batch: [258] | Training_Loss: 1.730 |
Epoch: [3] | Batch: [259] | Training_Loss: 0.285 |
Epoch: [3] | Batch: [260] | Training_Loss: 0.427 |
Epoch: [3] | Batch: [261] | Training_Loss: 1.699 |
Epoch: [3] | Batch: [262] | Training_Loss: 1.263 |
Epoch: [3] | Batch: [263] | Training_Loss: 0.277 |
Epoch: [3] | Batch: [264] | Training_Loss: 1.413 |
Epoch: [3] | Batch: [265] | Training_Loss: 0.297 |
Epoch: [3] | Batch: [266] | Training_Loss: 0.256 |
Epoch: [3] | Batch: [267] | Training_Loss: 0.880 |
Epoch: [3] | Batch: [268] | Training_Loss: 1.669 |
Epoch: [3] | Batch: [269] | Training_Loss: 2.073 |
Epoch: [3] | Batch: [270] | Training_Loss: 0.504 |
Epoch: [3] | Batch: [271] | Training_Loss: 0.531 |
Epoch: [3] | Batch: [272] | Training_Loss: 0.261 |
Epoch: [3] | Batch: [273] | Training_Loss: 1.165 |
Epoch: [3] | Batch: [274] | Training_Loss: 1.923 |
Epoch: [3] | Batch: [275] | Training_Loss: 2.412 |
Epoch: [3] | Batch: [276] | Training_Loss: 0.212 |
Epoch: [3] | Batch: [277] | Training_Loss: 1.073 |
Epoch: [3] | Batch: [278] | Training_Loss: 1.150 |
Epoch: [3] | Batch: [279] | Training_Loss: 1.089 |
Epoch: [3] | Batch: [280] | Training_Loss: 0.256 |
Epoch: [3] | Batch: [281] | Training_Loss: 0.223 |
Epoch: [3] | Batch: [282] | Training_Loss: 1.137 |
Epoch: [3] | Batch: [283] | Training_Loss: 1.105 |
Epoch: [3] | Batch: [284] | Training_Loss: 1.096 |
Epoch: [3] | Batch: [285] | Training_Loss: 1.017 |
Epoch: [3] | Batch: [286] | Training_Loss: 0.512 |
Epoch: [3] | Batch: [287] | Training_Loss: 0.264 |
Epoch: [3] | Batch: [288] | Training_Loss: 1.354 |
Epoch: [3] | Batch: [289] | Training_Loss: 0.353 |
Epoch: [3] | Batch: [290] | Training_Loss: 1.906 |
Epoch: [3] | Batch: [291] | Training_Loss: 0.686 |
Epoch: [3] | Batch: [292] | Training_Loss: 0.404 |
Epoch: [3] | Batch: [293] | Training_Loss: 0.616 |
Epoch: [3] | Batch: [294] | Training_Loss: 0.403 |
Epoch: [3] | Batch: [295] | Training_Loss: 1.302 |
Epoch: [3] | Batch: [296] | Training_Loss: 2.062 |
Epoch: [3] | Batch: [297] | Training_Loss: 1.750 |
Epoch: [3] | Batch: [298] | Training_Loss: 1.221 |
Epoch: [3] | Batch: [299] | Training_Loss: 0.876 |
Epoch: [3] | Batch: [300] | Training_Loss: 1.526 |
Epoch: [3] | Batch: [301] | Training_Loss: 2.394 |
Epoch: [3] | Batch: [302] | Training_Loss: 0.307 |
Epoch: [3] | Batch: [303] | Training_Loss: 0.291 |
Epoch: [3] | Batch: [304] | Training_Loss: 1.125 |
Epoch: [3] | Batch: [305] | Training_Loss: 0.531 |
Epoch: [3] | Batch: [306] | Training_Loss: 0.256 |
Epoch: [3] | Batch: [307] | Training_Loss: 0.223 |
Epoch: [3] | Batch: [308] | Training_Loss: 0.432 |
Epoch: [3] | Batch: [309] | Training_Loss: 0.291 |
Epoch: [3] | Batch: [310] | Training_Loss: 0.644 |
Epoch: [3] | Batch: [311] | Training_Loss: 0.554 |
Epoch: [3] | Batch: [312] | Training_Loss: 0.472 |
Epoch: [3] | Batch: [313] | Training_Loss: 0.349 |
Epoch: [3] | Batch: [314] | Training_Loss: 2.072 |
Epoch: [3] | Batch: [315] | Training_Loss: 0.248 |
Epoch: [3] | Batch: [316] | Training_Loss: 0.336 |
Epoch: [3] | Batch: [317] | Training_Loss: 2.043 |
Epoch: [3] | Batch: [318] | Training_Loss: 0.914 |
Epoch: [3] | Batch: [319] | Training_Loss: 0.729 |
Epoch: [3] | Batch: [320] | Training_Loss: 0.371 |
Epoch: [3] | Batch: [321] | Training_Loss: 0.256 |
Epoch: [3] | Batch: [322] | Training_Loss: 1.313 |
Epoch: [3] | Batch: [323] | Training_Loss: 0.216 |
Epoch: [3] | Batch: [324] | Training_Loss: 0.229 |
Epoch: [3] | Batch: [325] | Training_Loss: 1.950 |
Epoch: [3] | Batch: [326] | Training_Loss: 0.498 |
Epoch: [3] | Batch: [327] | Training_Loss: 0.591 |
Epoch: [3] | Batch: [328] | Training_Loss: 1.822 |
Epoch: [3] | Batch: [329] | Training_Loss: 0.258 |
Epoch: [3] | Batch: [330] | Training_Loss: 0.226 |
Epoch: [3] | Batch: [331] | Training_Loss: 1.592 |
Epoch: [3] | Batch: [332] | Training_Loss: 1.710 |
Epoch: [3] | Batch: [333] | Training_Loss: 0.411 |
Epoch: [3] | Batch: [334] | Training_Loss: 0.348 |
Epoch: [3] | Batch: [335] | Training_Loss: 2.258 |
Epoch: [3] | Batch: [336] | Training_Loss: 0.218 |
Epoch: [3] | Batch: [337] | Training_Loss: 1.068 |
Epoch: [3] | Batch: [338] | Training_Loss: 0.574 |
Epoch: [3] | Batch: [339] | Training_Loss: 1.338 |
Epoch: [3] | Batch: [340] | Training_Loss: 1.472 |
Epoch: [3] | Batch: [341] | Training_Loss: 0.321 |
Epoch: [3] | Batch: [342] | Training_Loss: 0.904 |
Epoch: [3] | Batch: [343] | Training_Loss: 0.392 |
Epoch: [3] | Batch: [344] | Training_Loss: 0.257 |
Epoch: [3] | Batch: [345] | Training_Loss: 1.480 |
Epoch: [3] | Batch: [346] | Training_Loss: 1.329 |
Epoch: [3] | Batch: [347] | Training_Loss: 0.492 |
Epoch: [3] | Batch: [348] | Training_Loss: 0.392 |
Epoch: [3] | Batch: [349] | Training_Loss: 2.348 |
Epoch: [3] | Batch: [350] | Training_Loss: 1.465 |
Epoch: [3] | Batch: [351] | Training_Loss: 0.419 |
Epoch: [3] | Batch: [352] | Training_Loss: 0.245 |
Epoch: [3] | Batch: [353] | Training_Loss: 0.200 |
Epoch: [3] | Batch: [354] | Training_Loss: 1.706 |
Epoch: [3] | Batch: [355] | Training_Loss: 0.684 |
Epoch: [3] | Batch: [356] | Training_Loss: 0.288 |
Epoch: [3] | Batch: [357] | Training_Loss: 1.386 |
Epoch: [3] | Batch: [358] | Training_Loss: 1.041 |
Epoch: [3] | Batch: [359] | Training_Loss: 1.924 |
Epoch: [3] | Batch: [360] | Training_Loss: 0.350 |
Epoch: [3] | Batch: [361] | Training_Loss: 2.113 |
Epoch: [3] | Batch: [362] | Training_Loss: 1.181 |
Epoch: [3] | Batch: [363] | Training_Loss: 0.668 |
Epoch: [3] | Batch: [364] | Training_Loss: 0.286 |
Epoch: [3] | Batch: [365] | Training_Loss: 0.747 |
Epoch: [3] | Batch: [366] | Training_Loss: 0.305 |
Epoch: [3] | Batch: [367] | Training_Loss: 1.361 |
Epoch: [3] | Batch: [368] | Training_Loss: 0.305 |
Epoch: [3] | Batch: [369] | Training_Loss: 0.796 |
Epoch: [3] | Batch: [370] | Training_Loss: 1.124 |
Epoch: [3] | Batch: [371] | Training_Loss: 1.739 |
Epoch: [3] | Batch: [372] | Training_Loss: 1.658 |
Epoch: [3] | Batch: [373] | Training_Loss: 0.294 |
Epoch: [3] | Batch: [374] | Training_Loss: 1.008 |
Epoch: [3] | Batch: [375] | Training_Loss: 0.230 |
Epoch: [3] | Batch: [376] | Training_Loss: 1.039 |
Epoch: [3] | Batch: [377] | Training_Loss: 0.355 |
Epoch: [3] | Batch: [378] | Training_Loss: 0.406 |
Epoch: [3] | Batch: [379] | Training_Loss: 2.211 |
Epoch: [3] | Batch: [380] | Training_Loss: 0.788 |
Epoch: [3] | Batch: [381] | Training_Loss: 0.390 |
Epoch: [3] | Batch: [382] | Training_Loss: 1.049 |
Epoch: [3] | Batch: [383] | Training_Loss: 1.662 |
Epoch: [3] | Batch: [384] | Training_Loss: 0.586 |
Epoch: [3] | Batch: [385] | Training_Loss: 0.477 |
Epoch: [3] | Batch: [386] | Training_Loss: 1.568 |
Epoch: [3] | Batch: [387] | Training_Loss: 0.882 |
Epoch: [3] | Batch: [388] | Training_Loss: 0.309 |
Epoch: [3] | Batch: [389] | Training_Loss: 0.594 |
Epoch: [3] | Batch: [390] | Training_Loss: 0.224 |
Epoch: [3] | Batch: [391] | Training_Loss: 1.095 |
Epoch: [3] | Batch: [392] | Training_Loss: 2.570 |
Epoch: [3] | Batch: [393] | Training_Loss: 0.235 |
Epoch: [3] | Batch: [394] | Training_Loss: 0.371 |
Epoch: [3] | Batch: [395] | Training_Loss: 0.294 |
Epoch: [3] | Batch: [396] | Training_Loss: 1.915 |
Epoch: [3] | Batch: [397] | Training_Loss: 1.374 |
Epoch: [3] | Batch: [398] | Training_Loss: 1.649 |
Epoch: [3] | Batch: [399] | Training_Loss: 2.045 |
Epoch: [3] | Batch: [400] | Training_Loss: 0.553 |
Epoch: [3] | Batch: [401] | Training_Loss: 0.342 |
Epoch: [3] | Batch: [402] | Training_Loss: 0.222 |
Epoch: [3] | Batch: [403] | Training_Loss: 0.972 |
Epoch: [3] | Batch: [404] | Training_Loss: 1.432 |
Epoch: [3] | Batch: [405] | Training_Loss: 0.461 |
Epoch: [3] | Batch: [406] | Training_Loss: 1.061 |
Epoch: [3] | Batch: [407] | Training_Loss: 1.898 |
Epoch: [3] | Batch: [408] | Training_Loss: 0.310 |
Epoch: [3] | Batch: [409] | Training_Loss: 0.592 |
Epoch: [3] | Batch: [410] | Training_Loss: 1.000 |
Epoch: [3] | Batch: [411] | Training_Loss: 0.626 |
Epoch: [3] | Batch: [412] | Training_Loss: 1.409 |
Epoch: [3] | Batch: [413] | Training_Loss: 2.009 |
Epoch: [3] | Batch: [414] | Training_Loss: 1.664 |
Epoch: [3] | Batch: [415] | Training_Loss: 1.663 |
Epoch: [3] | Batch: [416] | Training_Loss: 1.921 |
Epoch: [3] | Batch: [417] | Training_Loss: 0.166 |
Epoch: [3] | Batch: [418] | Training_Loss: 1.336 |
Epoch: [3] | Batch: [419] | Training_Loss: 0.622 |
Epoch: [3] | Batch: [420] | Training_Loss: 0.235 |
Epoch: [3] | Batch: [421] | Training_Loss: 1.103 |
Epoch: [3] | Batch: [422] | Training_Loss: 0.795 |
Epoch: [3] | Batch: [423] | Training_Loss: 0.819 |
Epoch: [3] | Batch: [424] | Training_Loss: 1.900 |
Epoch: [3] | Batch: [425] | Training_Loss: 0.429 |
Epoch: [3] | Batch: [426] | Training_Loss: 1.744 |
Epoch: [3] | Batch: [427] | Training_Loss: 1.262 |
Epoch: [3] | Batch: [428] | Training_Loss: 0.861 |
Epoch: [3] | Batch: [429] | Training_Loss: 1.643 |
Epoch: [3] | Batch: [430] | Training_Loss: 1.124 |
Epoch: [3] | Batch: [431] | Training_Loss: 1.092 |
Epoch: [3] | Batch: [432] | Training_Loss: 1.099 |
Epoch: [3] | Batch: [433] | Training_Loss: 0.312 |
Epoch: [3] | Batch: [434] | Training_Loss: 1.816 |
Epoch: [3] | Batch: [435] | Training_Loss: 0.309 |
Epoch: [3] | Batch: [436] | Training_Loss: 1.153 |
Epoch: [3] | Batch: [437] | Training_Loss: 0.418 |
Epoch: [3] | Batch: [438] | Training_Loss: 0.234 |
Epoch: [3] | Batch: [439] | Training_Loss: 1.688 |
Epoch: [3] | Batch: [440] | Training_Loss: 0.407 |
Epoch: [3] | Batch: [441] | Training_Loss: 2.140 |
Epoch: [3] | Batch: [442] | Training_Loss: 0.263 |
Epoch: [3] | Batch: [443] | Training_Loss: 0.239 |
Epoch: [3] | Batch: [444] | Training_Loss: 2.294 |
Epoch: [3] | Batch: [445] | Training_Loss: 0.196 |
Epoch: [3] | Batch: [446] | Training_Loss: 0.252 |
Epoch: [3] | Batch: [447] | Training_Loss: 0.407 |
Epoch: [3] | Batch: [448] | Training_Loss: 2.753 |
Epoch: [3] | Batch: [449] | Training_Loss: 1.973 |
Epoch: [3] | Batch: [450] | Training_Loss: 0.232 |
Epoch: [3] | Batch: [451] | Training_Loss: 1.381 |
Epoch: [3] | Batch: [452] | Training_Loss: 1.497 |
Epoch: [3] | Batch: [453] | Training_Loss: 1.106 |
Epoch: [3] | Batch: [454] | Training_Loss: 2.572 |
Epoch: [3] | Batch: [455] | Training_Loss: 1.278 |
Epoch: [3] | Batch: [456] | Training_Loss: 0.474 |
Epoch: [3] | Batch: [457] | Training_Loss: 1.415 |
Epoch: [3] | Batch: [458] | Training_Loss: 0.134 |
Epoch: [3] | Batch: [459] | Training_Loss: 0.944 |
Epoch: [3] | Batch: [460] | Training_Loss: 1.826 |
Epoch: [3] | Batch: [461] | Training_Loss: 0.258 |
Epoch: [3] | Batch: [462] | Training_Loss: 0.908 |
Epoch: [3] | Batch: [463] | Training_Loss: 1.353 |
Epoch: [3] | Training_Loss: 0.941 |
Epoch: [4] | Batch: [1] | Training_Loss: 2.293 | Total_Accuracy: 95.441 | Ones_Accuracy: 35.417 | Zeros_Accuracy: 97.713 |
Epoch: [4] | Batch: [2] | Training_Loss: 0.308 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [3] | Training_Loss: 1.384 | Total_Accuracy: 97.500 | Ones_Accuracy: 86.667 | Zeros_Accuracy: 98.621 |
Epoch: [4] | Batch: [4] | Training_Loss: 1.179 | Total_Accuracy: 93.478 | Ones_Accuracy: 33.333 | Zeros_Accuracy: 97.674 |
Epoch: [4] | Batch: [5] | Training_Loss: 0.245 | Total_Accuracy: 99.737 | Ones_Accuracy: 95.652 | Zeros_Accuracy: 99.838 |
Epoch: [4] | Batch: [6] | Training_Loss: 1.835 | Total_Accuracy: 96.352 | Ones_Accuracy: 55.263 | Zeros_Accuracy: 98.098 |
Epoch: [4] | Batch: [7] | Training_Loss: 0.462 | Total_Accuracy: 99.760 | Ones_Accuracy: 93.333 | Zeros_Accuracy: 99.938 |
Epoch: [4] | Batch: [8] | Training_Loss: 1.628 | Total_Accuracy: 96.253 | Ones_Accuracy: 37.931 | Zeros_Accuracy: 98.122 |
Epoch: [4] | Batch: [9] | Training_Loss: 0.333 | Total_Accuracy: 99.165 | Ones_Accuracy: 84.091 | Zeros_Accuracy: 99.603 |
Epoch: [4] | Batch: [10] | Training_Loss: 0.203 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [11] | Training_Loss: 0.218 | Total_Accuracy: 99.829 | Ones_Accuracy: 96.078 | Zeros_Accuracy: 99.913 |
Epoch: [4] | Batch: [12] | Training_Loss: 1.212 | Total_Accuracy: 96.975 | Ones_Accuracy: 58.140 | Zeros_Accuracy: 98.520 |
Epoch: [4] | Batch: [13] | Training_Loss: 2.436 | Total_Accuracy: 94.961 | Ones_Accuracy: 53.571 | Zeros_Accuracy: 97.336 |
Epoch: [4] | Batch: [14] | Training_Loss: 0.691 | Total_Accuracy: 99.167 | Ones_Accuracy: 84.483 | Zeros_Accuracy: 99.572 |
Epoch: [4] | Batch: [15] | Training_Loss: 0.187 | Total_Accuracy: 99.912 | Ones_Accuracy: 98.113 | Zeros_Accuracy: 99.955 |
Epoch: [4] | Batch: [16] | Training_Loss: 0.297 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [17] | Training_Loss: 0.250 | Total_Accuracy: 99.749 | Ones_Accuracy: 94.444 | Zeros_Accuracy: 99.871 |
Epoch: [4] | Batch: [18] | Training_Loss: 2.201 | Total_Accuracy: 94.695 | Ones_Accuracy: 34.615 | Zeros_Accuracy: 97.315 |
Epoch: [4] | Batch: [19] | Training_Loss: 0.241 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [20] | Training_Loss: 1.991 | Total_Accuracy: 96.003 | Ones_Accuracy: 32.558 | Zeros_Accuracy: 98.012 |
Epoch: [4] | Batch: [21] | Training_Loss: 0.696 | Total_Accuracy: 97.414 | Ones_Accuracy: 81.818 | Zeros_Accuracy: 99.048 |
Epoch: [4] | Batch: [22] | Training_Loss: 1.162 | Total_Accuracy: 97.368 | Ones_Accuracy: 75.000 | Zeros_Accuracy: 98.611 |
Epoch: [4] | Batch: [23] | Training_Loss: 0.296 | Total_Accuracy: 99.253 | Ones_Accuracy: 84.746 | Zeros_Accuracy: 99.664 |
Epoch: [4] | Batch: [24] | Training_Loss: 0.914 | Total_Accuracy: 99.206 | Ones_Accuracy: 94.444 | Zeros_Accuracy: 99.573 |
Epoch: [4] | Batch: [25] | Training_Loss: 0.456 | Total_Accuracy: 99.409 | Ones_Accuracy: 87.234 | Zeros_Accuracy: 99.757 |
Epoch: [4] | Batch: [26] | Training_Loss: 0.214 | Total_Accuracy: 99.700 | Ones_Accuracy: 92.982 | Zeros_Accuracy: 99.868 |
Epoch: [4] | Batch: [27] | Training_Loss: 2.310 | Total_Accuracy: 92.958 | Ones_Accuracy: 30.303 | Zeros_Accuracy: 96.370 |
Epoch: [4] | Batch: [28] | Training_Loss: 1.337 | Total_Accuracy: 94.444 | Ones_Accuracy: 64.286 | Zeros_Accuracy: 97.692 |
Epoch: [4] | Batch: [29] | Training_Loss: 0.206 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [30] | Training_Loss: 1.008 | Total_Accuracy: 98.182 | Ones_Accuracy: 88.889 | Zeros_Accuracy: 99.010 |
Epoch: [4] | Batch: [31] | Training_Loss: 1.621 | Total_Accuracy: 95.923 | Ones_Accuracy: 50.000 | Zeros_Accuracy: 97.875 |
Epoch: [4] | Batch: [32] | Training_Loss: 0.671 | Total_Accuracy: 99.400 | Ones_Accuracy: 88.679 | Zeros_Accuracy: 99.719 |
Epoch: [4] | Batch: [33] | Training_Loss: 0.313 | Total_Accuracy: 99.729 | Ones_Accuracy: 92.500 | Zeros_Accuracy: 99.930 |
Epoch: [4] | Batch: [34] | Training_Loss: 0.212 | Total_Accuracy: 99.976 | Ones_Accuracy: 98.876 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [35] | Training_Loss: 1.167 | Total_Accuracy: 94.608 | Ones_Accuracy: 41.667 | Zeros_Accuracy: 97.917 |
Epoch: [4] | Batch: [36] | Training_Loss: 1.419 | Total_Accuracy: 97.743 | Ones_Accuracy: 67.857 | Zeros_Accuracy: 98.771 |
Epoch: [4] | Batch: [37] | Training_Loss: 1.794 | Total_Accuracy: 96.185 | Ones_Accuracy: 45.714 | Zeros_Accuracy: 98.023 |
Epoch: [4] | Batch: [38] | Training_Loss: 1.966 | Total_Accuracy: 93.676 | Ones_Accuracy: 55.000 | Zeros_Accuracy: 96.996 |
Epoch: [4] | Batch: [39] | Training_Loss: 0.212 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [40] | Training_Loss: 0.130 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [41] | Training_Loss: 0.897 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [42] | Training_Loss: 0.693 | Total_Accuracy: 98.867 | Ones_Accuracy: 81.818 | Zeros_Accuracy: 99.415 |
Epoch: [4] | Batch: [43] | Training_Loss: 1.804 | Total_Accuracy: 96.135 | Ones_Accuracy: 50.000 | Zeros_Accuracy: 98.039 |
Epoch: [4] | Batch: [44] | Training_Loss: 1.131 | Total_Accuracy: 98.936 | Ones_Accuracy: 92.857 | Zeros_Accuracy: 99.425 |
Epoch: [4] | Batch: [45] | Training_Loss: 0.262 | Total_Accuracy: 99.698 | Ones_Accuracy: 92.982 | Zeros_Accuracy: 99.867 |
Epoch: [4] | Batch: [46] | Training_Loss: 0.193 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [47] | Training_Loss: 0.229 | Total_Accuracy: 99.781 | Ones_Accuracy: 94.737 | Zeros_Accuracy: 99.910 |
Epoch: [4] | Batch: [48] | Training_Loss: 0.197 | Total_Accuracy: 99.689 | Ones_Accuracy: 94.118 | Zeros_Accuracy: 99.840 |
Epoch: [4] | Batch: [49] | Training_Loss: 0.518 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [50] | Training_Loss: 1.394 | Total_Accuracy: 94.991 | Ones_Accuracy: 30.435 | Zeros_Accuracy: 97.868 |
Epoch: [4] | Batch: [51] | Training_Loss: 0.538 | Total_Accuracy: 99.577 | Ones_Accuracy: 93.333 | Zeros_Accuracy: 99.782 |
Epoch: [4] | Batch: [52] | Training_Loss: 1.584 | Total_Accuracy: 96.364 | Ones_Accuracy: 70.000 | Zeros_Accuracy: 98.065 |
Epoch: [4] | Batch: [53] | Training_Loss: 1.584 | Total_Accuracy: 97.035 | Ones_Accuracy: 64.706 | Zeros_Accuracy: 98.588 |
Epoch: [4] | Batch: [54] | Training_Loss: 1.787 | Total_Accuracy: 96.011 | Ones_Accuracy: 72.000 | Zeros_Accuracy: 97.721 |
Epoch: [4] | Batch: [55] | Training_Loss: 1.254 | Total_Accuracy: 97.484 | Ones_Accuracy: 73.913 | Zeros_Accuracy: 98.678 |
Epoch: [4] | Batch: [56] | Training_Loss: 2.567 | Total_Accuracy: 92.322 | Ones_Accuracy: 13.636 | Zeros_Accuracy: 95.792 |
Epoch: [4] | Batch: [57] | Training_Loss: 1.497 | Total_Accuracy: 94.444 | Ones_Accuracy: 41.667 | Zeros_Accuracy: 97.849 |
Epoch: [4] | Batch: [58] | Training_Loss: 0.929 | Total_Accuracy: 97.130 | Ones_Accuracy: 59.524 | Zeros_Accuracy: 98.602 |
Epoch: [4] | Batch: [59] | Training_Loss: 0.212 | Total_Accuracy: 99.927 | Ones_Accuracy: 97.561 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [60] | Training_Loss: 0.691 | Total_Accuracy: 99.030 | Ones_Accuracy: 83.673 | Zeros_Accuracy: 99.533 |
Epoch: [4] | Batch: [61] | Training_Loss: 0.323 | Total_Accuracy: 99.680 | Ones_Accuracy: 94.545 | Zeros_Accuracy: 99.835 |
Epoch: [4] | Batch: [62] | Training_Loss: 0.365 | Total_Accuracy: 99.638 | Ones_Accuracy: 91.837 | Zeros_Accuracy: 99.841 |
Epoch: [4] | Batch: [63] | Training_Loss: 2.026 | Total_Accuracy: 93.309 | Ones_Accuracy: 25.000 | Zeros_Accuracy: 96.952 |
Epoch: [4] | Batch: [64] | Training_Loss: 0.402 | Total_Accuracy: 99.364 | Ones_Accuracy: 86.364 | Zeros_Accuracy: 99.781 |
Epoch: [4] | Batch: [65] | Training_Loss: 1.660 | Total_Accuracy: 95.038 | Ones_Accuracy: 48.000 | Zeros_Accuracy: 97.395 |
Epoch: [4] | Batch: [66] | Training_Loss: 0.301 | Total_Accuracy: 99.408 | Ones_Accuracy: 88.235 | Zeros_Accuracy: 99.697 |
Epoch: [4] | Batch: [67] | Training_Loss: 0.926 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [68] | Training_Loss: 0.713 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [69] | Training_Loss: 1.570 | Total_Accuracy: 93.514 | Ones_Accuracy: 60.000 | Zeros_Accuracy: 96.471 |
Epoch: [4] | Batch: [70] | Training_Loss: 0.762 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [71] | Training_Loss: 0.701 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [72] | Training_Loss: 1.750 | Total_Accuracy: 95.122 | Ones_Accuracy: 39.394 | Zeros_Accuracy: 97.587 |
Epoch: [4] | Batch: [73] | Training_Loss: 2.049 | Total_Accuracy: 95.376 | Ones_Accuracy: 63.636 | Zeros_Accuracy: 97.531 |
Epoch: [4] | Batch: [74] | Training_Loss: 1.355 | Total_Accuracy: 97.835 | Ones_Accuracy: 78.571 | Zeros_Accuracy: 98.958 |
Epoch: [4] | Batch: [75] | Training_Loss: 0.222 | Total_Accuracy: 99.948 | Ones_Accuracy: 98.851 | Zeros_Accuracy: 99.974 |
Epoch: [4] | Batch: [76] | Training_Loss: 1.922 | Total_Accuracy: 95.977 | Ones_Accuracy: 43.590 | Zeros_Accuracy: 98.010 |
Epoch: [4] | Batch: [77] | Training_Loss: 0.640 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [78] | Training_Loss: 1.164 | Total_Accuracy: 97.495 | Ones_Accuracy: 77.778 | Zeros_Accuracy: 98.673 |
Epoch: [4] | Batch: [79] | Training_Loss: 0.232 | Total_Accuracy: 99.913 | Ones_Accuracy: 96.296 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [80] | Training_Loss: 0.225 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [81] | Training_Loss: 0.387 | Total_Accuracy: 99.766 | Ones_Accuracy: 93.478 | Zeros_Accuracy: 99.940 |
Epoch: [4] | Batch: [82] | Training_Loss: 0.277 | Total_Accuracy: 99.551 | Ones_Accuracy: 90.385 | Zeros_Accuracy: 99.770 |
Epoch: [4] | Batch: [83] | Training_Loss: 1.912 | Total_Accuracy: 94.192 | Ones_Accuracy: 36.842 | Zeros_Accuracy: 97.082 |
Epoch: [4] | Batch: [84] | Training_Loss: 1.644 | Total_Accuracy: 97.426 | Ones_Accuracy: 65.000 | Zeros_Accuracy: 98.712 |
Epoch: [4] | Batch: [85] | Training_Loss: 0.515 | Total_Accuracy: 99.535 | Ones_Accuracy: 90.196 | Zeros_Accuracy: 99.788 |
Epoch: [4] | Batch: [86] | Training_Loss: 0.181 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [87] | Training_Loss: 0.496 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [88] | Training_Loss: 0.357 | Total_Accuracy: 99.152 | Ones_Accuracy: 81.132 | Zeros_Accuracy: 99.613 |
Epoch: [4] | Batch: [89] | Training_Loss: 0.347 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [90] | Training_Loss: 0.558 | Total_Accuracy: 99.578 | Ones_Accuracy: 91.489 | Zeros_Accuracy: 99.784 |
Epoch: [4] | Batch: [91] | Training_Loss: 0.606 | Total_Accuracy: 99.388 | Ones_Accuracy: 89.655 | Zeros_Accuracy: 99.685 |
Epoch: [4] | Batch: [92] | Training_Loss: 1.486 | Total_Accuracy: 95.146 | Ones_Accuracy: 68.750 | Zeros_Accuracy: 97.368 |
Epoch: [4] | Batch: [93] | Training_Loss: 0.776 | Total_Accuracy: 98.202 | Ones_Accuracy: 77.419 | Zeros_Accuracy: 99.133 |
Epoch: [4] | Batch: [94] | Training_Loss: 1.476 | Total_Accuracy: 95.742 | Ones_Accuracy: 44.444 | Zeros_Accuracy: 97.841 |
Epoch: [4] | Batch: [95] | Training_Loss: 0.439 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [96] | Training_Loss: 0.190 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [97] | Training_Loss: 2.029 | Total_Accuracy: 95.341 | Ones_Accuracy: 65.000 | Zeros_Accuracy: 97.683 |
Epoch: [4] | Batch: [98] | Training_Loss: 0.766 | Total_Accuracy: 98.279 | Ones_Accuracy: 62.821 | Zeros_Accuracy: 99.134 |
Epoch: [4] | Batch: [99] | Training_Loss: 0.520 | Total_Accuracy: 99.937 | Ones_Accuracy: 98.113 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [100] | Training_Loss: 0.310 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [101] | Training_Loss: 0.401 | Total_Accuracy: 99.801 | Ones_Accuracy: 95.000 | Zeros_Accuracy: 99.932 |
Epoch: [4] | Batch: [102] | Training_Loss: 0.236 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [103] | Training_Loss: 1.782 | Total_Accuracy: 94.074 | Ones_Accuracy: 46.667 | Zeros_Accuracy: 96.863 |
Epoch: [4] | Batch: [104] | Training_Loss: 0.427 | Total_Accuracy: 99.901 | Ones_Accuracy: 98.000 | Zeros_Accuracy: 99.949 |
Epoch: [4] | Batch: [105] | Training_Loss: 1.540 | Total_Accuracy: 97.368 | Ones_Accuracy: 83.333 | Zeros_Accuracy: 98.571 |
Epoch: [4] | Batch: [106] | Training_Loss: 0.222 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [107] | Training_Loss: 2.514 | Total_Accuracy: 95.423 | Ones_Accuracy: 71.429 | Zeros_Accuracy: 97.338 |
Epoch: [4] | Batch: [108] | Training_Loss: 0.181 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [109] | Training_Loss: 0.294 | Total_Accuracy: 99.877 | Ones_Accuracy: 97.826 | Zeros_Accuracy: 99.936 |
Epoch: [4] | Batch: [110] | Training_Loss: 1.284 | Total_Accuracy: 99.048 | Ones_Accuracy: 92.857 | Zeros_Accuracy: 99.490 |
Epoch: [4] | Batch: [111] | Training_Loss: 0.930 | Total_Accuracy: 97.978 | Ones_Accuracy: 74.194 | Zeros_Accuracy: 99.015 |
Epoch: [4] | Batch: [112] | Training_Loss: 1.396 | Total_Accuracy: 97.482 | Ones_Accuracy: 71.429 | Zeros_Accuracy: 98.864 |
Epoch: [4] | Batch: [113] | Training_Loss: 0.786 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [114] | Training_Loss: 0.976 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [115] | Training_Loss: 1.297 | Total_Accuracy: 94.231 | Ones_Accuracy: 40.000 | Zeros_Accuracy: 97.551 |
Epoch: [4] | Batch: [116] | Training_Loss: 0.333 | Total_Accuracy: 99.538 | Ones_Accuracy: 89.796 | Zeros_Accuracy: 99.763 |
Epoch: [4] | Batch: [117] | Training_Loss: 0.564 | Total_Accuracy: 99.333 | Ones_Accuracy: 85.417 | Zeros_Accuracy: 99.714 |
Epoch: [4] | Batch: [118] | Training_Loss: 1.372 | Total_Accuracy: 92.818 | Ones_Accuracy: 34.783 | Zeros_Accuracy: 96.755 |
Epoch: [4] | Batch: [119] | Training_Loss: 1.022 | Total_Accuracy: 98.555 | Ones_Accuracy: 73.134 | Zeros_Accuracy: 99.257 |
Epoch: [4] | Batch: [120] | Training_Loss: 0.269 | Total_Accuracy: 99.896 | Ones_Accuracy: 97.619 | Zeros_Accuracy: 99.947 |
Epoch: [4] | Batch: [121] | Training_Loss: 1.102 | Total_Accuracy: 98.326 | Ones_Accuracy: 89.474 | Zeros_Accuracy: 99.091 |
Epoch: [4] | Batch: [122] | Training_Loss: 2.172 | Total_Accuracy: 93.894 | Ones_Accuracy: 40.625 | Zeros_Accuracy: 96.864 |
Epoch: [4] | Batch: [123] | Training_Loss: 0.219 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [124] | Training_Loss: 0.307 | Total_Accuracy: 99.865 | Ones_Accuracy: 97.260 | Zeros_Accuracy: 99.931 |
Epoch: [4] | Batch: [125] | Training_Loss: 0.414 | Total_Accuracy: 99.725 | Ones_Accuracy: 94.444 | Zeros_Accuracy: 99.859 |
Epoch: [4] | Batch: [126] | Training_Loss: 0.756 | Total_Accuracy: 96.226 | Ones_Accuracy: 72.727 | Zeros_Accuracy: 98.947 |
Epoch: [4] | Batch: [127] | Training_Loss: 0.421 | Total_Accuracy: 99.763 | Ones_Accuracy: 95.918 | Zeros_Accuracy: 99.878 |
Epoch: [4] | Batch: [128] | Training_Loss: 1.192 | Total_Accuracy: 98.253 | Ones_Accuracy: 89.474 | Zeros_Accuracy: 99.048 |
Epoch: [4] | Batch: [129] | Training_Loss: 1.226 | Total_Accuracy: 98.703 | Ones_Accuracy: 84.848 | Zeros_Accuracy: 99.322 |
Epoch: [4] | Batch: [130] | Training_Loss: 0.258 | Total_Accuracy: 99.508 | Ones_Accuracy: 88.679 | Zeros_Accuracy: 99.771 |
Epoch: [4] | Batch: [131] | Training_Loss: 1.589 | Total_Accuracy: 97.308 | Ones_Accuracy: 80.000 | Zeros_Accuracy: 98.750 |
Epoch: [4] | Batch: [132] | Training_Loss: 0.219 | Total_Accuracy: 99.774 | Ones_Accuracy: 94.505 | Zeros_Accuracy: 99.884 |
Epoch: [4] | Batch: [133] | Training_Loss: 1.905 | Total_Accuracy: 91.880 | Ones_Accuracy: 23.077 | Zeros_Accuracy: 95.928 |
Epoch: [4] | Batch: [134] | Training_Loss: 0.360 | Total_Accuracy: 99.723 | Ones_Accuracy: 95.833 | Zeros_Accuracy: 99.857 |
Epoch: [4] | Batch: [135] | Training_Loss: 1.285 | Total_Accuracy: 94.194 | Ones_Accuracy: 68.750 | Zeros_Accuracy: 97.122 |
Epoch: [4] | Batch: [136] | Training_Loss: 1.177 | Total_Accuracy: 97.346 | Ones_Accuracy: 68.750 | Zeros_Accuracy: 98.684 |
Epoch: [4] | Batch: [137] | Training_Loss: 0.391 | Total_Accuracy: 99.341 | Ones_Accuracy: 87.931 | Zeros_Accuracy: 99.661 |
Epoch: [4] | Batch: [138] | Training_Loss: 1.762 | Total_Accuracy: 97.049 | Ones_Accuracy: 78.261 | Zeros_Accuracy: 98.582 |
Epoch: [4] | Batch: [139] | Training_Loss: 1.485 | Total_Accuracy: 92.248 | Ones_Accuracy: 60.000 | Zeros_Accuracy: 96.491 |
Epoch: [4] | Batch: [140] | Training_Loss: 0.371 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [141] | Training_Loss: 0.299 | Total_Accuracy: 99.704 | Ones_Accuracy: 93.182 | Zeros_Accuracy: 99.849 |
Epoch: [4] | Batch: [142] | Training_Loss: 0.232 | Total_Accuracy: 99.943 | Ones_Accuracy: 98.765 | Zeros_Accuracy: 99.971 |
Epoch: [4] | Batch: [143] | Training_Loss: 0.505 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [144] | Training_Loss: 1.317 | Total_Accuracy: 97.208 | Ones_Accuracy: 66.667 | Zeros_Accuracy: 98.543 |
Epoch: [4] | Batch: [145] | Training_Loss: 1.849 | Total_Accuracy: 93.013 | Ones_Accuracy: 55.000 | Zeros_Accuracy: 96.651 |
Epoch: [4] | Batch: [146] | Training_Loss: 1.100 | Total_Accuracy: 97.338 | Ones_Accuracy: 73.077 | Zeros_Accuracy: 98.600 |
Epoch: [4] | Batch: [147] | Training_Loss: 1.853 | Total_Accuracy: 95.560 | Ones_Accuracy: 57.143 | Zeros_Accuracy: 97.755 |
Epoch: [4] | Batch: [148] | Training_Loss: 1.123 | Total_Accuracy: 95.062 | Ones_Accuracy: 77.778 | Zeros_Accuracy: 97.222 |
Epoch: [4] | Batch: [149] | Training_Loss: 0.685 | Total_Accuracy: 99.017 | Ones_Accuracy: 77.966 | Zeros_Accuracy: 99.587 |
Epoch: [4] | Batch: [150] | Training_Loss: 1.401 | Total_Accuracy: 96.364 | Ones_Accuracy: 52.174 | Zeros_Accuracy: 98.292 |
Epoch: [4] | Batch: [151] | Training_Loss: 1.032 | Total_Accuracy: 96.942 | Ones_Accuracy: 52.000 | Zeros_Accuracy: 98.728 |
Epoch: [4] | Batch: [152] | Training_Loss: 1.190 | Total_Accuracy: 98.493 | Ones_Accuracy: 71.667 | Zeros_Accuracy: 99.226 |
Epoch: [4] | Batch: [153] | Training_Loss: 1.183 | Total_Accuracy: 97.327 | Ones_Accuracy: 66.667 | Zeros_Accuracy: 98.608 |
Epoch: [4] | Batch: [154] | Training_Loss: 0.679 | Total_Accuracy: 99.241 | Ones_Accuracy: 86.364 | Zeros_Accuracy: 99.609 |
Epoch: [4] | Batch: [155] | Training_Loss: 0.833 | Total_Accuracy: 99.219 | Ones_Accuracy: 86.207 | Zeros_Accuracy: 99.836 |
Epoch: [4] | Batch: [156] | Training_Loss: 0.189 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [157] | Training_Loss: 0.266 | Total_Accuracy: 99.767 | Ones_Accuracy: 94.545 | Zeros_Accuracy: 99.904 |
Epoch: [4] | Batch: [158] | Training_Loss: 1.260 | Total_Accuracy: 98.054 | Ones_Accuracy: 66.667 | Zeros_Accuracy: 98.998 |
Epoch: [4] | Batch: [159] | Training_Loss: 1.146 | Total_Accuracy: 98.034 | Ones_Accuracy: 75.758 | Zeros_Accuracy: 98.976 |
Epoch: [4] | Batch: [160] | Training_Loss: 0.988 | Total_Accuracy: 97.902 | Ones_Accuracy: 85.714 | Zeros_Accuracy: 98.868 |
Epoch: [4] | Batch: [161] | Training_Loss: 1.587 | Total_Accuracy: 93.050 | Ones_Accuracy: 28.571 | Zeros_Accuracy: 96.735 |
Epoch: [4] | Batch: [162] | Training_Loss: 1.257 | Total_Accuracy: 95.813 | Ones_Accuracy: 62.500 | Zeros_Accuracy: 97.906 |
Epoch: [4] | Batch: [163] | Training_Loss: 0.835 | Total_Accuracy: 99.227 | Ones_Accuracy: 95.833 | Zeros_Accuracy: 99.451 |
Epoch: [4] | Batch: [164] | Training_Loss: 0.438 | Total_Accuracy: 99.477 | Ones_Accuracy: 90.476 | Zeros_Accuracy: 99.731 |
Epoch: [4] | Batch: [165] | Training_Loss: 0.227 | Total_Accuracy: 99.791 | Ones_Accuracy: 96.000 | Zeros_Accuracy: 99.892 |
Epoch: [4] | Batch: [166] | Training_Loss: 0.607 | Total_Accuracy: 99.020 | Ones_Accuracy: 94.118 | Zeros_Accuracy: 99.465 |
Epoch: [4] | Batch: [167] | Training_Loss: 0.231 | Total_Accuracy: 99.286 | Ones_Accuracy: 86.000 | Zeros_Accuracy: 99.634 |
Epoch: [4] | Batch: [168] | Training_Loss: 0.814 | Total_Accuracy: 97.264 | Ones_Accuracy: 71.429 | Zeros_Accuracy: 99.026 |
Epoch: [4] | Batch: [169] | Training_Loss: 0.324 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [170] | Training_Loss: 2.324 | Total_Accuracy: 93.617 | Ones_Accuracy: 55.000 | Zeros_Accuracy: 96.565 |
Epoch: [4] | Batch: [171] | Training_Loss: 0.207 | Total_Accuracy: 99.857 | Ones_Accuracy: 97.143 | Zeros_Accuracy: 99.927 |
Epoch: [4] | Batch: [172] | Training_Loss: 0.191 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [173] | Training_Loss: 2.921 | Total_Accuracy: 94.937 | Ones_Accuracy: 60.000 | Zeros_Accuracy: 97.297 |
Epoch: [4] | Batch: [174] | Training_Loss: 1.253 | Total_Accuracy: 96.324 | Ones_Accuracy: 68.750 | Zeros_Accuracy: 98.047 |
Epoch: [4] | Batch: [175] | Training_Loss: 0.250 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [176] | Training_Loss: 0.471 | Total_Accuracy: 98.816 | Ones_Accuracy: 76.667 | Zeros_Accuracy: 99.415 |
Epoch: [4] | Batch: [177] | Training_Loss: 1.723 | Total_Accuracy: 96.749 | Ones_Accuracy: 64.865 | Zeros_Accuracy: 98.361 |
Epoch: [4] | Batch: [178] | Training_Loss: 1.423 | Total_Accuracy: 96.703 | Ones_Accuracy: 70.588 | Zeros_Accuracy: 98.438 |
Epoch: [4] | Batch: [179] | Training_Loss: 0.315 | Total_Accuracy: 99.565 | Ones_Accuracy: 91.045 | Zeros_Accuracy: 99.777 |
Epoch: [4] | Batch: [180] | Training_Loss: 1.107 | Total_Accuracy: 94.515 | Ones_Accuracy: 56.250 | Zeros_Accuracy: 97.285 |
Epoch: [4] | Batch: [181] | Training_Loss: 1.575 | Total_Accuracy: 96.415 | Ones_Accuracy: 51.724 | Zeros_Accuracy: 98.138 |
Epoch: [4] | Batch: [182] | Training_Loss: 0.478 | Total_Accuracy: 99.875 | Ones_Accuracy: 97.959 | Zeros_Accuracy: 99.935 |
Epoch: [4] | Batch: [183] | Training_Loss: 1.032 | Total_Accuracy: 98.675 | Ones_Accuracy: 76.923 | Zeros_Accuracy: 99.318 |
Epoch: [4] | Batch: [184] | Training_Loss: 0.410 | Total_Accuracy: 99.808 | Ones_Accuracy: 95.349 | Zeros_Accuracy: 99.934 |
Epoch: [4] | Batch: [185] | Training_Loss: 2.614 | Total_Accuracy: 93.011 | Ones_Accuracy: 58.824 | Zeros_Accuracy: 96.450 |
Epoch: [4] | Batch: [186] | Training_Loss: 0.218 | Total_Accuracy: 99.506 | Ones_Accuracy: 90.000 | Zeros_Accuracy: 99.830 |
Epoch: [4] | Batch: [187] | Training_Loss: 1.657 | Total_Accuracy: 95.429 | Ones_Accuracy: 46.667 | Zeros_Accuracy: 97.612 |
Epoch: [4] | Batch: [188] | Training_Loss: 0.366 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [189] | Training_Loss: 0.300 | Total_Accuracy: 99.869 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 99.865 |
Epoch: [4] | Batch: [190] | Training_Loss: 0.253 | Total_Accuracy: 99.824 | Ones_Accuracy: 96.364 | Zeros_Accuracy: 99.910 |
Epoch: [4] | Batch: [191] | Training_Loss: 0.990 | Total_Accuracy: 99.111 | Ones_Accuracy: 87.500 | Zeros_Accuracy: 99.539 |
Epoch: [4] | Batch: [192] | Training_Loss: 1.705 | Total_Accuracy: 95.788 | Ones_Accuracy: 52.381 | Zeros_Accuracy: 97.851 |
Epoch: [4] | Batch: [193] | Training_Loss: 0.217 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [194] | Training_Loss: 1.636 | Total_Accuracy: 93.182 | Ones_Accuracy: 54.545 | Zeros_Accuracy: 96.694 |
Epoch: [4] | Batch: [195] | Training_Loss: 1.836 | Total_Accuracy: 95.489 | Ones_Accuracy: 42.857 | Zeros_Accuracy: 97.652 |
Epoch: [4] | Batch: [196] | Training_Loss: 2.012 | Total_Accuracy: 95.179 | Ones_Accuracy: 42.857 | Zeros_Accuracy: 97.545 |
Epoch: [4] | Batch: [197] | Training_Loss: 0.322 | Total_Accuracy: 99.705 | Ones_Accuracy: 93.333 | Zeros_Accuracy: 99.879 |
Epoch: [4] | Batch: [198] | Training_Loss: 0.996 | Total_Accuracy: 98.174 | Ones_Accuracy: 78.261 | Zeros_Accuracy: 99.149 |
Epoch: [4] | Batch: [199] | Training_Loss: 0.521 | Total_Accuracy: 99.529 | Ones_Accuracy: 91.837 | Zeros_Accuracy: 99.758 |
Epoch: [4] | Batch: [200] | Training_Loss: 0.384 | Total_Accuracy: 99.894 | Ones_Accuracy: 97.872 | Zeros_Accuracy: 99.946 |
Epoch: [4] | Batch: [201] | Training_Loss: 0.929 | Total_Accuracy: 98.316 | Ones_Accuracy: 72.222 | Zeros_Accuracy: 99.132 |
Epoch: [4] | Batch: [202] | Training_Loss: 0.184 | Total_Accuracy: 99.946 | Ones_Accuracy: 98.824 | Zeros_Accuracy: 99.973 |
Epoch: [4] | Batch: [203] | Training_Loss: 0.702 | Total_Accuracy: 98.517 | Ones_Accuracy: 72.414 | Zeros_Accuracy: 99.315 |
Epoch: [4] | Batch: [204] | Training_Loss: 1.926 | Total_Accuracy: 95.797 | Ones_Accuracy: 58.621 | Zeros_Accuracy: 97.786 |
Epoch: [4] | Batch: [205] | Training_Loss: 0.257 | Total_Accuracy: 99.625 | Ones_Accuracy: 93.182 | Zeros_Accuracy: 99.807 |
Epoch: [4] | Batch: [206] | Training_Loss: 0.263 | Total_Accuracy: 99.882 | Ones_Accuracy: 97.826 | Zeros_Accuracy: 99.939 |
Epoch: [4] | Batch: [207] | Training_Loss: 1.344 | Total_Accuracy: 97.811 | Ones_Accuracy: 71.739 | Zeros_Accuracy: 98.862 |
Epoch: [4] | Batch: [208] | Training_Loss: 1.026 | Total_Accuracy: 98.675 | Ones_Accuracy: 91.304 | Zeros_Accuracy: 99.283 |
Epoch: [4] | Batch: [209] | Training_Loss: 1.412 | Total_Accuracy: 97.382 | Ones_Accuracy: 64.516 | Zeros_Accuracy: 98.772 |
Epoch: [4] | Batch: [210] | Training_Loss: 0.997 | Total_Accuracy: 97.826 | Ones_Accuracy: 83.333 | Zeros_Accuracy: 99.206 |
Epoch: [4] | Batch: [211] | Training_Loss: 0.204 | Total_Accuracy: 99.937 | Ones_Accuracy: 97.826 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [212] | Training_Loss: 1.354 | Total_Accuracy: 97.581 | Ones_Accuracy: 83.333 | Zeros_Accuracy: 99.107 |
Epoch: [4] | Batch: [213] | Training_Loss: 0.504 | Total_Accuracy: 99.408 | Ones_Accuracy: 90.000 | Zeros_Accuracy: 99.695 |
Epoch: [4] | Batch: [214] | Training_Loss: 0.287 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [215] | Training_Loss: 0.671 | Total_Accuracy: 99.192 | Ones_Accuracy: 86.275 | Zeros_Accuracy: 99.584 |
Epoch: [4] | Batch: [216] | Training_Loss: 0.286 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [217] | Training_Loss: 0.226 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [218] | Training_Loss: 0.789 | Total_Accuracy: 98.768 | Ones_Accuracy: 77.586 | Zeros_Accuracy: 99.367 |
Epoch: [4] | Batch: [219] | Training_Loss: 0.811 | Total_Accuracy: 98.428 | Ones_Accuracy: 85.185 | Zeros_Accuracy: 99.170 |
Epoch: [4] | Batch: [220] | Training_Loss: 0.563 | Total_Accuracy: 99.789 | Ones_Accuracy: 97.619 | Zeros_Accuracy: 99.890 |
Epoch: [4] | Batch: [221] | Training_Loss: 0.210 | Total_Accuracy: 99.957 | Ones_Accuracy: 98.113 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [222] | Training_Loss: 0.242 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [223] | Training_Loss: 3.122 | Total_Accuracy: 93.617 | Ones_Accuracy: 57.143 | Zeros_Accuracy: 96.552 |
Epoch: [4] | Batch: [224] | Training_Loss: 0.294 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [225] | Training_Loss: 1.687 | Total_Accuracy: 96.591 | Ones_Accuracy: 67.857 | Zeros_Accuracy: 98.200 |
Epoch: [4] | Batch: [226] | Training_Loss: 0.254 | Total_Accuracy: 99.459 | Ones_Accuracy: 89.474 | Zeros_Accuracy: 99.761 |
Epoch: [4] | Batch: [227] | Training_Loss: 0.315 | Total_Accuracy: 99.740 | Ones_Accuracy: 94.828 | Zeros_Accuracy: 99.867 |
Epoch: [4] | Batch: [228] | Training_Loss: 0.644 | Total_Accuracy: 98.772 | Ones_Accuracy: 80.357 | Zeros_Accuracy: 99.366 |
Epoch: [4] | Batch: [229] | Training_Loss: 1.976 | Total_Accuracy: 94.902 | Ones_Accuracy: 27.273 | Zeros_Accuracy: 97.413 |
Epoch: [4] | Batch: [230] | Training_Loss: 0.391 | Total_Accuracy: 99.796 | Ones_Accuracy: 96.364 | Zeros_Accuracy: 99.895 |
Epoch: [4] | Batch: [231] | Training_Loss: 1.830 | Total_Accuracy: 95.480 | Ones_Accuracy: 45.161 | Zeros_Accuracy: 97.784 |
Epoch: [4] | Batch: [232] | Training_Loss: 1.056 | Total_Accuracy: 96.154 | Ones_Accuracy: 77.778 | Zeros_Accuracy: 97.895 |
Epoch: [4] | Batch: [233] | Training_Loss: 1.991 | Total_Accuracy: 95.446 | Ones_Accuracy: 23.529 | Zeros_Accuracy: 97.698 |
Epoch: [4] | Batch: [234] | Training_Loss: 0.203 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [235] | Training_Loss: 0.342 | Total_Accuracy: 99.567 | Ones_Accuracy: 86.667 | Zeros_Accuracy: 99.936 |
Epoch: [4] | Batch: [236] | Training_Loss: 0.206 | Total_Accuracy: 99.935 | Ones_Accuracy: 98.649 | Zeros_Accuracy: 99.967 |
Epoch: [4] | Batch: [237] | Training_Loss: 0.692 | Total_Accuracy: 99.604 | Ones_Accuracy: 94.737 | Zeros_Accuracy: 99.794 |
Epoch: [4] | Batch: [238] | Training_Loss: 1.940 | Total_Accuracy: 96.476 | Ones_Accuracy: 46.512 | Zeros_Accuracy: 98.217 |
Epoch: [4] | Batch: [239] | Training_Loss: 0.641 | Total_Accuracy: 99.762 | Ones_Accuracy: 96.154 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [240] | Training_Loss: 0.420 | Total_Accuracy: 99.379 | Ones_Accuracy: 86.000 | Zeros_Accuracy: 99.683 |
Epoch: [4] | Batch: [241] | Training_Loss: 0.997 | Total_Accuracy: 98.558 | Ones_Accuracy: 83.333 | Zeros_Accuracy: 99.246 |
Epoch: [4] | Batch: [242] | Training_Loss: 0.821 | Total_Accuracy: 99.091 | Ones_Accuracy: 92.308 | Zeros_Accuracy: 99.517 |
Epoch: [4] | Batch: [243] | Training_Loss: 1.055 | Total_Accuracy: 96.970 | Ones_Accuracy: 80.000 | Zeros_Accuracy: 98.361 |
Epoch: [4] | Batch: [244] | Training_Loss: 0.563 | Total_Accuracy: 99.714 | Ones_Accuracy: 94.737 | Zeros_Accuracy: 99.853 |
Epoch: [4] | Batch: [245] | Training_Loss: 0.557 | Total_Accuracy: 99.273 | Ones_Accuracy: 85.106 | Zeros_Accuracy: 99.688 |
Epoch: [4] | Batch: [246] | Training_Loss: 0.423 | Total_Accuracy: 98.896 | Ones_Accuracy: 77.966 | Zeros_Accuracy: 99.413 |
Epoch: [4] | Batch: [247] | Training_Loss: 0.172 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [248] | Training_Loss: 1.370 | Total_Accuracy: 96.964 | Ones_Accuracy: 58.140 | Zeros_Accuracy: 98.468 |
Epoch: [4] | Batch: [249] | Training_Loss: 1.496 | Total_Accuracy: 95.489 | Ones_Accuracy: 72.727 | Zeros_Accuracy: 97.541 |
Epoch: [4] | Batch: [250] | Training_Loss: 0.186 | Total_Accuracy: 99.650 | Ones_Accuracy: 92.308 | Zeros_Accuracy: 99.821 |
Epoch: [4] | Batch: [251] | Training_Loss: 0.366 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [252] | Training_Loss: 1.386 | Total_Accuracy: 97.418 | Ones_Accuracy: 65.789 | Zeros_Accuracy: 98.658 |
Epoch: [4] | Batch: [253] | Training_Loss: 1.735 | Total_Accuracy: 95.623 | Ones_Accuracy: 29.412 | Zeros_Accuracy: 97.837 |
Epoch: [4] | Batch: [254] | Training_Loss: 1.105 | Total_Accuracy: 98.551 | Ones_Accuracy: 72.308 | Zeros_Accuracy: 99.256 |
Epoch: [4] | Batch: [255] | Training_Loss: 0.349 | Total_Accuracy: 99.633 | Ones_Accuracy: 93.750 | Zeros_Accuracy: 99.785 |
Epoch: [4] | Batch: [256] | Training_Loss: 1.176 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [257] | Training_Loss: 1.083 | Total_Accuracy: 98.526 | Ones_Accuracy: 71.429 | Zeros_Accuracy: 99.265 |
Epoch: [4] | Batch: [258] | Training_Loss: 2.269 | Total_Accuracy: 94.499 | Ones_Accuracy: 37.500 | Zeros_Accuracy: 97.194 |
Epoch: [4] | Batch: [259] | Training_Loss: 1.754 | Total_Accuracy: 95.246 | Ones_Accuracy: 36.364 | Zeros_Accuracy: 97.531 |
Epoch: [4] | Batch: [260] | Training_Loss: 0.164 | Total_Accuracy: 99.709 | Ones_Accuracy: 92.453 | Zeros_Accuracy: 99.900 |
Epoch: [4] | Batch: [261] | Training_Loss: 1.085 | Total_Accuracy: 97.826 | Ones_Accuracy: 88.889 | Zeros_Accuracy: 98.795 |
Epoch: [4] | Batch: [262] | Training_Loss: 0.768 | Total_Accuracy: 99.229 | Ones_Accuracy: 86.441 | Zeros_Accuracy: 99.603 |
Epoch: [4] | Batch: [263] | Training_Loss: 0.230 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [264] | Training_Loss: 0.980 | Total_Accuracy: 98.387 | Ones_Accuracy: 91.667 | Zeros_Accuracy: 99.107 |
Epoch: [4] | Batch: [265] | Training_Loss: 1.092 | Total_Accuracy: 98.570 | Ones_Accuracy: 74.194 | Zeros_Accuracy: 99.288 |
Epoch: [4] | Batch: [266] | Training_Loss: 0.384 | Total_Accuracy: 99.525 | Ones_Accuracy: 88.000 | Zeros_Accuracy: 99.878 |
Epoch: [4] | Batch: [267] | Training_Loss: 0.264 | Total_Accuracy: 99.837 | Ones_Accuracy: 96.471 | Zeros_Accuracy: 99.916 |
Epoch: [4] | Batch: [268] | Training_Loss: 0.278 | Total_Accuracy: 99.854 | Ones_Accuracy: 96.629 | Zeros_Accuracy: 99.925 |
Epoch: [4] | Batch: [269] | Training_Loss: 2.303 | Total_Accuracy: 92.818 | Ones_Accuracy: 53.333 | Zeros_Accuracy: 96.386 |
Epoch: [4] | Batch: [270] | Training_Loss: 0.884 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [271] | Training_Loss: 0.186 | Total_Accuracy: 99.913 | Ones_Accuracy: 98.113 | Zeros_Accuracy: 99.956 |
Epoch: [4] | Batch: [272] | Training_Loss: 0.687 | Total_Accuracy: 98.800 | Ones_Accuracy: 79.245 | Zeros_Accuracy: 99.382 |
Epoch: [4] | Batch: [273] | Training_Loss: 1.965 | Total_Accuracy: 94.872 | Ones_Accuracy: 66.667 | Zeros_Accuracy: 97.222 |
Epoch: [4] | Batch: [274] | Training_Loss: 0.186 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [275] | Training_Loss: 0.202 | Total_Accuracy: 99.869 | Ones_Accuracy: 96.154 | Zeros_Accuracy: 99.955 |
Epoch: [4] | Batch: [276] | Training_Loss: 0.292 | Total_Accuracy: 99.803 | Ones_Accuracy: 95.455 | Zeros_Accuracy: 99.899 |
Epoch: [4] | Batch: [277] | Training_Loss: 0.632 | Total_Accuracy: 98.458 | Ones_Accuracy: 82.609 | Zeros_Accuracy: 99.454 |
Epoch: [4] | Batch: [278] | Training_Loss: 1.172 | Total_Accuracy: 95.455 | Ones_Accuracy: 66.667 | Zeros_Accuracy: 98.137 |
Epoch: [4] | Batch: [279] | Training_Loss: 1.208 | Total_Accuracy: 99.211 | Ones_Accuracy: 89.655 | Zeros_Accuracy: 99.669 |
Epoch: [4] | Batch: [280] | Training_Loss: 0.210 | Total_Accuracy: 99.675 | Ones_Accuracy: 93.617 | Zeros_Accuracy: 99.833 |
Epoch: [4] | Batch: [281] | Training_Loss: 0.841 | Total_Accuracy: 98.331 | Ones_Accuracy: 76.000 | Zeros_Accuracy: 99.303 |
Epoch: [4] | Batch: [282] | Training_Loss: 0.278 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [283] | Training_Loss: 0.762 | Total_Accuracy: 98.938 | Ones_Accuracy: 82.759 | Zeros_Accuracy: 99.452 |
Epoch: [4] | Batch: [284] | Training_Loss: 0.482 | Total_Accuracy: 98.919 | Ones_Accuracy: 76.190 | Zeros_Accuracy: 99.465 |
Epoch: [4] | Batch: [285] | Training_Loss: 1.409 | Total_Accuracy: 95.434 | Ones_Accuracy: 73.684 | Zeros_Accuracy: 97.500 |
Epoch: [4] | Batch: [286] | Training_Loss: 0.948 | Total_Accuracy: 99.172 | Ones_Accuracy: 90.909 | Zeros_Accuracy: 99.650 |
Epoch: [4] | Batch: [287] | Training_Loss: 0.578 | Total_Accuracy: 98.545 | Ones_Accuracy: 69.737 | Zeros_Accuracy: 99.271 |
Epoch: [4] | Batch: [288] | Training_Loss: 0.162 | Total_Accuracy: 99.840 | Ones_Accuracy: 95.918 | Zeros_Accuracy: 99.945 |
Epoch: [4] | Batch: [289] | Training_Loss: 0.230 | Total_Accuracy: 99.817 | Ones_Accuracy: 94.444 | Zeros_Accuracy: 99.930 |
Epoch: [4] | Batch: [290] | Training_Loss: 1.130 | Total_Accuracy: 97.176 | Ones_Accuracy: 67.857 | Zeros_Accuracy: 98.606 |
Epoch: [4] | Batch: [291] | Training_Loss: 0.451 | Total_Accuracy: 99.776 | Ones_Accuracy: 96.226 | Zeros_Accuracy: 99.884 |
Epoch: [4] | Batch: [292] | Training_Loss: 1.286 | Total_Accuracy: 97.696 | Ones_Accuracy: 79.412 | Zeros_Accuracy: 98.703 |
Epoch: [4] | Batch: [293] | Training_Loss: 0.269 | Total_Accuracy: 100.000 | Ones_Accuracy: 100.000 | Zeros_Accuracy: 100.000 |
Epoch: [4] | Batch: [294] | Training_Loss: 1.463 | Total_Accuracy: 96.375 | Ones_Accuracy: 48.276 | Zeros_Accuracy: 98.184 |
Epoch: [4] | Batch: [295] | Training_Loss: 2.163 | Total_Accuracy: 95.879 | Ones_Accuracy: 36.111 | Zeros_Accuracy: 97.917 |
Epoch: [4] | Batch: [296] | Training_Loss: 0.657 | Total_Accuracy: 99.456 | Ones_Accuracy: 90.164 | Zeros_Accuracy: 99.720 |
Epoch: [4] | Batch: [297] | Training_Loss: 0.797 | Total_Accuracy: 99.558 | Ones_Accuracy: 90.625 | Zeros_Accuracy: 99.794 |
Epoch: [4] | Batch: [298] | Training_Loss: 1.836 | Total_Accuracy: 95.849 | Ones_Accuracy: 37.838 | Zeros_Accuracy: 97.947 |
Epoch: [4] | Batch: [299] | Training_Loss: 0.353 | Total_Accuracy: 99.807 | Ones_Accuracy: 96.364 | Zeros_Accuracy: 99.901 |
Epoch: [4] | Batch: [300] | Training_Loss: 1.168 | Total_Accuracy: 98.848 | Ones_Accuracy: 86.111 | Zeros_Accuracy: 99.399 |
Epoch: [4] | Batch: [301] | Training_Loss: 1.085 | Total_Accuracy: 98.584 | Ones_Accuracy: 73.333 | Zeros_Accuracy: 99.273 |
Epoch: [4] | Batch: [302] | Training_Loss: 0.615 | Total_Accuracy: 98.414 | Ones_Accuracy: 73.585 | Zeros_Accuracy: 99.212 |
Epoch: [4] | Batch: [303] | Training_Loss: 0.316 | Total_Accuracy: 99.744 | Ones_Accuracy: 93.478 | Zeros_Accuracy: 99.934 |
Epoch: [4] | Batch: [304] | Training_Loss: 1.841 | Total_Accuracy: 95.907 | Ones_Accuracy: 32.258 | Zeros_Accuracy: 98.167 |
Epoch: [4] | Batch: [305] | Training_Loss: 0.399 | Total_Accuracy: 99.229 | Ones_Accuracy: 80.882 | Zeros_Accuracy: 99.677 |
Traceback (most recent call last):
  File "./tracking.py", line 90, in <module>
    model_training(data_list_train, data_list_valid, epochs, acc_epoch, acc_epoch2, save_model_epochs, validation_epochs, batchsize, "logfile", load_checkpoint)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/model_training.py", line 264, in model_training
    output, output2, ground_truth, ground_truth2, det_num, tracklet_num= complete_net(batch)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch_geometric/nn/data_parallel.py", line 60, in forward
    return self.module(data)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/network/complete_net.py", line 82, in forward
    output = self.optim_net(node_embedding, combined_edge_features, edge_index, coords, frame)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/network/optimizationGNN.py", line 66, in forward
    edge_attr = self.similarity1(out, edge_index, edge_attr_transformed)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/network/optimizationGNN.py", line 39, in similarity1
    node_feat_src = self.node_feature_mlp(node_embedding[edge_index[0][i]].unsqueeze(0))
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [2,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [3,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [4,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [5,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [6,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [7,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [8,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [9,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [10,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [11,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [12,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [13,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [14,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1704987290659/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [16,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && "index out of bounds"` failed.
/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataListLoader' is deprecated, use 'loader.DataListLoader' instead
  warnings.warn(out)
/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loaded training pickle files
Loaded validation pickle files
Train Samples List: 926
Valid Samples List: 143
DataParallel(
  (module): completeNet(
    (cnn): EncoderCNN(
      (cnn): Sequential(
        (0): Sequential(
          (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu0): ReLU(inplace=True)
          (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (denseblock1): _DenseBlock(
            (denselayer1): _DenseLayer(
              (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer2): _DenseLayer(
              (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer3): _DenseLayer(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer4): _DenseLayer(
              (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer5): _DenseLayer(
              (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer6): _DenseLayer(
              (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (transition1): _Transition(
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (denseblock2): _DenseBlock(
            (denselayer1): _DenseLayer(
              (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer2): _DenseLayer(
              (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer3): _DenseLayer(
              (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer4): _DenseLayer(
              (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer5): _DenseLayer(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer6): _DenseLayer(
              (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer7): _DenseLayer(
              (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer8): _DenseLayer(
              (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer9): _DenseLayer(
              (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer10): _DenseLayer(
              (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer11): _DenseLayer(
              (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer12): _DenseLayer(
              (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (transition2): _Transition(
            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (denseblock3): _DenseBlock(
            (denselayer1): _DenseLayer(
              (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer2): _DenseLayer(
              (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer3): _DenseLayer(
              (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer4): _DenseLayer(
              (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer5): _DenseLayer(
              (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer6): _DenseLayer(
              (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer7): _DenseLayer(
              (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer8): _DenseLayer(
              (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer9): _DenseLayer(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer10): _DenseLayer(
              (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer11): _DenseLayer(
              (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer12): _DenseLayer(
              (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer13): _DenseLayer(
              (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer14): _DenseLayer(
              (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer15): _DenseLayer(
              (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer16): _DenseLayer(
              (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer17): _DenseLayer(
              (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer18): _DenseLayer(
              (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer19): _DenseLayer(
              (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer20): _DenseLayer(
              (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer21): _DenseLayer(
              (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer22): _DenseLayer(
              (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer23): _DenseLayer(
              (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer24): _DenseLayer(
              (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (transition3): _Transition(
            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (denseblock4): _DenseBlock(
            (denselayer1): _DenseLayer(
              (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer2): _DenseLayer(
              (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer3): _DenseLayer(
              (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer4): _DenseLayer(
              (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer5): _DenseLayer(
              (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer6): _DenseLayer(
              (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer7): _DenseLayer(
              (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer8): _DenseLayer(
              (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer9): _DenseLayer(
              (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer10): _DenseLayer(
              (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer11): _DenseLayer(
              (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer12): _DenseLayer(
              (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer13): _DenseLayer(
              (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer14): _DenseLayer(
              (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer15): _DenseLayer(
              (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
            (denselayer16): _DenseLayer(
              (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu1): ReLU(inplace=True)
              (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu2): ReLU(inplace=True)
              (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            )
          )
          (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (affinity_net): affinityNet(
      (mlp): Sequential(
        (0): Linear(in_features=2, out_features=1, bias=True)
        (1): ReLU()
      )
    )
    (affinity_appearance_net): affinity_appearanceNet(
      (mlp): Sequential(
        (0): Linear(in_features=2048, out_features=1, bias=True)
        (1): ReLU()
      )
    )
    (affinity_geom_net): affinity_geomNet(
      (mlp): Sequential(
        (0): Linear(in_features=8, out_features=1, bias=True)
        (1): ReLU()
      )
    )
    (affinity_final_net): affinity_finalNet(
      (mlp): Sequential(
        (0): Linear(in_features=2, out_features=1, bias=True)
        (1): ReLU()
      )
    )
    (positional_affinity_net): PositionalAffinityNet(
      (mlp): Sequential(
        (0): Linear(in_features=4, out_features=1, bias=True)
        (1): ReLU()
      )
    )
    (optim_net): optimNet(
      (conv1): GCNConv(2049, 512)
      (conv2): GCNConv(1408, 128)
      (edge_feature_transform): Linear(in_features=2, out_features=1, bias=True)
      (node_feature_mlp): Sequential(
        (0): Linear(in_features=512, out_features=128, bias=True)
        (1): ReLU()
      )
      (edge_feature_mlp): Sequential(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): ReLU()
      )
    )
    (cos): CosineSimilarity()
  )
)
Epoch: [1] | Batch: [1] | Training_Loss: 2.800 |
Epoch: [1] | Batch: [2] | Training_Loss: 2.831 |
Epoch: [1] | Batch: [3] | Training_Loss: 4.919 |
Epoch: [1] | Batch: [4] | Training_Loss: 4.196 |
Epoch: [1] | Batch: [5] | Training_Loss: 5.724 |
Epoch: [1] | Batch: [6] | Training_Loss: 3.586 |
Epoch: [1] | Batch: [7] | Training_Loss: 7.448 |
Epoch: [1] | Batch: [8] | Training_Loss: 2.925 |
Epoch: [1] | Batch: [9] | Training_Loss: 3.067 |
Epoch: [1] | Batch: [10] | Training_Loss: 4.860 |
Epoch: [1] | Batch: [11] | Training_Loss: 2.246 |
Epoch: [1] | Batch: [12] | Training_Loss: 2.384 |
Epoch: [1] | Batch: [13] | Training_Loss: 2.308 |
Epoch: [1] | Batch: [14] | Training_Loss: 2.832 |
Epoch: [1] | Batch: [15] | Training_Loss: 2.665 |
Epoch: [1] | Batch: [16] | Training_Loss: 3.088 |
Epoch: [1] | Batch: [17] | Training_Loss: 4.126 |
Epoch: [1] | Batch: [18] | Training_Loss: 2.502 |
Epoch: [1] | Batch: [19] | Training_Loss: 2.773 |
Epoch: [1] | Batch: [20] | Training_Loss: 2.879 |
Epoch: [1] | Batch: [21] | Training_Loss: 3.333 |
Epoch: [1] | Batch: [22] | Training_Loss: 2.353 |
Epoch: [1] | Batch: [23] | Training_Loss: 4.911 |
Epoch: [1] | Batch: [24] | Training_Loss: 3.216 |
Epoch: [1] | Batch: [25] | Training_Loss: 4.554 |
Epoch: [1] | Batch: [26] | Training_Loss: 2.187 |
Epoch: [1] | Batch: [27] | Training_Loss: 3.372 |
Epoch: [1] | Batch: [28] | Training_Loss: 2.651 |
Epoch: [1] | Batch: [29] | Training_Loss: 3.819 |
Epoch: [1] | Batch: [30] | Training_Loss: 8.292 |
Epoch: [1] | Batch: [31] | Training_Loss: 3.114 |
Epoch: [1] | Batch: [32] | Training_Loss: 3.057 |
Epoch: [1] | Batch: [33] | Training_Loss: 3.272 |
Epoch: [1] | Batch: [34] | Training_Loss: 2.906 |
Epoch: [1] | Batch: [35] | Training_Loss: 2.414 |
Epoch: [1] | Batch: [36] | Training_Loss: 2.587 |
Epoch: [1] | Batch: [37] | Training_Loss: 4.421 |
Epoch: [1] | Batch: [38] | Training_Loss: 3.251 |
Epoch: [1] | Batch: [39] | Training_Loss: 2.524 |
Epoch: [1] | Batch: [40] | Training_Loss: 4.117 |
Epoch: [1] | Batch: [41] | Training_Loss: 5.206 |
Epoch: [1] | Batch: [42] | Training_Loss: 2.171 |
Epoch: [1] | Batch: [43] | Training_Loss: 2.088 |
Epoch: [1] | Batch: [44] | Training_Loss: 3.807 |
Epoch: [1] | Batch: [45] | Training_Loss: 3.389 |
Epoch: [1] | Batch: [46] | Training_Loss: 6.118 |
Epoch: [1] | Batch: [47] | Training_Loss: 2.891 |
Epoch: [1] | Batch: [48] | Training_Loss: 2.851 |
Epoch: [1] | Batch: [49] | Training_Loss: 4.212 |
Epoch: [1] | Batch: [50] | Training_Loss: 4.615 |
Epoch: [1] | Batch: [51] | Training_Loss: 2.763 |
Epoch: [1] | Batch: [52] | Training_Loss: 5.117 |
Epoch: [1] | Batch: [53] | Training_Loss: 4.960 |
Epoch: [1] | Batch: [54] | Training_Loss: 2.487 |
Epoch: [1] | Batch: [55] | Training_Loss: 3.608 |
Epoch: [1] | Batch: [56] | Training_Loss: 4.825 |
Epoch: [1] | Batch: [57] | Training_Loss: 2.921 |
Epoch: [1] | Batch: [58] | Training_Loss: 4.750 |
Epoch: [1] | Batch: [59] | Training_Loss: 3.127 |
Epoch: [1] | Batch: [60] | Training_Loss: 4.105 |
Epoch: [1] | Batch: [61] | Training_Loss: 3.386 |
Epoch: [1] | Batch: [62] | Training_Loss: 2.252 |
Epoch: [1] | Batch: [63] | Training_Loss: 2.550 |
Epoch: [1] | Batch: [64] | Training_Loss: 2.779 |
Epoch: [1] | Batch: [65] | Training_Loss: 4.236 |
Epoch: [1] | Batch: [66] | Training_Loss: 2.851 |
Epoch: [1] | Batch: [67] | Training_Loss: 2.151 |
Epoch: [1] | Batch: [68] | Training_Loss: 2.370 |
Epoch: [1] | Batch: [69] | Training_Loss: 4.409 |
Epoch: [1] | Batch: [70] | Training_Loss: 3.007 |
Epoch: [1] | Batch: [71] | Training_Loss: 4.407 |
Epoch: [1] | Batch: [72] | Training_Loss: 2.231 |
Epoch: [1] | Batch: [73] | Training_Loss: 2.133 |
Epoch: [1] | Batch: [74] | Training_Loss: 2.687 |
Epoch: [1] | Batch: [75] | Training_Loss: 2.211 |
Epoch: [1] | Batch: [76] | Training_Loss: 2.135 |
Epoch: [1] | Batch: [77] | Training_Loss: 4.282 |
Epoch: [1] | Batch: [78] | Training_Loss: 3.421 |
Epoch: [1] | Batch: [79] | Training_Loss: 3.455 |
Epoch: [1] | Batch: [80] | Training_Loss: 2.303 |
Epoch: [1] | Batch: [81] | Training_Loss: 2.706 |
Epoch: [1] | Batch: [82] | Training_Loss: 2.244 |
Epoch: [1] | Batch: [83] | Training_Loss: 3.462 |
Epoch: [1] | Batch: [84] | Training_Loss: 4.597 |
Epoch: [1] | Batch: [85] | Training_Loss: 2.096 |
Epoch: [1] | Batch: [86] | Training_Loss: 2.737 |
Epoch: [1] | Batch: [87] | Training_Loss: 7.726 |
Epoch: [1] | Batch: [88] | Training_Loss: 5.988 |
Epoch: [1] | Batch: [89] | Training_Loss: 4.082 |
Epoch: [1] | Batch: [90] | Training_Loss: 4.981 |
Epoch: [1] | Batch: [91] | Training_Loss: 3.578 |
Epoch: [1] | Batch: [92] | Training_Loss: 4.509 |
Epoch: [1] | Batch: [93] | Training_Loss: 2.035 |
Epoch: [1] | Batch: [94] | Training_Loss: 2.153 |
Epoch: [1] | Batch: [95] | Training_Loss: 4.002 |
Epoch: [1] | Batch: [96] | Training_Loss: 3.263 |
Epoch: [1] | Batch: [97] | Training_Loss: 2.863 |
Epoch: [1] | Batch: [98] | Training_Loss: 2.207 |
Epoch: [1] | Batch: [99] | Training_Loss: 2.258 |
Epoch: [1] | Batch: [100] | Training_Loss: 2.851 |
Epoch: [1] | Batch: [101] | Training_Loss: 3.822 |
Epoch: [1] | Batch: [102] | Training_Loss: 2.336 |
Epoch: [1] | Batch: [103] | Training_Loss: 2.433 |
Epoch: [1] | Batch: [104] | Training_Loss: 3.026 |
Epoch: [1] | Batch: [105] | Training_Loss: 2.367 |
Epoch: [1] | Batch: [106] | Training_Loss: 3.698 |
Epoch: [1] | Batch: [107] | Training_Loss: 3.290 |
Epoch: [1] | Batch: [108] | Training_Loss: 2.744 |
Epoch: [1] | Batch: [109] | Training_Loss: 3.115 |
Epoch: [1] | Batch: [110] | Training_Loss: 3.531 |
Epoch: [1] | Batch: [111] | Training_Loss: 4.597 |
Epoch: [1] | Batch: [112] | Training_Loss: 2.367 |
Epoch: [1] | Batch: [113] | Training_Loss: 4.146 |
Epoch: [1] | Batch: [114] | Training_Loss: 4.827 |
Epoch: [1] | Batch: [115] | Training_Loss: 3.245 |
Epoch: [1] | Batch: [116] | Training_Loss: 2.091 |
Epoch: [1] | Batch: [117] | Training_Loss: 4.541 |
Epoch: [1] | Batch: [118] | Training_Loss: 2.802 |
Epoch: [1] | Batch: [119] | Training_Loss: 2.338 |
Epoch: [1] | Batch: [120] | Training_Loss: 3.709 |
Epoch: [1] | Batch: [121] | Training_Loss: 3.641 |
Epoch: [1] | Batch: [122] | Training_Loss: 4.294 |
Epoch: [1] | Batch: [123] | Training_Loss: 4.312 |
Epoch: [1] | Batch: [124] | Training_Loss: 2.600 |
Epoch: [1] | Batch: [125] | Training_Loss: 1.907 |
Epoch: [1] | Batch: [126] | Training_Loss: 4.768 |
Epoch: [1] | Batch: [127] | Training_Loss: 2.235 |
Epoch: [1] | Batch: [128] | Training_Loss: 3.732 |
Epoch: [1] | Batch: [129] | Training_Loss: 3.593 |
Epoch: [1] | Batch: [130] | Training_Loss: 4.552 |
Epoch: [1] | Batch: [131] | Training_Loss: 2.612 |
Epoch: [1] | Batch: [132] | Training_Loss: 3.213 |
Epoch: [1] | Batch: [133] | Training_Loss: 4.094 |
Epoch: [1] | Batch: [134] | Training_Loss: 2.130 |
Epoch: [1] | Batch: [135] | Training_Loss: 2.902 |
Epoch: [1] | Batch: [136] | Training_Loss: 2.111 |
Epoch: [1] | Batch: [137] | Training_Loss: 2.108 |
Epoch: [1] | Batch: [138] | Training_Loss: 2.695 |
Epoch: [1] | Batch: [139] | Training_Loss: 3.326 |
Epoch: [1] | Batch: [140] | Training_Loss: 4.002 |
Epoch: [1] | Batch: [141] | Training_Loss: 2.066 |
Epoch: [1] | Batch: [142] | Training_Loss: 2.365 |
Epoch: [1] | Batch: [143] | Training_Loss: 3.763 |
Epoch: [1] | Batch: [144] | Training_Loss: 2.068 |
Epoch: [1] | Batch: [145] | Training_Loss: 2.288 |
Epoch: [1] | Batch: [146] | Training_Loss: 2.814 |
Epoch: [1] | Batch: [147] | Training_Loss: 3.077 |
Epoch: [1] | Batch: [148] | Training_Loss: 2.364 |
Epoch: [1] | Batch: [149] | Training_Loss: 3.527 |
Epoch: [1] | Batch: [150] | Training_Loss: 2.497 |
Epoch: [1] | Batch: [151] | Training_Loss: 4.260 |
Epoch: [1] | Batch: [152] | Training_Loss: 3.092 |
Epoch: [1] | Batch: [153] | Training_Loss: 2.765 |
Epoch: [1] | Batch: [154] | Training_Loss: 2.087 |
Epoch: [1] | Batch: [155] | Training_Loss: 3.782 |
Epoch: [1] | Batch: [156] | Training_Loss: 3.331 |
Epoch: [1] | Batch: [157] | Training_Loss: 3.985 |
Epoch: [1] | Batch: [158] | Training_Loss: 2.329 |
Epoch: [1] | Batch: [159] | Training_Loss: 4.604 |
Epoch: [1] | Batch: [160] | Training_Loss: 4.539 |
Epoch: [1] | Batch: [161] | Training_Loss: 2.830 |
Epoch: [1] | Batch: [162] | Training_Loss: 2.599 |
Epoch: [1] | Batch: [163] | Training_Loss: 3.545 |
Epoch: [1] | Batch: [164] | Training_Loss: 2.640 |
Epoch: [1] | Batch: [165] | Training_Loss: 3.583 |
Epoch: [1] | Batch: [166] | Training_Loss: 4.418 |
Epoch: [1] | Batch: [167] | Training_Loss: 3.155 |
Epoch: [1] | Batch: [168] | Training_Loss: 3.163 |
Epoch: [1] | Batch: [169] | Training_Loss: 1.909 |
Epoch: [1] | Batch: [170] | Training_Loss: 3.679 |
Epoch: [1] | Batch: [171] | Training_Loss: 3.078 |
Epoch: [1] | Batch: [172] | Training_Loss: 4.210 |
Epoch: [1] | Batch: [173] | Training_Loss: 4.273 |
Epoch: [1] | Batch: [174] | Training_Loss: 2.833 |
Epoch: [1] | Batch: [175] | Training_Loss: 1.991 |
Epoch: [1] | Batch: [176] | Training_Loss: 2.736 |
Epoch: [1] | Batch: [177] | Training_Loss: 4.442 |
Epoch: [1] | Batch: [178] | Training_Loss: 2.399 |
Epoch: [1] | Batch: [179] | Training_Loss: 4.303 |
Epoch: [1] | Batch: [180] | Training_Loss: 2.603 |
Epoch: [1] | Batch: [181] | Training_Loss: 2.761 |
Epoch: [1] | Batch: [182] | Training_Loss: 3.219 |
Epoch: [1] | Batch: [183] | Training_Loss: 3.681 |
Epoch: [1] | Batch: [184] | Training_Loss: 4.014 |
Epoch: [1] | Batch: [185] | Training_Loss: 2.214 |
Epoch: [1] | Batch: [186] | Training_Loss: 2.284 |
Epoch: [1] | Batch: [187] | Training_Loss: 2.047 |
Epoch: [1] | Batch: [188] | Training_Loss: 2.250 |
Epoch: [1] | Batch: [189] | Training_Loss: 4.082 |
Epoch: [1] | Batch: [190] | Training_Loss: 2.306 |
Epoch: [1] | Batch: [191] | Training_Loss: 4.306 |
Epoch: [1] | Batch: [192] | Training_Loss: 4.300 |
Epoch: [1] | Batch: [193] | Training_Loss: 3.640 |
Epoch: [1] | Batch: [194] | Training_Loss: 4.237 |
Epoch: [1] | Batch: [195] | Training_Loss: 3.590 |
Epoch: [1] | Batch: [196] | Training_Loss: 2.120 |
Epoch: [1] | Batch: [197] | Training_Loss: 4.552 |
Epoch: [1] | Batch: [198] | Training_Loss: 2.272 |
Epoch: [1] | Batch: [199] | Training_Loss: 4.180 |
Epoch: [1] | Batch: [200] | Training_Loss: 2.846 |
Epoch: [1] | Batch: [201] | Training_Loss: 3.647 |
Epoch: [1] | Batch: [202] | Training_Loss: 3.253 |
Epoch: [1] | Batch: [203] | Training_Loss: 2.103 |
Epoch: [1] | Batch: [204] | Training_Loss: 3.327 |
Epoch: [1] | Batch: [205] | Training_Loss: 2.809 |
Epoch: [1] | Batch: [206] | Training_Loss: 4.245 |
Epoch: [1] | Batch: [207] | Training_Loss: 1.951 |
Epoch: [1] | Batch: [208] | Training_Loss: 4.557 |
Epoch: [1] | Batch: [209] | Training_Loss: 2.269 |
Epoch: [1] | Batch: [210] | Training_Loss: 3.370 |
Epoch: [1] | Batch: [211] | Training_Loss: 2.810 |
Epoch: [1] | Batch: [212] | Training_Loss: 4.176 |
Epoch: [1] | Batch: [213] | Training_Loss: 3.452 |
Epoch: [1] | Batch: [214] | Training_Loss: 4.686 |
Epoch: [1] | Batch: [215] | Training_Loss: 5.218 |
Epoch: [1] | Batch: [216] | Training_Loss: 2.770 |
Epoch: [1] | Batch: [217] | Training_Loss: 2.513 |
Epoch: [1] | Batch: [218] | Training_Loss: 2.872 |
Epoch: [1] | Batch: [219] | Training_Loss: 2.533 |
Epoch: [1] | Batch: [220] | Training_Loss: 3.794 |
Epoch: [1] | Batch: [221] | Training_Loss: 3.892 |
Epoch: [1] | Batch: [222] | Training_Loss: 3.482 |
Epoch: [1] | Batch: [223] | Training_Loss: 4.182 |
Epoch: [1] | Batch: [224] | Training_Loss: 4.173 |
Epoch: [1] | Batch: [225] | Training_Loss: 4.082 |
Epoch: [1] | Batch: [226] | Training_Loss: 3.442 |
Epoch: [1] | Batch: [227] | Training_Loss: 4.406 |
Epoch: [1] | Batch: [228] | Training_Loss: 2.374 |
Epoch: [1] | Batch: [229] | Training_Loss: 4.006 |
Epoch: [1] | Batch: [230] | Training_Loss: 3.986 |
Epoch: [1] | Batch: [231] | Training_Loss: 2.521 |
Epoch: [1] | Batch: [232] | Training_Loss: 3.075 |
Epoch: [1] | Batch: [233] | Training_Loss: 3.292 |
Epoch: [1] | Batch: [234] | Training_Loss: 4.043 |
Epoch: [1] | Batch: [235] | Training_Loss: 2.693 |
Epoch: [1] | Batch: [236] | Training_Loss: 1.935 |
Epoch: [1] | Batch: [237] | Training_Loss: 3.405 |
Epoch: [1] | Batch: [238] | Training_Loss: 2.843 |
Epoch: [1] | Batch: [239] | Training_Loss: 3.627 |
Epoch: [1] | Batch: [240] | Training_Loss: 2.658 |
Epoch: [1] | Batch: [241] | Training_Loss: 3.233 |
Epoch: [1] | Batch: [242] | Training_Loss: 2.324 |
Epoch: [1] | Batch: [243] | Training_Loss: 3.516 |
Epoch: [1] | Batch: [244] | Training_Loss: 2.258 |
Epoch: [1] | Batch: [245] | Training_Loss: 1.983 |
Epoch: [1] | Batch: [246] | Training_Loss: 2.816 |
Epoch: [1] | Batch: [247] | Training_Loss: 3.627 |
Epoch: [1] | Batch: [248] | Training_Loss: 2.567 |
Epoch: [1] | Batch: [249] | Training_Loss: 2.264 |
Epoch: [1] | Batch: [250] | Training_Loss: 2.452 |
Epoch: [1] | Batch: [251] | Training_Loss: 3.708 |
Epoch: [1] | Batch: [252] | Training_Loss: 2.185 |
Epoch: [1] | Batch: [253] | Training_Loss: 2.223 |
Epoch: [1] | Batch: [254] | Training_Loss: 3.596 |
Epoch: [1] | Batch: [255] | Training_Loss: 2.416 |
Epoch: [1] | Batch: [256] | Training_Loss: 2.051 |
Epoch: [1] | Batch: [257] | Training_Loss: 3.231 |
Epoch: [1] | Batch: [258] | Training_Loss: 2.037 |
Epoch: [1] | Batch: [259] | Training_Loss: 2.457 |
Epoch: [1] | Batch: [260] | Training_Loss: 2.085 |
Epoch: [1] | Batch: [261] | Training_Loss: 2.714 |
Epoch: [1] | Batch: [262] | Training_Loss: 2.900 |
Epoch: [1] | Batch: [263] | Training_Loss: 3.152 |
Epoch: [1] | Batch: [264] | Training_Loss: 4.402 |
Epoch: [1] | Batch: [265] | Training_Loss: 5.505 |
Epoch: [1] | Batch: [266] | Training_Loss: 2.210 |
Epoch: [1] | Batch: [267] | Training_Loss: 4.097 |
Epoch: [1] | Batch: [268] | Training_Loss: 1.979 |
Epoch: [1] | Batch: [269] | Training_Loss: 2.616 |
Epoch: [1] | Batch: [270] | Training_Loss: 2.516 |
Epoch: [1] | Batch: [271] | Training_Loss: 3.891 |
Epoch: [1] | Batch: [272] | Training_Loss: 3.256 |
Epoch: [1] | Batch: [273] | Training_Loss: 2.796 |
Epoch: [1] | Batch: [274] | Training_Loss: 2.880 |
Epoch: [1] | Batch: [275] | Training_Loss: 3.940 |
Epoch: [1] | Batch: [276] | Training_Loss: 3.208 |
Epoch: [1] | Batch: [277] | Training_Loss: 2.716 |
Epoch: [1] | Batch: [278] | Training_Loss: 2.119 |
Epoch: [1] | Batch: [279] | Training_Loss: 5.293 |
Epoch: [1] | Batch: [280] | Training_Loss: 1.758 |
Epoch: [1] | Batch: [281] | Training_Loss: 3.605 |
Epoch: [1] | Batch: [282] | Training_Loss: 3.391 |
Epoch: [1] | Batch: [283] | Training_Loss: 4.016 |
Epoch: [1] | Batch: [284] | Training_Loss: 3.876 |
Epoch: [1] | Batch: [285] | Training_Loss: 3.769 |
Epoch: [1] | Batch: [286] | Training_Loss: 4.725 |
Epoch: [1] | Batch: [287] | Training_Loss: 2.005 |
Epoch: [1] | Batch: [288] | Training_Loss: 3.035 |
Epoch: [1] | Batch: [289] | Training_Loss: 4.317 |
Epoch: [1] | Batch: [290] | Training_Loss: 2.806 |
Epoch: [1] | Batch: [291] | Training_Loss: 3.773 |
Epoch: [1] | Batch: [292] | Training_Loss: 2.751 |
Epoch: [1] | Batch: [293] | Training_Loss: 4.864 |
Epoch: [1] | Batch: [294] | Training_Loss: 5.640 |
Epoch: [1] | Batch: [295] | Training_Loss: 3.063 |
Epoch: [1] | Batch: [296] | Training_Loss: 2.871 |
Epoch: [1] | Batch: [297] | Training_Loss: 2.422 |
Epoch: [1] | Batch: [298] | Training_Loss: 1.995 |
Epoch: [1] | Batch: [299] | Training_Loss: 2.914 |
Epoch: [1] | Batch: [300] | Training_Loss: 4.147 |
Epoch: [1] | Batch: [301] | Training_Loss: 2.956 |
Epoch: [1] | Batch: [302] | Training_Loss: 2.321 |
Epoch: [1] | Batch: [303] | Training_Loss: 3.071 |
Epoch: [1] | Batch: [304] | Training_Loss: 2.940 |
Epoch: [1] | Batch: [305] | Training_Loss: 3.696 |
Epoch: [1] | Batch: [306] | Training_Loss: 1.693 |
Epoch: [1] | Batch: [307] | Training_Loss: 1.997 |
Epoch: [1] | Batch: [308] | Training_Loss: 3.123 |
Epoch: [1] | Batch: [309] | Training_Loss: 3.500 |
Epoch: [1] | Batch: [310] | Training_Loss: 4.335 |
Epoch: [1] | Batch: [311] | Training_Loss: 4.373 |
Epoch: [1] | Batch: [312] | Training_Loss: 2.504 |
Epoch: [1] | Batch: [313] | Training_Loss: 4.020 |
Epoch: [1] | Batch: [314] | Training_Loss: 2.844 |
Epoch: [1] | Batch: [315] | Training_Loss: 2.017 |
Epoch: [1] | Batch: [316] | Training_Loss: 2.050 |
Epoch: [1] | Batch: [317] | Training_Loss: 2.614 |
Epoch: [1] | Batch: [318] | Training_Loss: 3.439 |
Epoch: [1] | Batch: [319] | Training_Loss: 2.023 |
Epoch: [1] | Batch: [320] | Training_Loss: 2.105 |
Epoch: [1] | Batch: [321] | Training_Loss: 2.362 |
Epoch: [1] | Batch: [322] | Training_Loss: 2.848 |
Epoch: [1] | Batch: [323] | Training_Loss: 3.021 |
Epoch: [1] | Batch: [324] | Training_Loss: 2.897 |
Epoch: [1] | Batch: [325] | Training_Loss: 1.826 |
Epoch: [1] | Batch: [326] | Training_Loss: 3.103 |
Epoch: [1] | Batch: [327] | Training_Loss: 5.872 |
Epoch: [1] | Batch: [328] | Training_Loss: 2.069 |
Epoch: [1] | Batch: [329] | Training_Loss: 3.064 |
Epoch: [1] | Batch: [330] | Training_Loss: 3.345 |
Epoch: [1] | Batch: [331] | Training_Loss: 1.908 |
Epoch: [1] | Batch: [332] | Training_Loss: 3.860 |
Epoch: [1] | Batch: [333] | Training_Loss: 2.327 |
Epoch: [1] | Batch: [334] | Training_Loss: 2.170 |
Epoch: [1] | Batch: [335] | Training_Loss: 2.128 |
Epoch: [1] | Batch: [336] | Training_Loss: 3.184 |
Epoch: [1] | Batch: [337] | Training_Loss: 3.067 |
Epoch: [1] | Batch: [338] | Training_Loss: 3.064 |
Epoch: [1] | Batch: [339] | Training_Loss: 3.451 |
Epoch: [1] | Batch: [340] | Training_Loss: 3.051 |
Epoch: [1] | Batch: [341] | Training_Loss: 5.853 |
Epoch: [1] | Batch: [342] | Training_Loss: 3.796 |
Epoch: [1] | Batch: [343] | Training_Loss: 3.678 |
Epoch: [1] | Batch: [344] | Training_Loss: 2.070 |
Epoch: [1] | Batch: [345] | Training_Loss: 3.007 |
Epoch: [1] | Batch: [346] | Training_Loss: 3.002 |
Epoch: [1] | Batch: [347] | Training_Loss: 2.652 |
Epoch: [1] | Batch: [348] | Training_Loss: 2.198 |
Epoch: [1] | Batch: [349] | Training_Loss: 3.191 |
Epoch: [1] | Batch: [350] | Training_Loss: 3.649 |
Epoch: [1] | Batch: [351] | Training_Loss: 2.842 |
Epoch: [1] | Batch: [352] | Training_Loss: 2.858 |
Epoch: [1] | Batch: [353] | Training_Loss: 3.626 |
Epoch: [1] | Batch: [354] | Training_Loss: 3.013 |
Epoch: [1] | Batch: [355] | Training_Loss: 1.849 |
Epoch: [1] | Batch: [356] | Training_Loss: 2.240 |
Epoch: [1] | Batch: [357] | Training_Loss: 2.972 |
Epoch: [1] | Batch: [358] | Training_Loss: 3.070 |
Epoch: [1] | Batch: [359] | Training_Loss: 1.759 |
Epoch: [1] | Batch: [360] | Training_Loss: 3.404 |
Epoch: [1] | Batch: [361] | Training_Loss: 3.301 |
Epoch: [1] | Batch: [362] | Training_Loss: 2.843 |
Epoch: [1] | Batch: [363] | Training_Loss: 2.083 |
Epoch: [1] | Batch: [364] | Training_Loss: 4.110 |
Epoch: [1] | Batch: [365] | Training_Loss: 1.751 |
Epoch: [1] | Batch: [366] | Training_Loss: 2.618 |
Epoch: [1] | Batch: [367] | Training_Loss: 3.245 |
Epoch: [1] | Batch: [368] | Training_Loss: 1.773 |
Epoch: [1] | Batch: [369] | Training_Loss: 2.734 |
Epoch: [1] | Batch: [370] | Training_Loss: 3.095 |
Epoch: [1] | Batch: [371] | Training_Loss: 3.014 |
Epoch: [1] | Batch: [372] | Training_Loss: 4.109 |
Epoch: [1] | Batch: [373] | Training_Loss: 4.883 |
Epoch: [1] | Batch: [374] | Training_Loss: 2.752 |
Epoch: [1] | Batch: [375] | Training_Loss: 3.166 |
Epoch: [1] | Batch: [376] | Training_Loss: 3.060 |
Epoch: [1] | Batch: [377] | Training_Loss: 4.135 |
Epoch: [1] | Batch: [378] | Training_Loss: 2.788 |
Epoch: [1] | Batch: [379] | Training_Loss: 1.779 |
Epoch: [1] | Batch: [380] | Training_Loss: 1.882 |
Epoch: [1] | Batch: [381] | Training_Loss: 1.986 |
Epoch: [1] | Batch: [382] | Training_Loss: 1.891 |
Epoch: [1] | Batch: [383] | Training_Loss: 3.785 |
Epoch: [1] | Batch: [384] | Training_Loss: 3.296 |
Epoch: [1] | Batch: [385] | Training_Loss: 4.108 |
Epoch: [1] | Batch: [386] | Training_Loss: 2.513 |
Epoch: [1] | Batch: [387] | Training_Loss: 1.895 |
Epoch: [1] | Batch: [388] | Training_Loss: 2.945 |
Epoch: [1] | Batch: [389] | Training_Loss: 2.994 |
Epoch: [1] | Batch: [390] | Training_Loss: 2.839 |
Epoch: [1] | Batch: [391] | Training_Loss: 3.074 |
Epoch: [1] | Batch: [392] | Training_Loss: 2.416 |
Epoch: [1] | Batch: [393] | Training_Loss: 2.751 |
Epoch: [1] | Batch: [394] | Training_Loss: 3.433 |
Epoch: [1] | Batch: [395] | Training_Loss: 2.977 |
Epoch: [1] | Batch: [396] | Training_Loss: 2.617 |
Epoch: [1] | Batch: [397] | Training_Loss: 3.198 |
Epoch: [1] | Batch: [398] | Training_Loss: 2.773 |
Epoch: [1] | Batch: [399] | Training_Loss: 1.909 |
Epoch: [1] | Batch: [400] | Training_Loss: 2.804 |
Epoch: [1] | Batch: [401] | Training_Loss: 4.493 |
Epoch: [1] | Batch: [402] | Training_Loss: 2.063 |
Epoch: [1] | Batch: [403] | Training_Loss: 2.034 |
Epoch: [1] | Batch: [404] | Training_Loss: 3.525 |
Epoch: [1] | Batch: [405] | Training_Loss: 2.663 |
Epoch: [1] | Batch: [406] | Training_Loss: 1.979 |
Epoch: [1] | Batch: [407] | Training_Loss: 2.850 |
Epoch: [1] | Batch: [408] | Training_Loss: 2.891 |
Epoch: [1] | Batch: [409] | Training_Loss: 2.931 |
Epoch: [1] | Batch: [410] | Training_Loss: 2.675 |
Epoch: [1] | Batch: [411] | Training_Loss: 2.563 |
Epoch: [1] | Batch: [412] | Training_Loss: 2.642 |
Epoch: [1] | Batch: [413] | Training_Loss: 2.753 |
Epoch: [1] | Batch: [414] | Training_Loss: 1.738 |
Epoch: [1] | Batch: [415] | Training_Loss: 3.251 |
Epoch: [1] | Batch: [416] | Training_Loss: 1.580 |
Epoch: [1] | Batch: [417] | Training_Loss: 3.042 |
Epoch: [1] | Batch: [418] | Training_Loss: 3.824 |
Epoch: [1] | Batch: [419] | Training_Loss: 2.031 |
Epoch: [1] | Batch: [420] | Training_Loss: 3.063 |
Epoch: [1] | Batch: [421] | Training_Loss: 2.833 |
Epoch: [1] | Batch: [422] | Training_Loss: 3.001 |
Epoch: [1] | Batch: [423] | Training_Loss: 2.689 |
Epoch: [1] | Batch: [424] | Training_Loss: 3.167 |
Epoch: [1] | Batch: [425] | Training_Loss: 1.734 |
Epoch: [1] | Batch: [426] | Training_Loss: 1.914 |
Epoch: [1] | Batch: [427] | Training_Loss: 2.406 |
Epoch: [1] | Batch: [428] | Training_Loss: 2.571 |
Epoch: [1] | Batch: [429] | Training_Loss: 2.729 |
Epoch: [1] | Batch: [430] | Training_Loss: 3.860 |
Epoch: [1] | Batch: [431] | Training_Loss: 1.710 |
Epoch: [1] | Batch: [432] | Training_Loss: 3.223 |
Epoch: [1] | Batch: [433] | Training_Loss: 2.672 |
Epoch: [1] | Batch: [434] | Training_Loss: 2.800 |
Epoch: [1] | Batch: [435] | Training_Loss: 2.758 |
Epoch: [1] | Batch: [436] | Training_Loss: 3.069 |
Epoch: [1] | Batch: [437] | Training_Loss: 2.904 |
Epoch: [1] | Batch: [438] | Training_Loss: 2.576 |
Epoch: [1] | Batch: [439] | Training_Loss: 2.995 |
Epoch: [1] | Batch: [440] | Training_Loss: 2.540 |
Epoch: [1] | Batch: [441] | Training_Loss: 3.101 |
Epoch: [1] | Batch: [442] | Training_Loss: 2.408 |
Epoch: [1] | Batch: [443] | Training_Loss: 1.870 |
Epoch: [1] | Batch: [444] | Training_Loss: 2.761 |
Epoch: [1] | Batch: [445] | Training_Loss: 2.747 |
Epoch: [1] | Batch: [446] | Training_Loss: 3.772 |
Epoch: [1] | Batch: [447] | Training_Loss: 2.963 |
Epoch: [1] | Batch: [448] | Training_Loss: 4.680 |
Epoch: [1] | Batch: [449] | Training_Loss: 2.456 |
Epoch: [1] | Batch: [450] | Training_Loss: 2.619 |
Epoch: [1] | Batch: [451] | Training_Loss: 3.389 |
Epoch: [1] | Batch: [452] | Training_Loss: 3.136 |
Epoch: [1] | Batch: [453] | Training_Loss: 1.779 |
Epoch: [1] | Batch: [454] | Training_Loss: 2.392 |
Epoch: [1] | Batch: [455] | Training_Loss: 2.571 |
Epoch: [1] | Batch: [456] | Training_Loss: 1.622 |
Epoch: [1] | Batch: [457] | Training_Loss: 1.762 |
Epoch: [1] | Batch: [458] | Training_Loss: 2.606 |
Epoch: [1] | Batch: [459] | Training_Loss: 2.609 |
Epoch: [1] | Batch: [460] | Training_Loss: 2.862 |
Epoch: [1] | Batch: [461] | Training_Loss: 1.730 |
Epoch: [1] | Batch: [462] | Training_Loss: 2.555 |
Epoch: [1] | Batch: [463] | Training_Loss: 2.577 |
Epoch: [1] | Training_Loss: 3.132 |
Epoch: [2] | Batch: [1] | Training_Loss: 3.338 |
Epoch: [2] | Batch: [2] | Training_Loss: 2.794 |
Epoch: [2] | Batch: [3] | Training_Loss: 1.824 |
Epoch: [2] | Batch: [4] | Training_Loss: 2.308 |
Epoch: [2] | Batch: [5] | Training_Loss: 1.692 |
Epoch: [2] | Batch: [6] | Training_Loss: 1.709 |
Epoch: [2] | Batch: [7] | Training_Loss: 3.312 |
Epoch: [2] | Batch: [8] | Training_Loss: 2.498 |
Epoch: [2] | Batch: [9] | Training_Loss: 3.048 |
Epoch: [2] | Batch: [10] | Training_Loss: 2.526 |
Epoch: [2] | Batch: [11] | Training_Loss: 2.272 |
Epoch: [2] | Batch: [12] | Training_Loss: 2.858 |
Epoch: [2] | Batch: [13] | Training_Loss: 2.837 |
Epoch: [2] | Batch: [14] | Training_Loss: 2.762 |
Epoch: [2] | Batch: [15] | Training_Loss: 1.779 |
Epoch: [2] | Batch: [16] | Training_Loss: 3.141 |
Epoch: [2] | Batch: [17] | Training_Loss: 2.280 |
Epoch: [2] | Batch: [18] | Training_Loss: 1.987 |
Epoch: [2] | Batch: [19] | Training_Loss: 3.163 |
Epoch: [2] | Batch: [20] | Training_Loss: 2.133 |
Epoch: [2] | Batch: [21] | Training_Loss: 2.359 |
Epoch: [2] | Batch: [22] | Training_Loss: 3.624 |
Epoch: [2] | Batch: [23] | Training_Loss: 3.187 |
Epoch: [2] | Batch: [24] | Training_Loss: 2.732 |
Epoch: [2] | Batch: [25] | Training_Loss: 2.492 |
Epoch: [2] | Batch: [26] | Training_Loss: 2.260 |
Epoch: [2] | Batch: [27] | Training_Loss: 2.765 |
Epoch: [2] | Batch: [28] | Training_Loss: 2.692 |
Epoch: [2] | Batch: [29] | Training_Loss: 3.105 |
Epoch: [2] | Batch: [30] | Training_Loss: 2.855 |
Epoch: [2] | Batch: [31] | Training_Loss: 2.542 |
Epoch: [2] | Batch: [32] | Training_Loss: 2.747 |
Epoch: [2] | Batch: [33] | Training_Loss: 2.197 |
Epoch: [2] | Batch: [34] | Training_Loss: 1.845 |
Epoch: [2] | Batch: [35] | Training_Loss: 2.770 |
Epoch: [2] | Batch: [36] | Training_Loss: 2.824 |
Epoch: [2] | Batch: [37] | Training_Loss: 1.565 |
Epoch: [2] | Batch: [38] | Training_Loss: 2.925 |
Epoch: [2] | Batch: [39] | Training_Loss: 2.688 |
Epoch: [2] | Batch: [40] | Training_Loss: 3.014 |
Epoch: [2] | Batch: [41] | Training_Loss: 2.214 |
Epoch: [2] | Batch: [42] | Training_Loss: 2.958 |
Epoch: [2] | Batch: [43] | Training_Loss: 2.554 |
Epoch: [2] | Batch: [44] | Training_Loss: 2.727 |
Epoch: [2] | Batch: [45] | Training_Loss: 2.257 |
Epoch: [2] | Batch: [46] | Training_Loss: 1.649 |
Epoch: [2] | Batch: [47] | Training_Loss: 1.529 |
Epoch: [2] | Batch: [48] | Training_Loss: 2.625 |
Epoch: [2] | Batch: [49] | Training_Loss: 2.916 |
Epoch: [2] | Batch: [50] | Training_Loss: 2.934 |
Epoch: [2] | Batch: [51] | Training_Loss: 2.190 |
Epoch: [2] | Batch: [52] | Training_Loss: 2.227 |
Epoch: [2] | Batch: [53] | Training_Loss: 1.946 |
Epoch: [2] | Batch: [54] | Training_Loss: 3.239 |
Epoch: [2] | Batch: [55] | Training_Loss: 1.638 |
Epoch: [2] | Batch: [56] | Training_Loss: 3.209 |
Epoch: [2] | Batch: [57] | Training_Loss: 2.373 |
Epoch: [2] | Batch: [58] | Training_Loss: 2.504 |
Epoch: [2] | Batch: [59] | Training_Loss: 3.218 |
Epoch: [2] | Batch: [60] | Training_Loss: 2.680 |
Epoch: [2] | Batch: [61] | Training_Loss: 2.534 |
Epoch: [2] | Batch: [62] | Training_Loss: 2.035 |
Epoch: [2] | Batch: [63] | Training_Loss: 1.978 |
Epoch: [2] | Batch: [64] | Training_Loss: 3.354 |
Epoch: [2] | Batch: [65] | Training_Loss: 2.912 |
Epoch: [2] | Batch: [66] | Training_Loss: 3.725 |
Epoch: [2] | Batch: [67] | Training_Loss: 2.268 |
Epoch: [2] | Batch: [68] | Training_Loss: 2.986 |
Epoch: [2] | Batch: [69] | Training_Loss: 2.818 |
Epoch: [2] | Batch: [70] | Training_Loss: 3.271 |
Epoch: [2] | Batch: [71] | Training_Loss: 2.912 |
Epoch: [2] | Batch: [72] | Training_Loss: 2.429 |
Epoch: [2] | Batch: [73] | Training_Loss: 1.817 |
Epoch: [2] | Batch: [74] | Training_Loss: 1.719 |
Epoch: [2] | Batch: [75] | Training_Loss: 2.381 |
Epoch: [2] | Batch: [76] | Training_Loss: 2.077 |
Epoch: [2] | Batch: [77] | Training_Loss: 2.625 |
Epoch: [2] | Batch: [78] | Training_Loss: 2.806 |
Epoch: [2] | Batch: [79] | Training_Loss: 2.479 |
Epoch: [2] | Batch: [80] | Training_Loss: 1.689 |
Epoch: [2] | Batch: [81] | Training_Loss: 2.467 |
Epoch: [2] | Batch: [82] | Training_Loss: 2.977 |
Epoch: [2] | Batch: [83] | Training_Loss: 1.536 |
Epoch: [2] | Batch: [84] | Training_Loss: 2.615 |
Epoch: [2] | Batch: [85] | Training_Loss: 2.858 |
Epoch: [2] | Batch: [86] | Training_Loss: 3.016 |
Epoch: [2] | Batch: [87] | Training_Loss: 1.929 |
Epoch: [2] | Batch: [88] | Training_Loss: 2.577 |
Epoch: [2] | Batch: [89] | Training_Loss: 1.527 |
Epoch: [2] | Batch: [90] | Training_Loss: 2.403 |
Epoch: [2] | Batch: [91] | Training_Loss: 2.724 |
Epoch: [2] | Batch: [92] | Training_Loss: 2.990 |
Epoch: [2] | Batch: [93] | Training_Loss: 2.225 |
Epoch: [2] | Batch: [94] | Training_Loss: 2.255 |
Epoch: [2] | Batch: [95] | Training_Loss: 1.439 |
Epoch: [2] | Batch: [96] | Training_Loss: 2.476 |
Epoch: [2] | Batch: [97] | Training_Loss: 1.519 |
Epoch: [2] | Batch: [98] | Training_Loss: 1.791 |
Epoch: [2] | Batch: [99] | Training_Loss: 2.383 |
Epoch: [2] | Batch: [100] | Training_Loss: 1.759 |
Epoch: [2] | Batch: [101] | Training_Loss: 1.687 |
Epoch: [2] | Batch: [102] | Training_Loss: 1.677 |
Epoch: [2] | Batch: [103] | Training_Loss: 1.583 |
Epoch: [2] | Batch: [104] | Training_Loss: 2.573 |
Epoch: [2] | Batch: [105] | Training_Loss: 2.831 |
Epoch: [2] | Batch: [106] | Training_Loss: 1.498 |
Epoch: [2] | Batch: [107] | Training_Loss: 2.327 |
Epoch: [2] | Batch: [108] | Training_Loss: 2.149 |
Epoch: [2] | Batch: [109] | Training_Loss: 2.823 |
Epoch: [2] | Batch: [110] | Training_Loss: 2.861 |
Epoch: [2] | Batch: [111] | Training_Loss: 1.806 |
Epoch: [2] | Batch: [112] | Training_Loss: 2.288 |
Epoch: [2] | Batch: [113] | Training_Loss: 2.527 |
Epoch: [2] | Batch: [114] | Training_Loss: 1.750 |
Epoch: [2] | Batch: [115] | Training_Loss: 2.118 |
Epoch: [2] | Batch: [116] | Training_Loss: 2.334 |
Epoch: [2] | Batch: [117] | Training_Loss: 2.515 |
Epoch: [2] | Batch: [118] | Training_Loss: 3.020 |
Epoch: [2] | Batch: [119] | Training_Loss: 2.921 |
Epoch: [2] | Batch: [120] | Training_Loss: 3.685 |
Epoch: [2] | Batch: [121] | Training_Loss: 2.312 |
Epoch: [2] | Batch: [122] | Training_Loss: 3.216 |
Epoch: [2] | Batch: [123] | Training_Loss: 1.482 |
Epoch: [2] | Batch: [124] | Training_Loss: 1.938 |
Epoch: [2] | Batch: [125] | Training_Loss: 1.391 |
Epoch: [2] | Batch: [126] | Training_Loss: 2.289 |
Epoch: [2] | Batch: [127] | Training_Loss: 2.701 |
Epoch: [2] | Batch: [128] | Training_Loss: 1.484 |
Epoch: [2] | Batch: [129] | Training_Loss: 2.717 |
Epoch: [2] | Batch: [130] | Training_Loss: 3.223 |
Epoch: [2] | Batch: [131] | Training_Loss: 2.992 |
Epoch: [2] | Batch: [132] | Training_Loss: 2.847 |
Epoch: [2] | Batch: [133] | Training_Loss: 2.167 |
Epoch: [2] | Batch: [134] | Training_Loss: 2.117 |
Epoch: [2] | Batch: [135] | Training_Loss: 1.897 |
Epoch: [2] | Batch: [136] | Training_Loss: 2.273 |
Epoch: [2] | Batch: [137] | Training_Loss: 2.351 |
Epoch: [2] | Batch: [138] | Training_Loss: 1.516 |
Epoch: [2] | Batch: [139] | Training_Loss: 2.306 |
Epoch: [2] | Batch: [140] | Training_Loss: 2.702 |
Epoch: [2] | Batch: [141] | Training_Loss: 2.537 |
Epoch: [2] | Batch: [142] | Training_Loss: 2.269 |
Epoch: [2] | Batch: [143] | Training_Loss: 2.411 |
Epoch: [2] | Batch: [144] | Training_Loss: 3.767 |
Epoch: [2] | Batch: [145] | Training_Loss: 2.838 |
Epoch: [2] | Batch: [146] | Training_Loss: 1.798 |
Epoch: [2] | Batch: [147] | Training_Loss: 2.584 |
Epoch: [2] | Batch: [148] | Training_Loss: 1.732 |
Epoch: [2] | Batch: [149] | Training_Loss: 1.351 |
Epoch: [2] | Batch: [150] | Training_Loss: 2.228 |
Epoch: [2] | Batch: [151] | Training_Loss: 2.313 |
Epoch: [2] | Batch: [152] | Training_Loss: 2.203 |
Epoch: [2] | Batch: [153] | Training_Loss: 2.303 |
Epoch: [2] | Batch: [154] | Training_Loss: 2.862 |
Epoch: [2] | Batch: [155] | Training_Loss: 1.892 |
Epoch: [2] | Batch: [156] | Training_Loss: 2.246 |
Epoch: [2] | Batch: [157] | Training_Loss: 2.160 |
Epoch: [2] | Batch: [158] | Training_Loss: 1.735 |
Epoch: [2] | Batch: [159] | Training_Loss: 1.598 |
Epoch: [2] | Batch: [160] | Training_Loss: 1.503 |
Epoch: [2] | Batch: [161] | Training_Loss: 2.228 |
Epoch: [2] | Batch: [162] | Training_Loss: 2.340 |
Epoch: [2] | Batch: [163] | Training_Loss: 3.004 |
Epoch: [2] | Batch: [164] | Training_Loss: 2.081 |
Epoch: [2] | Batch: [165] | Training_Loss: 2.187 |
Epoch: [2] | Batch: [166] | Training_Loss: 1.909 |
Epoch: [2] | Batch: [167] | Training_Loss: 2.609 |
Epoch: [2] | Batch: [168] | Training_Loss: 2.191 |
Epoch: [2] | Batch: [169] | Training_Loss: 2.310 |
Epoch: [2] | Batch: [170] | Training_Loss: 1.395 |
Epoch: [2] | Batch: [171] | Training_Loss: 2.267 |
Epoch: [2] | Batch: [172] | Training_Loss: 2.695 |
Epoch: [2] | Batch: [173] | Training_Loss: 1.562 |
Epoch: [2] | Batch: [174] | Training_Loss: 2.053 |
Epoch: [2] | Batch: [175] | Training_Loss: 2.538 |
Epoch: [2] | Batch: [176] | Training_Loss: 1.375 |
Epoch: [2] | Batch: [177] | Training_Loss: 1.514 |
Epoch: [2] | Batch: [178] | Training_Loss: 2.266 |
Epoch: [2] | Batch: [179] | Training_Loss: 2.492 |
Epoch: [2] | Batch: [180] | Training_Loss: 2.471 |
Epoch: [2] | Batch: [181] | Training_Loss: 1.501 |
Epoch: [2] | Batch: [182] | Training_Loss: 2.494 |
Epoch: [2] | Batch: [183] | Training_Loss: 2.532 |
Epoch: [2] | Batch: [184] | Training_Loss: 2.385 |
Epoch: [2] | Batch: [185] | Training_Loss: 2.490 |
Epoch: [2] | Batch: [186] | Training_Loss: 2.086 |
Epoch: [2] | Batch: [187] | Training_Loss: 2.081 |
Epoch: [2] | Batch: [188] | Training_Loss: 1.495 |
Epoch: [2] | Batch: [189] | Training_Loss: 2.106 |
Epoch: [2] | Batch: [190] | Training_Loss: 1.865 |
Epoch: [2] | Batch: [191] | Training_Loss: 1.469 |
Epoch: [2] | Batch: [192] | Training_Loss: 2.253 |
Epoch: [2] | Batch: [193] | Training_Loss: 2.463 |
Epoch: [2] | Batch: [194] | Training_Loss: 1.830 |
Epoch: [2] | Batch: [195] | Training_Loss: 2.518 |
Epoch: [2] | Batch: [196] | Training_Loss: 2.473 |
Epoch: [2] | Batch: [197] | Training_Loss: 2.122 |
Epoch: [2] | Batch: [198] | Training_Loss: 2.292 |
Epoch: [2] | Batch: [199] | Training_Loss: 2.034 |
Epoch: [2] | Batch: [200] | Training_Loss: 2.440 |
Epoch: [2] | Batch: [201] | Training_Loss: 2.562 |
Epoch: [2] | Batch: [202] | Training_Loss: 1.776 |
Epoch: [2] | Batch: [203] | Training_Loss: 1.926 |
Epoch: [2] | Batch: [204] | Training_Loss: 2.248 |
Epoch: [2] | Batch: [205] | Training_Loss: 2.099 |
Epoch: [2] | Batch: [206] | Training_Loss: 1.381 |
Epoch: [2] | Batch: [207] | Training_Loss: 1.454 |
Epoch: [2] | Batch: [208] | Training_Loss: 3.148 |
Epoch: [2] | Batch: [209] | Training_Loss: 1.988 |
Epoch: [2] | Batch: [210] | Training_Loss: 2.049 |
Epoch: [2] | Batch: [211] | Training_Loss: 1.682 |
Epoch: [2] | Batch: [212] | Training_Loss: 2.034 |
Epoch: [2] | Batch: [213] | Training_Loss: 1.883 |
Epoch: [2] | Batch: [214] | Training_Loss: 2.215 |
Epoch: [2] | Batch: [215] | Training_Loss: 2.167 |
Epoch: [2] | Batch: [216] | Training_Loss: 3.154 |
Epoch: [2] | Batch: [217] | Training_Loss: 2.424 |
Epoch: [2] | Batch: [218] | Training_Loss: 1.699 |
Epoch: [2] | Batch: [219] | Training_Loss: 2.193 |
Epoch: [2] | Batch: [220] | Training_Loss: 2.062 |
Epoch: [2] | Batch: [221] | Training_Loss: 1.534 |
Epoch: [2] | Batch: [222] | Training_Loss: 2.312 |
Epoch: [2] | Batch: [223] | Training_Loss: 2.452 |
Epoch: [2] | Batch: [224] | Training_Loss: 2.066 |
Epoch: [2] | Batch: [225] | Training_Loss: 2.399 |
Epoch: [2] | Batch: [226] | Training_Loss: 2.784 |
Epoch: [2] | Batch: [227] | Training_Loss: 2.547 |
Epoch: [2] | Batch: [228] | Training_Loss: 1.592 |
Epoch: [2] | Batch: [229] | Training_Loss: 1.958 |
Epoch: [2] | Batch: [230] | Training_Loss: 2.776 |
Epoch: [2] | Batch: [231] | Training_Loss: 2.169 |
Epoch: [2] | Batch: [232] | Training_Loss: 1.447 |
Epoch: [2] | Batch: [233] | Training_Loss: 1.441 |
Epoch: [2] | Batch: [234] | Training_Loss: 1.969 |
Epoch: [2] | Batch: [235] | Training_Loss: 1.424 |
Epoch: [2] | Batch: [236] | Training_Loss: 1.481 |
Epoch: [2] | Batch: [237] | Training_Loss: 1.490 |
Epoch: [2] | Batch: [238] | Training_Loss: 2.418 |
Epoch: [2] | Batch: [239] | Training_Loss: 1.403 |
Epoch: [2] | Batch: [240] | Training_Loss: 2.044 |
Epoch: [2] | Batch: [241] | Training_Loss: 1.926 |
Epoch: [2] | Batch: [242] | Training_Loss: 2.814 |
Epoch: [2] | Batch: [243] | Training_Loss: 1.864 |
Epoch: [2] | Batch: [244] | Training_Loss: 2.228 |
Epoch: [2] | Batch: [245] | Training_Loss: 1.961 |
Epoch: [2] | Batch: [246] | Training_Loss: 2.252 |
Epoch: [2] | Batch: [247] | Training_Loss: 3.592 |
Epoch: [2] | Batch: [248] | Training_Loss: 2.004 |
Epoch: [2] | Batch: [249] | Training_Loss: 2.181 |
Epoch: [2] | Batch: [250] | Training_Loss: 1.289 |
Epoch: [2] | Batch: [251] | Training_Loss: 2.232 |
Epoch: [2] | Batch: [252] | Training_Loss: 3.156 |
Epoch: [2] | Batch: [253] | Training_Loss: 3.157 |
Epoch: [2] | Batch: [254] | Training_Loss: 2.292 |
Epoch: [2] | Batch: [255] | Training_Loss: 1.981 |
Epoch: [2] | Batch: [256] | Training_Loss: 2.202 |
Epoch: [2] | Batch: [257] | Training_Loss: 1.310 |
Epoch: [2] | Batch: [258] | Training_Loss: 1.306 |
Epoch: [2] | Batch: [259] | Training_Loss: 1.402 |
Epoch: [2] | Batch: [260] | Training_Loss: 2.429 |
Epoch: [2] | Batch: [261] | Training_Loss: 2.277 |
Epoch: [2] | Batch: [262] | Training_Loss: 1.374 |
Epoch: [2] | Batch: [263] | Training_Loss: 1.934 |
Epoch: [2] | Batch: [264] | Training_Loss: 2.055 |
Epoch: [2] | Batch: [265] | Training_Loss: 1.326 |
Epoch: [2] | Batch: [266] | Training_Loss: 2.161 |
Epoch: [2] | Batch: [267] | Training_Loss: 1.920 |
Epoch: [2] | Batch: [268] | Training_Loss: 1.870 |
Epoch: [2] | Batch: [269] | Training_Loss: 1.438 |
Epoch: [2] | Batch: [270] | Training_Loss: 2.058 |
Epoch: [2] | Batch: [271] | Training_Loss: 1.931 |
Epoch: [2] | Batch: [272] | Training_Loss: 2.825 |
Epoch: [2] | Batch: [273] | Training_Loss: 2.039 |
Epoch: [2] | Batch: [274] | Training_Loss: 2.677 |
Epoch: [2] | Batch: [275] | Training_Loss: 2.948 |
Epoch: [2] | Batch: [276] | Training_Loss: 1.928 |
Epoch: [2] | Batch: [277] | Training_Loss: 2.115 |
Epoch: [2] | Batch: [278] | Training_Loss: 2.077 |
Epoch: [2] | Batch: [279] | Training_Loss: 2.216 |
Epoch: [2] | Batch: [280] | Training_Loss: 2.096 |
Epoch: [2] | Batch: [281] | Training_Loss: 3.126 |
Epoch: [2] | Batch: [282] | Training_Loss: 1.850 |
Epoch: [2] | Batch: [283] | Training_Loss: 2.017 |
Epoch: [2] | Batch: [284] | Training_Loss: 3.144 |
Epoch: [2] | Batch: [285] | Training_Loss: 1.605 |
Epoch: [2] | Batch: [286] | Training_Loss: 2.267 |
Epoch: [2] | Batch: [287] | Training_Loss: 1.660 |
Epoch: [2] | Batch: [288] | Training_Loss: 1.356 |
Epoch: [2] | Batch: [289] | Training_Loss: 1.793 |
Epoch: [2] | Batch: [290] | Training_Loss: 1.296 |
Epoch: [2] | Batch: [291] | Training_Loss: 2.845 |
Epoch: [2] | Batch: [292] | Training_Loss: 2.701 |
Epoch: [2] | Batch: [293] | Training_Loss: 2.059 |
Epoch: [2] | Batch: [294] | Training_Loss: 2.274 |
Epoch: [2] | Batch: [295] | Training_Loss: 2.316 |
Epoch: [2] | Batch: [296] | Training_Loss: 2.496 |
Epoch: [2] | Batch: [297] | Training_Loss: 2.159 |
Epoch: [2] | Batch: [298] | Training_Loss: 2.335 |
Epoch: [2] | Batch: [299] | Training_Loss: 2.512 |
Epoch: [2] | Batch: [300] | Training_Loss: 1.793 |
Epoch: [2] | Batch: [301] | Training_Loss: 2.644 |
Epoch: [2] | Batch: [302] | Training_Loss: 2.441 |
Epoch: [2] | Batch: [303] | Training_Loss: 2.195 |
Epoch: [2] | Batch: [304] | Training_Loss: 2.048 |
Epoch: [2] | Batch: [305] | Training_Loss: 1.829 |
Epoch: [2] | Batch: [306] | Training_Loss: 2.131 |
Epoch: [2] | Batch: [307] | Training_Loss: 2.707 |
Epoch: [2] | Batch: [308] | Training_Loss: 2.319 |
Epoch: [2] | Batch: [309] | Training_Loss: 2.960 |
Epoch: [2] | Batch: [310] | Training_Loss: 1.389 |
Epoch: [2] | Batch: [311] | Training_Loss: 1.916 |
Epoch: [2] | Batch: [312] | Training_Loss: 2.047 |
Epoch: [2] | Batch: [313] | Training_Loss: 2.004 |
Epoch: [2] | Batch: [314] | Training_Loss: 1.284 |
Epoch: [2] | Batch: [315] | Training_Loss: 1.796 |
Epoch: [2] | Batch: [316] | Training_Loss: 2.849 |
Epoch: [2] | Batch: [317] | Training_Loss: 1.974 |
Epoch: [2] | Batch: [318] | Training_Loss: 2.199 |
Epoch: [2] | Batch: [319] | Training_Loss: 2.681 |
Epoch: [2] | Batch: [320] | Training_Loss: 2.031 |
Epoch: [2] | Batch: [321] | Training_Loss: 2.186 |
Epoch: [2] | Batch: [322] | Training_Loss: 1.277 |
Epoch: [2] | Batch: [323] | Training_Loss: 1.861 |
Epoch: [2] | Batch: [324] | Training_Loss: 2.239 |
Epoch: [2] | Batch: [325] | Training_Loss: 1.228 |
Epoch: [2] | Batch: [326] | Training_Loss: 1.662 |
Epoch: [2] | Batch: [327] | Training_Loss: 1.269 |
Epoch: [2] | Batch: [328] | Training_Loss: 1.170 |
Epoch: [2] | Batch: [329] | Training_Loss: 2.429 |
Epoch: [2] | Batch: [330] | Training_Loss: 2.192 |
Epoch: [2] | Batch: [331] | Training_Loss: 1.847 |
Epoch: [2] | Batch: [332] | Training_Loss: 1.819 |
Epoch: [2] | Batch: [333] | Training_Loss: 1.666 |
Epoch: [2] | Batch: [334] | Training_Loss: 1.204 |
Epoch: [2] | Batch: [335] | Training_Loss: 1.261 |
Epoch: [2] | Batch: [336] | Training_Loss: 1.199 |
Epoch: [2] | Batch: [337] | Training_Loss: 1.174 |
Epoch: [2] | Batch: [338] | Training_Loss: 2.171 |
Epoch: [2] | Batch: [339] | Training_Loss: 2.170 |
Epoch: [2] | Batch: [340] | Training_Loss: 1.946 |
Epoch: [2] | Batch: [341] | Training_Loss: 2.603 |
Epoch: [2] | Batch: [342] | Training_Loss: 3.557 |
Epoch: [2] | Batch: [343] | Training_Loss: 1.503 |
Epoch: [2] | Batch: [344] | Training_Loss: 1.894 |
Epoch: [2] | Batch: [345] | Training_Loss: 2.180 |
Epoch: [2] | Batch: [346] | Training_Loss: 2.353 |
Epoch: [2] | Batch: [347] | Training_Loss: 1.887 |
Epoch: [2] | Batch: [348] | Training_Loss: 1.854 |
Epoch: [2] | Batch: [349] | Training_Loss: 2.282 |
Epoch: [2] | Batch: [350] | Training_Loss: 1.214 |
Epoch: [2] | Batch: [351] | Training_Loss: 2.739 |
Epoch: [2] | Batch: [352] | Training_Loss: 1.397 |
Epoch: [2] | Batch: [353] | Training_Loss: 2.320 |
Epoch: [2] | Batch: [354] | Training_Loss: 1.301 |
Epoch: [2] | Batch: [355] | Training_Loss: 1.954 |
Epoch: [2] | Batch: [356] | Training_Loss: 1.955 |
Epoch: [2] | Batch: [357] | Training_Loss: 2.328 |
Epoch: [2] | Batch: [358] | Training_Loss: 2.058 |
Epoch: [2] | Batch: [359] | Training_Loss: 2.633 |
Epoch: [2] | Batch: [360] | Training_Loss: 2.930 |
Epoch: [2] | Batch: [361] | Training_Loss: 1.812 |
Epoch: [2] | Batch: [362] | Training_Loss: 1.304 |
Epoch: [2] | Batch: [363] | Training_Loss: 2.061 |
Epoch: [2] | Batch: [364] | Training_Loss: 2.249 |
Epoch: [2] | Batch: [365] | Training_Loss: 2.380 |
Epoch: [2] | Batch: [366] | Training_Loss: 2.727 |
Epoch: [2] | Batch: [367] | Training_Loss: 2.284 |
Epoch: [2] | Batch: [368] | Training_Loss: 3.384 |
Epoch: [2] | Batch: [369] | Training_Loss: 1.442 |
Epoch: [2] | Batch: [370] | Training_Loss: 1.921 |
Epoch: [2] | Batch: [371] | Training_Loss: 1.679 |
Epoch: [2] | Batch: [372] | Training_Loss: 2.221 |
Epoch: [2] | Batch: [373] | Training_Loss: 1.804 |
Epoch: [2] | Batch: [374] | Training_Loss: 2.212 |
Epoch: [2] | Batch: [375] | Training_Loss: 1.908 |
Epoch: [2] | Batch: [376] | Training_Loss: 1.712 |
Epoch: [2] | Batch: [377] | Training_Loss: 1.988 |
Epoch: [2] | Batch: [378] | Training_Loss: 1.912 |
Epoch: [2] | Batch: [379] | Training_Loss: 2.457 |
Epoch: [2] | Batch: [380] | Training_Loss: 2.404 |
Epoch: [2] | Batch: [381] | Training_Loss: 1.927 |
Epoch: [2] | Batch: [382] | Training_Loss: 2.187 |
Epoch: [2] | Batch: [383] | Training_Loss: 2.295 |
Epoch: [2] | Batch: [384] | Training_Loss: 1.132 |
Epoch: [2] | Batch: [385] | Training_Loss: 1.374 |
Epoch: [2] | Batch: [386] | Training_Loss: 1.104 |
Epoch: [2] | Batch: [387] | Training_Loss: 1.760 |
Epoch: [2] | Batch: [388] | Training_Loss: 1.174 |
Epoch: [2] | Batch: [389] | Training_Loss: 2.106 |
Epoch: [2] | Batch: [390] | Training_Loss: 1.942 |
Epoch: [2] | Batch: [391] | Training_Loss: 2.700 |
Epoch: [2] | Batch: [392] | Training_Loss: 1.768 |
Epoch: [2] | Batch: [393] | Training_Loss: 1.200 |
Epoch: [2] | Batch: [394] | Training_Loss: 1.993 |
Epoch: [2] | Batch: [395] | Training_Loss: 2.087 |
Epoch: [2] | Batch: [396] | Training_Loss: 1.864 |
Epoch: [2] | Batch: [397] | Training_Loss: 1.978 |
Epoch: [2] | Batch: [398] | Training_Loss: 2.805 |
Epoch: [2] | Batch: [399] | Training_Loss: 2.140 |
Epoch: [2] | Batch: [400] | Training_Loss: 1.274 |
Epoch: [2] | Batch: [401] | Training_Loss: 1.987 |
Epoch: [2] | Batch: [402] | Training_Loss: 1.332 |
Epoch: [2] | Batch: [403] | Training_Loss: 1.999 |
Epoch: [2] | Batch: [404] | Training_Loss: 1.965 |
Epoch: [2] | Batch: [405] | Training_Loss: 1.364 |
Epoch: [2] | Batch: [406] | Training_Loss: 2.967 |
Epoch: [2] | Batch: [407] | Training_Loss: 1.881 |
Epoch: [2] | Batch: [408] | Training_Loss: 1.596 |
Epoch: [2] | Batch: [409] | Training_Loss: 1.479 |
Epoch: [2] | Batch: [410] | Training_Loss: 2.142 |
Epoch: [2] | Batch: [411] | Training_Loss: 1.791 |
Epoch: [2] | Batch: [412] | Training_Loss: 1.352 |
Epoch: [2] | Batch: [413] | Training_Loss: 1.167 |
Epoch: [2] | Batch: [414] | Training_Loss: 1.978 |
Epoch: [2] | Batch: [415] | Training_Loss: 1.891 |
Epoch: [2] | Batch: [416] | Training_Loss: 1.708 |
Epoch: [2] | Batch: [417] | Training_Loss: 2.358 |
Epoch: [2] | Batch: [418] | Training_Loss: 1.814 |
Epoch: [2] | Batch: [419] | Training_Loss: 1.594 |
Epoch: [2] | Batch: [420] | Training_Loss: 2.178 |
Epoch: [2] | Batch: [421] | Training_Loss: 1.173 |
Epoch: [2] | Batch: [422] | Training_Loss: 2.518 |
Epoch: [2] | Batch: [423] | Training_Loss: 3.213 |
Epoch: [2] | Batch: [424] | Training_Loss: 2.021 |
Epoch: [2] | Batch: [425] | Training_Loss: 1.885 |
Epoch: [2] | Batch: [426] | Training_Loss: 2.546 |
Epoch: [2] | Batch: [427] | Training_Loss: 1.739 |
Epoch: [2] | Batch: [428] | Training_Loss: 2.443 |
Epoch: [2] | Batch: [429] | Training_Loss: 2.167 |
Epoch: [2] | Batch: [430] | Training_Loss: 1.705 |
Epoch: [2] | Batch: [431] | Training_Loss: 1.874 |
Epoch: [2] | Batch: [432] | Training_Loss: 2.011 |
Epoch: [2] | Batch: [433] | Training_Loss: 2.218 |
Epoch: [2] | Batch: [434] | Training_Loss: 1.924 |
Epoch: [2] | Batch: [435] | Training_Loss: 1.177 |
Epoch: [2] | Batch: [436] | Training_Loss: 1.630 |
Epoch: [2] | Batch: [437] | Training_Loss: 1.557 |
Epoch: [2] | Batch: [438] | Training_Loss: 2.254 |
Epoch: [2] | Batch: [439] | Training_Loss: 3.125 |
Epoch: [2] | Batch: [440] | Training_Loss: 1.551 |
Epoch: [2] | Batch: [441] | Training_Loss: 2.414 |
Epoch: [2] | Batch: [442] | Training_Loss: 2.296 |
Epoch: [2] | Batch: [443] | Training_Loss: 1.856 |
Epoch: [2] | Batch: [444] | Training_Loss: 2.291 |
Epoch: [2] | Batch: [445] | Training_Loss: 2.173 |
Epoch: [2] | Batch: [446] | Training_Loss: 1.533 |
Epoch: [2] | Batch: [447] | Training_Loss: 2.148 |
Epoch: [2] | Batch: [448] | Training_Loss: 1.239 |
Epoch: [2] | Batch: [449] | Training_Loss: 2.281 |
Epoch: [2] | Batch: [450] | Training_Loss: 1.226 |
Epoch: [2] | Batch: [451] | Training_Loss: 2.482 |
Epoch: [2] | Batch: [452] | Training_Loss: 2.266 |
Epoch: [2] | Batch: [453] | Training_Loss: 1.717 |
Epoch: [2] | Batch: [454] | Training_Loss: 1.978 |
Epoch: [2] | Batch: [455] | Training_Loss: 1.088 |
Epoch: [2] | Batch: [456] | Training_Loss: 1.154 |
Epoch: [2] | Batch: [457] | Training_Loss: 1.162 |
Epoch: [2] | Batch: [458] | Training_Loss: 1.746 |
Epoch: [2] | Batch: [459] | Training_Loss: 2.795 |
Epoch: [2] | Batch: [460] | Training_Loss: 2.265 |
Epoch: [2] | Batch: [461] | Training_Loss: 1.240 |
Epoch: [2] | Batch: [462] | Training_Loss: 2.154 |
Epoch: [2] | Batch: [463] | Training_Loss: 3.132 |
Epoch: [2] | Training_Loss: 2.169 |
Epoch: [2] | Validation_Loss: 1.840 |
Epoch: [3] | Batch: [1] | Training_Loss: 2.574 |
Epoch: [3] | Batch: [2] | Training_Loss: 1.709 |
Epoch: [3] | Batch: [3] | Training_Loss: 1.851 |
Epoch: [3] | Batch: [4] | Training_Loss: 2.016 |
Epoch: [3] | Batch: [5] | Training_Loss: 1.097 |
Epoch: [3] | Batch: [6] | Training_Loss: 1.274 |
Epoch: [3] | Batch: [7] | Training_Loss: 2.373 |
Epoch: [3] | Batch: [8] | Training_Loss: 2.139 |
Epoch: [3] | Batch: [9] | Training_Loss: 2.269 |
Epoch: [3] | Batch: [10] | Training_Loss: 2.062 |
Epoch: [3] | Batch: [11] | Training_Loss: 1.911 |
Epoch: [3] | Batch: [12] | Training_Loss: 2.637 |
Epoch: [3] | Batch: [13] | Training_Loss: 3.025 |
Epoch: [3] | Batch: [14] | Training_Loss: 1.604 |
Epoch: [3] | Batch: [15] | Training_Loss: 1.456 |
Epoch: [3] | Batch: [16] | Training_Loss: 1.263 |
Epoch: [3] | Batch: [17] | Training_Loss: 1.024 |
Epoch: [3] | Batch: [18] | Training_Loss: 2.147 |
Epoch: [3] | Batch: [19] | Training_Loss: 1.578 |
Epoch: [3] | Batch: [20] | Training_Loss: 1.581 |
Epoch: [3] | Batch: [21] | Training_Loss: 2.132 |
Epoch: [3] | Batch: [22] | Training_Loss: 1.937 |
Epoch: [3] | Batch: [23] | Training_Loss: 1.993 |
Epoch: [3] | Batch: [24] | Training_Loss: 1.732 |
Epoch: [3] | Batch: [25] | Training_Loss: 1.858 |
Epoch: [3] | Batch: [26] | Training_Loss: 1.965 |
Epoch: [3] | Batch: [27] | Training_Loss: 1.090 |
Epoch: [3] | Batch: [28] | Training_Loss: 1.468 |
Epoch: [3] | Batch: [29] | Training_Loss: 1.797 |
Epoch: [3] | Batch: [30] | Training_Loss: 1.970 |
Epoch: [3] | Batch: [31] | Training_Loss: 1.035 |
Traceback (most recent call last):
  File "./tracking.py", line 92, in <module>
    model_training(data_list_train, data_list_valid, epochs, acc_epoch, acc_epoch2, save_model_epochs, validation_epochs, batchsize, "logfile", load_checkpoint)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/model_training.py", line 266, in model_training
    output, output2, ground_truth, ground_truth2, det_num, tracklet_num= complete_net(batch)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch_geometric/nn/data_parallel.py", line 60, in forward
    return self.module(data)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/network/complete_net.py", line 82, in forward
    output = self.optim_net(node_embedding, combined_edge_features, edge_index, coords, frame)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/Galoaa.b/.conda/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/network/optimizationGNN.py", line 66, in forward
    edge_attr = self.similarity1(out, edge_index, edge_attr_transformed)
  File "/home/galoaa.b/ondemand/dev/GCNNMatch/network/optimizationGNN.py", line 39, in similarity1
    node_feat_src = self.node_feature_mlp(node_embedding[edge_index[0][i]].unsqueeze(0))
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

